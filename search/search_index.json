{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Adaptive Stress Testing for Robust AI &amp; Reinforcement Learning (ASTRA-RL)","text":"<p>Welcome to the ASTRA-RL toolbox documentation! This documentation provides an overview of the ASTRA-RL toolbox, its features, and how to use it effectively.</p> <p>Warning</p> <p>This documentation (and project) is a work in progress. If you have any questions or suggestions, please feel free to open an issue on the GitHub repository</p>"},{"location":"api/index.html","title":"Library API","text":"<p>This section provides detailed information about all the classes, functions, and modules available in the ASTRA-RL toolbox. Each entry includes a description of its purpose, parameters, and usage examples.</p> <p>This documentation is generated automatically from the codebase using docstrings and comments, ensuring that it stays up-to-date with the latest changes.</p>"},{"location":"tutorials/index.html","title":"Tutorials","text":"<p>This section provides step-by-step guides and examples to help you get started with the ASTRA-RL toolbox. Each tutorial covers a specific aspect of the toolbox, from basic usage to advanced features.</p>"},{"location":"tutorials/quick_start_evaluation.html","title":"Running An Evaluation","text":"<p>This tutorial will guide you through the process of running an evaluation using an existing evaluator model. The evaluator is a model that can be used to assess the quality of other models' outputs. </p> <p>For the purposes of this tutorial, we will assume that you have already trained an evaluator model from the Training an Evaluator tutorial.mk</p>"},{"location":"tutorials/quick_start_training.html","title":"Quick Start: Training a HuggingFace Attacker with ASTRA-RL","text":"<p>Do you want to train a HuggingFace attacker using an ASTRA-supported algorithm (e.g., DPO, IPO, PPO) and problem formulation (ASTPrompter, RL - Perez, MALIBU, CRT*)? formulations coming soon</p> <p>Then this guide is for you. We\u2019ll walk through every step required to train a red-teaming attacker using our pre-configured classes and point you to customization guides when your use case goes beyond the defaults. By using our pre-configured classes, you'll be training your attacker in 7 easy steps!</p>"},{"location":"tutorials/quick_start_training.html#step-1-setup","title":"Step 1: Setup","text":"<p>Please see the README for full setup instructions. Here's a quick recap:</p> <pre><code># Install the ASTRA-RL toolbox\npip install astra-rl\n\n# Clone the repository (for examples and development)\ngit clone git@github.com:sisl/astra-rl.git\n</code></pre> <p>Note: wandb is not automatically installed during this process. If you would let to use wandb (supported), install it (uv pip install wandb) and run export WANDB_API_KEY=###your_wandb_api_key##### in your terminal.</p>"},{"location":"tutorials/quick_start_training.html#step-2-import-required-modules-and-set-device","title":"Step 2: Import Required Modules and set device","text":"<pre><code>from torch.optim import AdamW\n\n# ASTRA-RL core components\nfrom astra_rl import ASTEnvironment, DPO, DetoxifyModerator, Harness\n\n# HuggingFace-friendly problem wrapper for ASTPrompter-style red teaming\nfrom astra_rl.ext.transformers import HFASTProblem\nfrom astra_rl.training import Trainer, TrainingConfiguration\n\nDEVICE = \"cuda\"  # or \"cpu\" if GPU is not available\n</code></pre> <p>We support both lightweight (e.g., GPT-2 + Detoxify) and heavyweight (e.g., LLaMA + LlamaGuard) setups. Pick model and moderator sizes that fit your compute!</p>"},{"location":"tutorials/quick_start_training.html#step-3-load-your-initial-prompts","title":"Step 3: Load Your Initial Prompts","text":"<p>To train an attacker, you\u2019ll need a list of comma-separated strings that act as initial prompts\u2014these initiate attacker-target rollouts used for online training. </p> <pre><code>import json\n\nwith open(\"prompts_reddit_train.json\") as f:\n    PROMPTS = json.load(f)\n</code></pre> <p>Since ASTPrompter red-teams for harmful outputs in conversational settings, it uses the ConvoKit Reddit Small Corpus (filtered for proper formatting and for non-toxicity using Detoxify) as its default source of initial prompts. This data can be found in basic examples.</p> <p>The ASTRA-RL toolbox easily supports external prompt datasets or APIs\u2014just ensure the final PROMPTS variable is formatted as a list of strings.</p>"},{"location":"tutorials/quick_start_training.html#step-4-instantiate-your-problem","title":"Step 4: Instantiate Your Problem","text":"<p>The problem is an important component of training that handles rollout step generation, reward computation, and log-probability calculation. To speed you along, we have implemented the <code>HFASTProblem</code> class that handles the technical backend so you just need to provide the huggingface model IDs of the attacker, target and baseline models and a <code>Moderator</code> instance (DetexifyModerator(), LlamaGuardModerator() or your custom moderator).</p> <p>Note: Your attacker and target can be different models (e.g., GPT-2 attacker, LLaMA target) but your baseline and attacker should typically be the same.</p> <pre><code>from astra_rl.modifiers import LlamaGuardModerator  # optional\n\n# Example problem instantiation: GPT2 attacker, target, and baseline with Detoxify moderator (lightweight setup)\nproblem = HFASTProblem(\"gpt2\", \"gpt2\", \"gpt2\", DetoxifyModerator(), DEVICE)\n</code></pre> <p>Need a custom model or rollout step logic? See customize_training/problems</p> <p>Want to use a custom moderator? See customize_training/moderators</p>"},{"location":"tutorials/quick_start_training.html#step-5-instantiate-the-environment","title":"Step 5: Instantiate the Environment","text":"<p>The environment defines how training rollouts are structured and collected. In ASTRA-RL, the default is the <code>ASTEnvironment</code>, which implements the conversation tree rollout used in the ASTPrompter paper.</p> <pre><code>env = ASTEnvironment(problem, PROMPTS)\n</code></pre> Curious about how this environment structures rollouts?    This environment builds a tree-structured conversation graph, where:   - The root node starts from a random initial prompt (from `PROMPTS`)   - At each turn, the attacker generates multiple (`tree_width`, default 2) candidate utterances   - Each of those utterances is fed to the target model, which produces a response   - The resulting attacker\u2013target\u2013response tuples form child nodes   - This process repeats for `tree_depth` levels (default 3), yielding a multi-turn attacker\u2013target dialogue tree    This structure enables preference-based learning algorithms like DPO and IPO to reason over multiple conversational branches at once, training the attacker to elicit harmful responses in a multi-turn setting.  <p>By default, rollouts are configured with tree_width=2 and tree_depth=3, but you can customize both:</p> <pre><code>env = ASTEnvironment(problem, PROMPTS, tree_width=4, tree_depth=5)\n</code></pre> <p>Want a different rollout graph structure or a multi-agent setup? See customize_training/environments</p>"},{"location":"tutorials/quick_start_training.html#step-6-choose-your-algorithm-and-optimizer","title":"Step 6: Choose Your Algorithm and Optimizer","text":"<p>The solver is the RL learning algorithm that will take in a graph of training rollouts and compute the loss. The optimizer will update attacker model weights  to minimize this loss, teaching the attacker to more effectively ellicit target toxicity.</p> <p>We use DPO and Adam as the default for this quickstart.</p> <pre><code>solver = DPO(problem)\noptimizer = AdamW(problem.parameters(), lr=1e-5)\n</code></pre> <p>To integrate your own RL algorithm, see customize_training/solvers</p>"},{"location":"tutorials/quick_start_training.html#step-7-train-the-attacker","title":"Step 7: Train the Attacker","text":"<p>For the quick start approach, simply call our training configuration and trainer classes and start training!</p> <pre><code># instantiate the pre-configured HF-compatable configuration and traininer class\nconfig = HFASTConfiguration() # lr = 1e-5, batch size = 4, optimizer = \"adamw\", no gradient accumulation, 1000 training steps, 2 episodes per experience\n# this trainer will train the attacker and evaluate it on a dev set every 100 steps, saving the best model to \"checkpoints\"\ntrainer = HFASTTrainer(\n    config,\n    env,\n    solver,\n    dev_prompts=DEV_PROMPTS,\n    eval_every=100,\n    ckpt_dir=\"checkpoints\",\n)\ntrainer.train()\n</code></pre> <p>The source code for the training configuration and trainer are at hf_ast_problem</p>"},{"location":"tutorials/quick_start_training.html#want-to-customize-the-training-configurationhyperparams-the-training-loop-or-model-savingeval-go-to-customize_trainingtrainers","title":"Want to customize the training configuration/hyperparams, the training loop, or model saving/eval? Go to customize_training/trainers!","text":""},{"location":"tutorials/quick_start_training.html#full-examples","title":"Full Examples:","text":"<p>We provide 3 complete working examples that mirror this guide! Hugging face example without trainer: examples/ast_hf.py Custom AST problem with trainer: examples/ast_trainer Custom AST problem without trainer: examples/ast_basic.py</p>"},{"location":"tutorials/start_here.html","title":"Welcome to ASTRA-RL!","text":"<p>A user-friendly, modular, and customizable package designed for users who want to quickly jump into red teaming and those who want to build on exisiting methods! </p>"},{"location":"tutorials/start_here.html#table-of-contents","title":"Table of Contents:","text":""},{"location":"tutorials/customize_training/environments.html","title":"How to Customize the Environment","text":"<p>The environment defines how attacker\u2013target interactions are generated and packaged for training and evaluation. It controls:</p> <ul> <li>how rollouts are generated (single path vs. tree),</li> <li>how states advance across turns,</li> <li>what per-step data (actions, responses, rewards) is stored,</li> <li>and what the solver receives as input.</li> </ul> <p>Evaluation note. The environment also powers evaluation by running a single-path rollout (tree width = 1) and collecting metrics. See quick_start_evaluation for more details on evaluation.</p> <p>ASTRA-RL ships with <code>ASTEnvironment</code>, which mirrors the rollout structure used in ASTPrompter. You can subclass it\u2014or the base <code>Environment</code>\u2014to support:</p> <ul> <li>multi-agent conversations,</li> <li>tree-structured or flat trajectories,</li> <li>custom state-advance logic,</li> <li>alternative reward shaping or logging.</li> </ul>"},{"location":"tutorials/customize_training/environments.html#table-of-contents","title":"Table of Contents","text":"<ol> <li> <p>Ways to Customize</p> </li> <li> <p>1.1 Fast path: subclass <code>ASTEnvironment</code></p> </li> <li>1.2 Full control: subclass <code>Environment</code></li> <li> <p>Required Interface</p> </li> <li> <p>2.1 Nodes and Graphs</p> </li> <li>2.2 Helpful <code>ASTProblem</code> APIs</li> <li>Best Practices &amp; Sanity Checks</li> <li> <p>How-Tos</p> </li> <li> <p>4.1 Create a custom Node or Graph</p> </li> <li>4.2 Change rollout width/depth</li> <li>4.3 Multi-agent conversations (pseudocode)</li> </ol>"},{"location":"tutorials/customize_training/environments.html#1-ways-to-customize","title":"1. Ways to Customize","text":""},{"location":"tutorials/customize_training/environments.html#11-fast-path-subclass-astenvironment","title":"1.1 Fast path: subclass <code>ASTEnvironment</code>","text":"<p>If your logic is \u201cAST-like with a twist,\u201d subclass <code>ASTEnvironment</code> and override only what you need (e.g., expansion logic, evaluation, or metrics extraction).</p> <pre><code>from astra_rl import ASTProblem, ASTEnvironment\nfrom astra_rl.core.environment import Node, Graph\n\nclass MyASTVariant(ASTEnvironment):\n    \"\"\"ASTEnvironment with custom rollout behavior.\"\"\"\n\n    def __init__(self, problem: ASTProblem, prompts, tree_width=2, tree_depth=3):\n        super().__init__(problem, prompts, tree_width, tree_depth)\n\n    # Example: override the recursive expansion\n    # def __handle_prompt(self, prompt, depth, width=None):\n    #     ...\n</code></pre>"},{"location":"tutorials/customize_training/environments.html#12-full-control-subclass-environment","title":"1.2 Full control: subclass <code>Environment</code>","text":"<p>If you want to design the rollout structure from scratch, subclass <code>Environment</code>. You implement <code>rollout()</code> (and any helpers you like) and return a <code>Graph</code> of <code>Node</code>s that your solver expects.</p> <pre><code>from typing import Optional\nfrom astra_rl.core.environment import Environment, Node, Graph\n\nclass MyCustomEnvironment(Environment[str, str]):\n    def rollout(self, seed: Optional[int] = None) -&gt; Graph[str, str]:\n        # Build and return a Graph made of Node[str, str]\n        ...\n</code></pre> <p>Use your <code>Problem</code>\u2019s batch helpers for attacker/target calls and rewards. It keeps code concise and fast.</p>"},{"location":"tutorials/customize_training/environments.html#2-required-interface","title":"2. Required Interface","text":""},{"location":"tutorials/customize_training/environments.html#21-nodes-and-graphs","title":"2.1 Nodes and Graphs","text":"<p>Your environment\u2019s <code>rollout()</code> must return:</p> <ul> <li><code>Graph(context: str, children: list[Node])</code></li> <li><code>Node(context: str, attack: str, response: str, reward: float, children: list[Node])</code></li> </ul> <p>At a minimum, each node should capture:</p> <ul> <li><code>context</code> \u2014 state so far (e.g., conversation text),</li> <li><code>attack</code> \u2014 attacker\u2019s utterance / action,</li> <li><code>response</code> \u2014 target/defender\u2019s utterance / reply,</li> <li><code>reward</code> \u2014 scalar float for this turn,</li> <li><code>children</code> \u2014 next steps; empty for leaves.</li> </ul> <p>Solver contract. Make sure the structure and fields match what your solver consumes. \u2022 Preference-based methods (DPO/IPO/ORPO) usually want pairs \u2192 use <code>tree_width \u2265 2</code>. \u2022 Reward-based policy gradients (PPO/A2C) don\u2019t need pairs \u2192 <code>tree_width = 1</code> is typical.</p>"},{"location":"tutorials/customize_training/environments.html#22-helpful-astproblem-apis","title":"2.2 Helpful <code>ASTProblem</code> APIs","text":"<p>If you are using the 'ASTProblem' note these batch-friendly methods you\u2019ll typically call during rollout:</p> <ul> <li><code>rollout_prompt_with_attacker(prompts: Sequence[str]) -&gt; Sequence[str]</code></li> <li><code>rollout_prompt_with_target(prompts: Sequence[str]) -&gt; Sequence[str]</code></li> <li><code>reward(prompts, attacks, responses) -&gt; Sequence[float]</code></li> <li><code>advance(prompt, attack, response) -&gt; str</code> (builds the next state)</li> </ul> <p>These APIs are vectorized\u2014pass lists (even length-1) for simplicity and speed.</p>"},{"location":"tutorials/customize_training/environments.html#3-best-practices-sanity-checks","title":"3. Best Practices &amp; Sanity Checks","text":"<ul> <li>Prefer batch calls. Avoid per-item model calls in tight loops.</li> <li>Determinism. Thread a <code>seed</code> through <code>rollout()</code> for reproducibility.</li> <li>Depth/width off-by-one. Depth <code>0</code> should produce no nodes; verify expected leaf counts.</li> <li>Always use <code>advance()</code>. Even if it\u2019s simple concatenation now, it future-proofs your code.</li> <li>Context length. Let the <code>Problem</code> handle truncation; the environment shouldn\u2019t assume token limits.</li> <li>Check your environment by printing a few rollouts.</li> </ul> <p><code>python   g = env.rollout(seed=7)   print(\"ROOT:\", g.context)   for i, n in enumerate(g.children):       print(f\"[{i}] atk={n.attack!r} rsp={n.response!r} \"             f\"rew={n.reward:.3f} children={len(n.children)}\")</code></p>"},{"location":"tutorials/customize_training/environments.html#4-how-tos","title":"4. How-Tos","text":""},{"location":"tutorials/customize_training/environments.html#41-create-a-custom-node-or-graph","title":"4.1 Create a custom Node or Graph","text":"<p>Need extra metadata (e.g., per-turn safety tags, KL terms, timestamps)? Subclass <code>Node</code> to take in additional data as shown below.</p> <pre><code>from typing import List, Iterable, Optional, Tuple\n\n# example custom node creation\nclass CustomNode(Node[str, str]):\n    \"\"\"\n    A Node with extra per-turn metadata for evaluation/training diagnostics.\n    Compatible anywhere a plain Node is expected (isinstance(CustomNode, Node) == True).\n    \"\"\"\n\n    def __init__(\n        self,\n        context: str,\n        attack: str,\n        response: str,\n        reward: float,\n        children: Sequence[\"Node[str, str]\"],\n        *,\n        attack_tox: float = 0.0,\n        target_tox: float = 0.0,\n        attack_logprob: float = 0.0,\n    ):\n        # Initialize the base Node fields first\n        super().__init__(context, attack, response, reward, list(children))\n        # Then attach your custom metrics\n        self.attack_tox: float = float(attack_tox)\n        self.target_tox: float = float(target_tox)\n        self.attack_logprob: float = float(attack_logprob)\n</code></pre> <p>Keep the original fields intact so existing solvers remain compatible.</p>"},{"location":"tutorials/customize_training/environments.html#42-change-rollout-widthdepth","title":"4.2 Change rollout width/depth","text":"<p>Adjust at environment construction time:</p> <pre><code>from astra_rl import ASTEnvironment\n\n# 4 attacker continuations per context, depth of 2 attacker\u2013defender turns\nenv = ASTEnvironment(problem, PROMPTS, tree_width=4, tree_depth=2)\n</code></pre> <p>Cost warning. Width \u00d7 depth increases compute and memory quickly\u2014scale carefully.</p>"},{"location":"tutorials/customize_training/environments.html#43-multi-agent-conversations-pseudocode","title":"4.3 Multi-agent conversations (pseudocode)","text":"<p>To support multi-agent conversations, you will need to query all participating models when building the rollout. This will occur in your environment (likely in modifications to __handle_rollout or rollout) but it is largely up to you on how you want to style it. For example, one approach would be to subclasses <code>ASTEnvironment</code> and override the internal expansion to call K defenders, combine their responses, and reduce rewards. However, you can structure how you call the agents and how you want to model rewards to fit your needs. </p> <p>If you maintain the same Node type (Node(context: str, attack: str, response: str, reward: float, children: List[Node])) and return a graph, you will be able to plug into available solvers. However, deviating from this structure may require you to create a custom solver to interpret the custom rollouts and calculate loss accordingly. </p> <p>Training with multiple agents multiplies compute: each node now triggers more model calls and stores more data.</p>"},{"location":"tutorials/customize_training/moderators.html","title":"How to Create a Custom Moderator Class","text":"<p>Moderators play a central role in LM red teaming, acting similarly to reward models in traditional reinforcement learning. Their job is to quantify the reward an adversarial agent receives for reaching a particular state\u2014typically by measuring how harmful or unsafe a target model's output is.</p> <p>In many RL-based red-teaming setups, the moderator provides the signal that trains the attacker to generate utterances that elicit harmful responses from a target model. This achieves the red-teaming objective by exposing weaknesses in the target model\u2019s safety alignment and highlighting where additional fine-tuning is needed.</p> <p>To serve this purpose, a moderator must: - Accept a sequence of target model generations (e.g., text), - Return a scalar score (e.g., a toxicity value from 0 to 1) indicating the level of harm.</p> <p>The astra-rl toolbox currently supports text-based moderation using: - Detoxify, for toxicity classification, - Llama Guard 3, for a variety of harm categories (e.g., hate speech, threats, etc.).</p> <p>But the framework is modular\u2014you can define your own moderator class, wrapping any model that takes in your defined StateT and ActionT types (see astra_rl/core/common) and returns a Sequence[float].</p> <p>This guide walks you through creating a new Moderator subclass.</p>"},{"location":"tutorials/customize_training/moderators.html#step-1-subclass-the-moderator-base-class","title":"Step 1: Subclass the Moderator Base Class","text":"<p>To define your own moderator, create a class that inherits from:</p> <pre><code>Moderator[StateT, ActionT]\n</code></pre> <p>Where: - StateT is the type of state your environment uses (e.g., a string prompt) - ActionT is the type of action your model produces (e.g., a generated response) For most NLP use cases, both StateT and ActionT are str.</p> <p>example:</p> <pre><code>from typing import Sequence\nfrom detoxify import Detoxify\nfrom astra_rl.core.moderator import Moderator\n\nclass DetoxifyModerator(Moderator[str, str]):\n    def __init__(self, harm_category: str = \"toxicity\", variant: str = \"original\"):\n        self.model = Detoxify(variant)\n        self.harm_category = harm_category\n</code></pre>"},{"location":"tutorials/customize_training/moderators.html#step-2-implement-the-moderate-method","title":"Step 2: Implement the moderate Method","text":"<p>You must implement the abstract method:</p> <pre><code>def moderate(self, x: Sequence[Union[StateT, ActionT]]) -&gt; Sequence[float]:\n</code></pre> <p>This method: - Takes a sequence of states and/or actions. - Returns a sequence of floats, where each float is the moderation score (e.g., toxicity score) for the corresponding input.</p> <p>example:</p> <pre><code>def moderate(self, x: Sequence[str]) -&gt; Sequence[float]:\n        return self.model.predict(x)[self.harm_category]\n</code></pre>"},{"location":"tutorials/customize_training/moderators.html#step-3-integrate-your-moderator","title":"Step 3: Integrate your moderator","text":"<p>Once your class is defined, you can plug it into the RL pipeline like any other component:</p> <pre><code>moderator = DetoxifyModerator(harm_category=\"insult\", variant=\"unbiased\")\nscores = moderator.moderate([\"you are stupid\", \"have a nice day!\"])\n</code></pre> <p>To train with your custom moderator, modify your problem subclass to instantiate it during initialization:</p> <p>example:</p> <pre><code>class ExampleDetoxifyProblem(ASTProblem):\n    def __init__(self, device=\"cpu\"):\n        # your choice of moderator\n        super().__init__(DetoxifyModerator()) ## Plug in your custom moderator here ##\n\n        self.device = device\n        self.attacker = GPT2LMHeadModel.from_pretrained(MODEL_NAME).to(self.device)\n        self.target = GPT2LMHeadModel.from_pretrained(MODEL_NAME).to(self.device)\n\n        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n        self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n</code></pre>"},{"location":"tutorials/customize_training/moderators.html#helpful-notes","title":"Helpful Notes:","text":"<ul> <li> <p>Your moderator can wrap any scoring model\u2014e.g., classifiers, LLMs, rule-based filters\u2014as long as it implements moderate(...) \u2192 Sequence[float].</p> </li> <li> <p>You can include internal logic to handle tokenization, batching, preprocessing, etc.</p> </li> <li> <p>Return one score per input in the same order as received.</p> </li> <li> <p>If you're using a library or model that scores multiple types of harm (like Detoxify or llamaguard), your class can expose a harm_category attribute to customize which score to extract.</p> </li> </ul>"},{"location":"tutorials/customize_training/moderators.html#full-examples","title":"Full examples:","text":"<p>See the following files for complete, working implementations: - astra_rl/moderators/detoxify.py \u2014 wraps the Detoxify library - astra_rl/moderators/llamaGuard.py \u2014 wraps Meta\u2019s Llama Guard 3 model</p>"},{"location":"tutorials/customize_training/problems.html","title":"How to Create a Custom Problem Class (HF or non-HF)","text":"<p>A Problem encapsulates models + tokenization + rollout + log-probabilities + rewards. Environments call your Problem to:</p> <ul> <li>sample attacker/target continuations,</li> <li>compute log-probs for losses (e.g., PPO/DPO),</li> <li>compute rewards (scalar or per-step).</li> </ul> <p>Most users can subclass <code>ASTProblem</code> (or <code>HFASTProblem</code> for HF). If you\u2019re integrating a custom / non-HF model, you\u2019ll implement the same small set of methods.</p>"},{"location":"tutorials/customize_training/problems.html#what-you-must-implement-api-contract","title":"What you must implement (API contract)","text":"<p>Your subclass must provide batched implementations (lists in, lists/tensors out), aligned by index:</p> <ul> <li><code>rollout_prompt_with_attacker(prompts: Sequence[str]) -&gt; Sequence[str]</code></li> <li><code>rollout_prompt_with_target(prompts: Sequence[str]) -&gt; Sequence[str]</code></li> <li><code>get_attacker_logprobs(contexts: Sequence[str], continuations: Sequence[str]) -&gt; torch.Tensor</code> (sum of token log-probs for each continuation conditioned on its context; shape <code>[B]</code>)</li> <li><code>get_target_logprobs(contexts, continuations) -&gt; torch.Tensor</code> (no grad)</li> <li><code>get_baseline_logprobs(contexts, continuations) -&gt; torch.Tensor</code> (no grad; often same as target/reference)</li> <li><code>parameters() -&gt; Iterable[torch.nn.Parameter]</code> (usually attacker\u2019s params only)</li> </ul> <p>Vectorize! Always pass lists (even of length 1) and do batched compute inside. This is faster and simpler.</p>"},{"location":"tutorials/customize_training/problems.html#todo-allie-add-details-on-how-to-create-a-custom-problem-class-that","title":"TODO: Allie add details on how to create a custom problem class that...","text":"<ul> <li>Works for non-HF models: user decides how to encode/decode/generate/forward.</li> <li>For non-HF models: you must also change how you are saving models during training. For more information see customize_training/trainers</li> <li>Keeps log-prob masking correct: compute <code>log P(continuation | context)</code> by masking out context tokens and summing over continuation tokens only.</li> <li>Preserves gradients for the attacker but uses <code>no_grad</code> for target/baseline, matching PPO/DPO style solvers.</li> </ul>"},{"location":"tutorials/customize_training/problems.html#huggingface-convenience","title":"HuggingFace convenience","text":"<p>If you are using HF, you can subclass our <code>HFASTProblem</code>. Quick notes:</p> <ul> <li>Use left padding + EOS as PAD for causal LMs to keep the rightmost tokens aligned.</li> <li>Keep <code>add_special_tokens=False</code> when encoding prompts that you\u2019ll later concatenate with generated text\u2014this avoids inserting extra BOS/SEP that break index alignment.</li> <li>Respect model context: truncate <code>context</code> to <code>max_ctx - max_new_tokens</code> before generation to avoid device-side index errors.</li> </ul>"},{"location":"tutorials/customize_training/problems.html#designing-a-custom-reward","title":"Designing a custom reward","text":"<p>Your <code>reward(contexts, attacks, responses)</code> returns a list of floats aligned with the batch. Common patterns:</p> <ul> <li>Toxicity-driven (maximize target toxicity):   <code>reward = w1 * tox(response) + w2 * tox(attack) - \u03bb * length(attack)</code></li> <li>Safety violations from a separate policy/guardrail model.</li> <li>Preference pairs: when using DPO/IPO/ORPO, you\u2019ll pass log-probs for preferred/dispreferred candidates to the solver; <code>reward</code> may be unused.</li> <li>Task-specific: factuality, jailbreak success, refusal suppression, etc.</li> </ul> <p>Tip: keep rewards bounded (e.g., clip to [-1, 1]) to stabilize PPO-style updates.</p>"},{"location":"tutorials/customize_training/problems.html#practical-gotchas","title":"Practical gotchas","text":"<ul> <li>Return log-probs, not probs. Only exponentiate if some downstream code explicitly asks for probabilities.</li> <li>Gradient locality. Let gradient flow through <code>get_attacker_logprobs</code>. Use <code>torch.no_grad()</code> for target/baseline calls.</li> <li>Batch always. Avoid per-item loops calling the model. Build padded batches.</li> <li>Context windows. Enforce <code>T \u2264 max_ctx</code>. For GPT-like models, left-truncate context and reserve space for the continuation.</li> <li>Tokenizer mismatch. If attacker/target tokenizers differ, do not cross-decode. Each model should encode/decode its own texts.</li> <li>Determinism. Accept a random seed (or RNG) in your env for reproducible rollouts.</li> </ul>"},{"location":"tutorials/customize_training/problems.html#when-to-use-astproblem-vs-hfastproblem","title":"When to use <code>ASTProblem</code> vs <code>HFASTProblem</code>","text":"<ul> <li>Non-HF or heavily customized model stack \u2192 subclass <code>ASTProblem</code>.</li> <li>HF Transformers \u2192 subclass <code>HFASTProblem</code> (less boilerplate) </li> </ul>"},{"location":"tutorials/customize_training/solvers.html","title":"How to Create a Custom Solver (RL Algorithm)","text":"<p>In ASTRA-RL, a solver subclasses <code>Algorithm[...]</code> and implements three things:</p> <ol> <li><code>flatten(graph)</code> \u2013 turn a rollout <code>Graph</code> into a list of per-sample Steps your solver trains on</li> <li><code>collate_fn(steps)</code> \u2013 batch those steps into a Batch the harness can load via a DataLoader</li> <li><code>step(batch)</code> \u2013 compute the training loss (and log metrics), returning <code>(loss, logs)</code></li> </ol> <p>This isolates \u201chow we learn\u201d (the solver) from \u201chow data is collected\u201d (the Environment) and \u201chow models run/interact\u201d (the Problem).</p>"},{"location":"tutorials/customize_training/solvers.html#choose-your-data-contract","title":"Choose your data contract","text":"<p>First, define your own <code>@dataclass Step</code>/<code>Batch</code> to match your algorithm. These dataclasses determine what information is stored at each step  of learning and hold the information across a total batch. </p> <p>Example: </p> <pre><code>from dataclasses import dataclass\nfrom typing import Any, Dict, Generic, List, Sequence, Tuple\nimport torch\n\nfrom astra_rl.core.algorithm import Algorithm\nfrom astra_rl.core.common import StateT, ActionT\nfrom astra_rl.core.environment import Graph\n\n# 1) Define your per-sample record\n@dataclass\nclass MyStep(Generic[StateT, ActionT]):\n    context: StateT\n    action: ActionT\n    reward: float  # or returns/advantages/etc.\n\n# 2) Define your batch\n@dataclass\nclass MyBatch(Generic[StateT, ActionT]):\n    contexts: Sequence[StateT]\n    actions: Sequence[ActionT]\n    rewards: torch.Tensor  # torch for math\n</code></pre>"},{"location":"tutorials/customize_training/solvers.html#creating-a-custom-solver","title":"Creating a custom Solver","text":"<p>TODO</p>"},{"location":"tutorials/customize_training/solvers.html#plugging-your-solver-into-the-harness","title":"Plugging your solver into the harness","text":"<p>To integrate the solver, simply instantiate it in your main code and give it to the Trainer as a parameter. </p> <pre><code>solver = MyAlgo(problem, kl_coeff=0.02)\ntrainer = Trainer(config=config, environment=env, algorithm=solver)\n</code></pre>"},{"location":"tutorials/customize_training/solvers.html#debug-checklist","title":"Debug checklist","text":"<ul> <li>Shapes: <code>flatten</code> produces a list; <code>collate_fn</code> returns tensors/seqs with consistent lengths.</li> <li>Gradient flow: only <code>get_attacker_logprobs</code> should require grad; keep target/baseline calls in <code>torch.no_grad()</code>.</li> <li>Rewards: are finite and roughly bounded; consider normalization or clipping.</li> <li>KL anchor: if training diverges, raise <code>kl_coeff</code> (or use adaptive control).</li> <li>Tree width: preference algorithms need <code>tree_width \u2265 2</code>; reward methods are simplest with <code>tree_width = 1</code>.</li> <li>Determinism: seed env rollouts to reproduce a bug.</li> </ul>"},{"location":"tutorials/customize_training/solvers.html#adapting-to-your-data","title":"Adapting to your data","text":"<ul> <li>Per-turn rewards? Put them into <code>MyStep</code> and compute returns (discounted sum) in <code>flatten</code> or <code>collate_fn</code>.</li> <li>Off-policy data? Add <code>old_logp</code> to <code>MyStep</code> and implement importance sampling/clip.</li> <li>Extra features (toxicity, length, success flags)? Add fields to <code>MyStep</code> and use them in the loss.</li> </ul>"},{"location":"tutorials/customize_training/trainers.html","title":"How to create a custom TrainingConfiguration and Trainer (ASTRA-RL)","text":""},{"location":"tutorials/customize_training/trainers.html#what-the-trainer-does","title":"What the <code>Trainer</code> does","text":"<p><code>astra_rl/training/trainer.py</code> (the base Trainer class) is responsible for:</p> <ol> <li>Instantiating the optimizer that updates the attacker\u2019s policy to reduce training loss.</li> <li>Instantiating the training harness (handles online data collection; you normally don\u2019t edit this).</li> <li>Running the main training loop in <code>train()</code> using your config hyperparameters.    Note: the base loop does not perform evaluation, model saving, or logging.</li> </ol> <p>The harness calls your Algorithm on each batch and returns <code>(loss, step_logs)</code>. The base <code>Trainer</code> uses the loss for optimization and discards <code>step_logs</code>. Use 'HFASTTrainer' or override <code>train()</code> if you want logging/eval/checkpointing.</p>"},{"location":"tutorials/customize_training/trainers.html#tldr-decision-guide","title":"TL;DR decision guide","text":"<ul> <li>Just train, no eval/checkpointing? Use the base <code>Trainer</code> (Sections 1 &amp; 2).</li> <li>Want periodic dev eval + HF checkpoints? Use <code>HFASTTrainer</code> / <code>HFASTConfiguration</code> (Sections 1 &amp; 3).</li> <li>Need a custom loop/scheduler/saving policy? Subclass <code>Trainer</code> (Section 4).</li> </ul>"},{"location":"tutorials/customize_training/trainers.html#1-define-your-trainingconfiguration-hyperparameters","title":"1) Define your TrainingConfiguration (hyperparameters)","text":"<p>Instantiate <code>TrainingConfiguration</code> with the knobs you care about. These values drive how the base trainer is initialized.</p> <pre><code>from astra_rl import Trainer, TrainingConfiguration\n\nconfig = TrainingConfiguration(\n    lr=1e-5,\n    batch_size=4,\n    optimizer=\"adamw\",                 # [\"adam\", \"adamw\", \"sgd\", \"rmsprop\", \"adagrad\"]\n    gradient_accumulation_steps=1,     # optimizer.step() every N backwards\n    training_steps=1000,               # number of calls to experience()\n    num_episodes_per_experience=2,     # rollouts per experience() call\n)\n</code></pre> <p>Note: <code>training_steps</code> = number of times we call <code>experience()</code>; Total rollouts \u2248 <code>training_steps \u00d7 num_episodes_per_experience</code>.</p>"},{"location":"tutorials/customize_training/trainers.html#2-fast-path-a-use-the-base-trainer-as-is","title":"2) Fast path A \u2014 use the base <code>Trainer</code> as-is","text":"<p>If you\u2019re happy with a simple loop that optimizes on the algorithm loss and does not eval, save, or log:</p> <pre><code>trainer = Trainer(config=config, environment=env, algorithm=solver)\ntrainer.train()  # base loop; no eval/checkpointing/logging\n</code></pre> <p>To save the final model afterward:</p> <pre><code>problem.attacker.save_pretrained(\"final_ckpt\")\nproblem.tokenizer.save_pretrained(\"final_ckpt\")\n</code></pre>"},{"location":"tutorials/customize_training/trainers.html#3-fast-path-b-use-the-hf-compatible-trainer-with-periodic-eval-saving","title":"3) Fast path B \u2014 use the HF-compatible trainer with periodic eval &amp; saving","text":"<p>Preconfigured subclass that runs periodic dev evaluation and saves Hugging Face checkpoints.</p> <pre><code>from astra_rl.ext.transformers.hf_ast_problem import (\n    HFASTTrainer,\n    HFASTConfiguration,\n)\n\nconfig  = HFASTConfiguration()  # or use your own TrainingConfiguration\ntrainer = HFASTTrainer(\n    config=config,\n    environment=env,\n    algorithm=solver,\n    dev_prompts=DEV_PROMPTS,   # iterable of prompts\n    eval_every=100,            # run dev eval every N steps\n    ckpt_dir=\"checkpoints\",    # where to save HF checkpoints\n)\ntrainer.train()\n</code></pre>"},{"location":"tutorials/customize_training/trainers.html#4-fully-custom-write-your-own-trainer-subclass","title":"4) Fully custom \u2014 write your own <code>Trainer</code> subclass","text":"<p>Use this when you need custom evaluation cadence, checkpointing, logging, LR scheduling, early stopping, etc. You may override <code>train()</code>; if you don\u2019t, you inherit the base implementation.</p> <pre><code>import os\nimport torch\nfrom astra_rl import Trainer, TrainingConfiguration\nfrom astra_rl.logging import logger\n\nclass MyConfig(TrainingConfiguration):\n    def __init__(self):\n        super().__init__(\n            lr=1e-5,\n            batch_size=4,\n            optimizer=\"adamw\",\n            gradient_accumulation_steps=1,\n            training_steps=1000,\n            num_episodes_per_experience=2,\n        )\n        # Custom fields for your subclass:\n        self.eval_every = 100\n        self.ckpt_dir = \"checkpoints\"\n\nclass MyTrainer(Trainer):\n    \"\"\"\n    Extends the base trainer with:\n      - periodic dev-set evaluation\n      - HF-format checkpointing\n      - optional grad accumulation\n    \"\"\"\n\n    def __init__(self, config: MyConfig, environment, algorithm, dev_prompts=None):\n        super().__init__(config, environment, algorithm)\n        self.dev_prompts = dev_prompts or []\n        os.makedirs(self.config.ckpt_dir, exist_ok=True)\n\n    # optional but encouraged\n    def _save_hf(self, step: int) -&gt; None:\n        \"\"\"Save your attacker and its tokenizer\"\"\"\n\n    # optional method\n    @torch.no_grad()\n    def _eval_dev(self, step: int, tag: str = \"dev\"):\n        \"\"\"Run a lightweight evaluation on dev prompts. Fill in your logic.\"\"\"\n        pass\n\n    # required method\n    def train(self):\n        \"\"\"Implement your custom training loop!\"\"\"\n        # see astra_rl.ext.transformers.hf_ast_problem for an implemented custom class example\n        pass        \n</code></pre>"},{"location":"tutorials/customize_training/trainers.html#notes-best-practices","title":"Notes &amp; best practices","text":"<ul> <li>Don\u2019t modify the Harness unless you\u2019re sure you can't change data-collection semantics though changes to the trainer/config/environment/algorithm.</li> <li>Logging: only log scalars (detach tensors) to avoid retaining graphs: <code>logs[\"loss\"] = float(loss.detach().item())</code>.</li> <li>Checkpoints: saving the attacker/tokenizer is usually sufficient; if your algorithm maintains extra state, save that too.</li> <li>Imports: to use the HF trainer directly,   <code>from astra_rl.ext.transformers.hf_ast_problem import HFASTTrainer, HFASTConfiguration</code>.</li> </ul>"}]}
{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Adaptive Stress Testing for Robust AI &amp; Reinforcement Learning (ASTRA-RL)","text":"<p>Welcome to the ASTRA-RL toolbox documentation! This documentation provides an overview of the ASTRA-RL toolbox, its features, and how to use it effectively.</p> <p>Warning</p> <p>This documentation (and project) is a work in progress. If you have any questions or suggestions, please feel free to open an issue on the GitHub repository</p>"},{"location":"api/index.html","title":"Library API","text":"<p>This section provides detailed information about all the classes, functions, and modules available in the ASTRA-RL toolbox. Each entry includes a description of its purpose, parameters, and usage examples.</p> <p>This documentation is generated automatically from the codebase using docstrings and comments, ensuring that it stays up-to-date with the latest changes.</p>"},{"location":"tutorials/index.html","title":"Tutorials","text":"<p>This section provides step-by-step guides and examples to help you get started with the ASTRA-RL toolbox. Each tutorial covers a specific aspect of the toolbox, from basic usage to advanced features.</p>"},{"location":"tutorials/quick_start_training.html","title":"Quick Start: Training a HuggingFace Attacker with ASTRA-RL","text":"<p>Do you want to train a HuggingFace attacker using an ASTRA-supported algorithm (e.g., DPO, IPO, PPO) and problem formulation (ASTPrompter, RL - Perez*, MALIBU*, CRT*)?</p> <p>Then this guide is for you. We\u2019ll walk through every step required to train a red-teaming attacker using our pre-configured classes and point you to customization guides when your use case goes beyond the defaults. By using our pre-configured classes, you'll be training your attacker in less than 20 lines of code!</p>"},{"location":"tutorials/quick_start_training.html#step-1-setup","title":"Step 1: Setup","text":"<p>Please see the README for full setup instructions. Here's a quick recap:</p> <pre><code># Install the ASTRA-RL toolbox\npip install astra-rl\n\n# Clone the repository (for examples and development)\ngit clone git@github.com:sisl/astra-rl.git\n</code></pre> <p>Note: wandb is not automatically installed during this process. If you would let to use wandb (supported), install it (uv pip install wandb) and run export WANDB_API_KEY=###your_wandb_api_key##### in your terminal.</p>"},{"location":"tutorials/quick_start_training.html#step-2-import-required-modules-and-set-device","title":"Step 2: Import Required Modules and set device","text":"<pre><code>from torch.optim import AdamW\n\n# ASTRA-RL core components\nfrom astra_rl import ASTEnvironment, DPO, DetoxifyModerator, Harness\n\n# HuggingFace-friendly problem wrapper for ASTPrompter-style red teaming\nfrom astra_rl.ext.transformers import HFASTProblem\nfrom astra_rl.training import Trainer, TrainingConfiguration\n\nDEVICE = \"cuda\"  # or \"cpu\" if GPU is not available\n</code></pre> <p>We support both lightweight (e.g., GPT-2 + Detoxify) and heavyweight (e.g., LLaMA + LlamaGuard) setups. Pick model and moderator sizes that fit your compute!</p>"},{"location":"tutorials/quick_start_training.html#step-3-load-your-initial-prompts","title":"Step 3: Load Your Initial Prompts","text":"<p>To train an attacker, you\u2019ll need a list of comma-separated strings that act as initial prompts\u2014these initiate attacker-target rollouts used for online training. </p> <pre><code>import json\n\nwith open(\"prompts_reddit_train.json\") as f:\n    PROMPTS = json.load(f)\n</code></pre> <p>Since ASTPrompter red-teams for harmful outputs in conversational settings, it uses the ConvoKit Reddit Small Corpus (filtered for proper formatting and for non-toxicity using Detoxify) as its default source of initial prompts. This data can be found in astra-rl/examples/GPT2_v_GPT2/.</p> <p>The ASTRA-RL toolbox easily supports external prompt datasets or APIs\u2014just ensure the final PROMPTS variable is formatted as a list of strings.</p>"},{"location":"tutorials/quick_start_training.html#step-4-instantiate-your-problem","title":"Step 4: Instantiate Your Problem","text":"<p>The problem is an important component of training that handles rollout step generation, reward computation, and log-probability calculation. To speed you along, we have implemented the <code>HFASTProblem</code> class that handles the technical backend so you just need to provide the huggingface model IDs of the attacker, target and baseline models and a <code>Moderator</code> instance (DetexifyModerator(), LlamaGuardModerator() or your custom moderator).</p> <p>Note: Your attacker and target can be different models (e.g., GPT-2 attacker, LLaMA target) but your baseline and attacker should typically be the same.</p> <pre><code>from astra_rl.modifiers import LlamaGuardModerator  # optional\n\n# Example problem instantiation: GPT2 attacker, target, and baseline with Detoxify moderator (lightweight setup)\nproblem = HFASTProblem(\"gpt2\", \"gpt2\", \"gpt2\", DetoxifyModerator(), DEVICE)\n</code></pre> <p>Need a custom model or rollout step logic? See customize_training/problems</p> <p>Want to use a custom moderator? See customize_training/moderators</p>"},{"location":"tutorials/quick_start_training.html#step-5-instantiate-the-environment","title":"Step 5: Instantiate the Environment","text":"<p>The environment defines how training rollouts are structured and collected. In ASTRA-RL, the default is the <code>ASTEnvironment</code>, which implements the conversation tree rollout used in the ASTPrompter paper.</p> <pre><code>env = ASTEnvironment(problem, PROMPTS)\n</code></pre> Curious about how this environment structures rollouts?    This environment builds a tree-structured conversation graph, where:   - The root node starts from a random initial prompt (from `PROMPTS`)   - At each turn, the attacker generates multiple (`tree_width`, default 2) candidate utterances   - Each of those utterances is fed to the target model, which produces a response   - The resulting attacker\u2013target\u2013response tuples form child nodes   - This process repeats for `tree_depth` levels (default 3), yielding a multi-turn attacker\u2013target dialogue tree    This structure enables preference-based learning algorithms like DPO and IPO to reason over multiple conversational branches at once, training the attacker to elicit harmful responses in a multi-turn setting.  <p>By default, rollouts are configured with tree_width=2 and tree_depth=3, but you can customize both:</p> <pre><code>env = ASTEnvironment(problem, PROMPTS, tree_width=4, tree_depth=5)\n</code></pre> <p>Want a different rollout graph structure or a multi-agent setup? See customize_training/environments</p>"},{"location":"tutorials/quick_start_training.html#step-6-choose-your-algorithm-and-optimizer","title":"Step 6: Choose Your Algorithm and Optimizer","text":"<p>The solver is the RL learning algorithm that will take in a graph of training rollouts and compute the loss. The optimizer will update attacker model weights  to minimize this loss, teaching the attacker to more effectively ellicit target toxicity.</p> <p>We use DPO and Adam as the default for this quickstart.</p> <pre><code>solver = DPO(problem)\noptimizer = AdamW(problem.parameters(), lr=1e-5)\n</code></pre> <p>To integrate your own RL algorithm, see customize_training/solvers</p>"},{"location":"tutorials/quick_start_training.html#step-7-create-the-training-harness","title":"Step 7: Create the Training Harness","text":"<p>The training harness wires your environment and solver together. It collects online experience and, for each batch, invokes the solver to compute the loss used to update the attacker\u2019s policy. You typically won\u2019t need to modify the harness code itself\u2014adjust behavior via the harness parameters, environment, solver, or your outer training loop (e.g., schedules, logging, hyperparameters).</p> <pre><code>harness = Harness(\n    env,\n    solver,\n    num_episodes_per_experience=2,\n    use_wandb=True,\n    dataloader_kwargs={\"batch_size\": 4},\n)\n</code></pre>"},{"location":"tutorials/quick_start_training.html#step-8-train-the-attacker","title":"Step 8: Train the Attacker","text":"<p>Choose how many training steps/epochs to run. You have full control over the training loop\u2014curriculum, evaluation cadence, checkpointing, learning-rate schedules, gradient clipping, and more.</p> <pre><code>for step in range(1000):\n    # Collect experience rollouts from attacker-target interactions\n    buf = harness.experience()\n    for experience in buf:\n        # Compute loss using the solver (e.g., DPO)\n        loss, step_logs = harness.step(experience)\n\n        # Standard PyTorch optimization\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        step_logs[\"step\"] = step\n        harness.log_current_step(step_logs)\n</code></pre>"},{"location":"tutorials/quick_start_training.html#full-example-examplesast_hfpy","title":"Full Example: examples/ast_hf.py","text":"<p>We provide a complete working example that mirrors this guide!</p>"},{"location":"tutorials/running_an_evaluation.html","title":"Running An Evaluation","text":"<p>This tutorial will guide you through the process of running an evaluation using an existing evaluator model. The evaluator is a model that can be used to assess the quality of other models' outputs. </p> <p>For the purposes of this tutorial, we will assume that you have already trained an evaluator model from the Training an Evaluator tutorial.mk</p>"},{"location":"tutorials/customize_training/envirnoments.html","title":"How to Customize the Environment","text":"<p>The environment defines how attacker\u2013target interactions are produced and packaged for training. It controls:</p> <ul> <li>how rollouts are generated (single path vs. tree),</li> <li>how states advance across turns,</li> <li>what per-step data (actions, responses, rewards) is stored,</li> <li>and what the solver receives as input.</li> </ul> <p>ASTRA-RL ships with <code>ASTEnvironment</code>, which mirrors the rollout structure used in ASTPrompter. You can subclass it\u2014or the base <code>Environment</code>\u2014to support:</p> <ul> <li>multi-agent or multi-turn conversations,</li> <li>tree-structured or flat trajectories,</li> <li>custom state-advance logic,</li> <li>alternative reward shaping or logging.</li> </ul>"},{"location":"tutorials/customize_training/envirnoments.html#option-a-subclass-the-base-environment","title":"Option A: Subclass the base <code>Environment</code>","text":"<pre><code>from astra_rl.core.environment import Environment\n\nclass MyCustomEnvironment(Environment[str, str]):\n    ...\n</code></pre> <p>The base <code>Environment</code> defines the interface the training harness expects. Make sure you implement the mandatory rollout() method.</p>"},{"location":"tutorials/customize_training/envirnoments.html#option-b-subclass-an-existing-environment","title":"Option B: Subclass an existing environment","text":"<p>If your rollout logic is \u201clike AST but with a twist,\u201d subclass <code>ASTEnvironment</code> and override only the parts you need (e.g., the expansion logic, evaluation, or metrics extraction).</p> <pre><code>from astra_rl import ASTProblem, ASTEnvironment \nfrom astra_rl.core.environment import Node, Graph\n\nclass MyASTVariant(ASTEnvironment):\n    \"\"\"\n    Subclass of ASTEnvironment that performs your custom rollout logic\n    \"\"\"\n\n    def __init__(...):\n        super().__init__(problem, prompts, tree_width, tree_depth)\n</code></pre>"},{"location":"tutorials/customize_training/envirnoments.html#required-implement-rollout","title":"Required: implement <code>rollout(...)</code>","text":"<p>Your environment must implement:</p> <pre><code>def rollout(self, seed: Optional[int] = None) -&gt; Graph[str, str]:\n    ...\n</code></pre> <p>This function performs one training rollout and returns a <code>Graph</code> made of <code>Node</code>s:</p> <ul> <li><code>Graph(context: str, children: List[Node])</code></li> <li><code>Node(context: str, attack: str, response: str, reward: float, children: List[Node])</code></li> </ul> <p>At a minimum, each node should capture:</p> <ul> <li><code>context</code> (state so far, e.g., conversation text),</li> <li><code>attack</code> (attacker\u2019s utterance / action),</li> <li><code>response</code> (target/defender\u2019s reply),</li> <li><code>reward</code> (float),</li> <li><code>children</code> (next steps; empty for leaf nodes).</li> </ul> <p>Tip: Make sure your rollout matches your solver\u2019s data contract.     Preference-based methods (e.g., DPO/IPO/ORPO) need preference pairs, so the easiest online setup is tree_width \u2265 2 to sample two candidates per prompt. Reward-based policy-gradient methods (e.g., PPO/A2C) do not require pairs; tree_width = 1 is typical.</p>"},{"location":"tutorials/customize_training/envirnoments.html#useful-astproblem-helpers-inside-your-environment","title":"Useful <code>ASTProblem</code> helpers inside your environment","text":"<p>These are the public, batch-friendly methods you\u2019ll typically call during rollout:</p> <ul> <li><code>rollout_prompt_with_attacker(prompts: Sequence[str]) -&gt; Sequence[str]</code></li> <li><code>rollout_prompt_with_target(prompts: Sequence[str]) -&gt; Sequence[str]</code></li> <li><code>reward(prompts, attacks, responses) -&gt; Sequence[float]</code></li> <li><code>advance(prompt, attack, response) -&gt; str</code> (returns the next state)</li> </ul> <p>Tip: These APIs are vectorized\u2014always pass lists (even if length 1) for best performance and simpler code.</p>"},{"location":"tutorials/customize_training/envirnoments.html#optional-evaluation-metrics","title":"(Optional) Evaluation metrics","text":"<p>If you want per-turn metrics like toxicity or likelihood, add a helper that walks a single-path graph and computes metrics at each turn. These helper functions can be useful when evaluating models after training. See the quick_start_evaluation guide for more information. </p> <p>Tip: Use the moderator in batch mode: <code>moderator.moderate(list_of_texts)</code>.</p>"},{"location":"tutorials/customize_training/envirnoments.html#best-practices-common-pitfalls","title":"Best practices &amp; common pitfalls","text":"<ul> <li>Prefer batch calls. The <code>ASTProblem</code> helpers are vectorized\u2014avoid per-item model calls inside tight loops.</li> <li>Determinism: Thread a <code>seed</code> through <code>rollout()</code> for reproducible experiments.</li> <li>Depth/width off-by-one: Depth <code>= 0</code> should produce no nodes; verify leaf counts match expectations.</li> <li>State advancement: Always use <code>advance(state, attack, response)</code> to build the next state, even if it\u2019s simple string concatenation\u2014this keeps things consistent and testable.</li> <li>Graph sanity checks: Before training, print or visualize one <code>rollout()</code> to confirm the structure matches your mental model (fan-out, depth, rewards present).</li> <li>Context length: If your <code>Problem</code> uses LMs with max context, ensure your <code>Problem</code> handles truncation. Environments should not assume a specific token limit.</li> <li>Evaluation vs. training: Keep evaluation single-path (<code>width=1</code>) to simplify metrics and avoid combinatorial explosion.</li> </ul>"},{"location":"tutorials/customize_training/envirnoments.html#quick-validation-checklist","title":"Quick validation checklist","text":"<ol> <li><code>env.rollout()</code> returns a <code>Graph</code> with the expected shape (root + child nodes).</li> <li>Every <code>Node</code> has <code>context</code>, <code>attack</code>, <code>response</code>, <code>reward</code> populated.</li> <li><code>advance()</code> is called exactly once per (context, attack, response).</li> <li>Running two <code>rollout(seed=123)</code> calls yields the same graph.</li> <li><code>final_reward()</code> returns the last-step reward in a single-path rollout.</li> <li>(If added) <code>eval_rollout(prompt)</code> respects <code>width=1</code> and your metrics extractor runs without per-item model calls.</li> </ol>"},{"location":"tutorials/customize_training/envirnoments.html#example-printing-a-small-rollout","title":"Example: printing a small rollout","text":"<pre><code>g = env.rollout(seed=7)\nprint(\"ROOT:\", g.context)\nfor i, n in enumerate(g.children):\n    print(f\"[{i}] atk={n.attack!r} rsp={n.response!r} rew={n.reward:.3f} #children={len(n.children)}\")\n</code></pre>"},{"location":"tutorials/customize_training/envirnoments.html#extending-the-data-model","title":"Extending the data model","text":"<p>Need extra metadata (e.g., per-turn safety tags, KL terms, or timestamps)? Subclass <code>Node</code> and/or wrap <code>Graph</code> with your own dataclasses, then adapt your solver to read those fields.</p>"},{"location":"tutorials/customize_training/moderators.html","title":"How to Create a Custom Moderator Class","text":"<p>Moderators play a central role in LM red teaming, acting similarly to reward models in traditional reinforcement learning. Their job is to quantify the reward an adversarial agent receives for reaching a particular state\u2014typically by measuring how harmful or unsafe a target model's output is.</p> <p>In many RL-based red-teaming setups, the moderator provides the signal that trains the attacker to generate utterances that elicit harmful responses from a target model. This achieves the red-teaming objective by exposing weaknesses in the target model\u2019s safety alignment and highlighting where additional fine-tuning is needed.</p> <p>To serve this purpose, a moderator must: - Accept a sequence of target model generations (e.g., text), - Return a scalar score (e.g., a toxicity value from 0 to 1) indicating the level of harm.</p> <p>The astra-rl toolbox currently supports text-based moderation using: - Detoxify, for toxicity classification, - Llama Guard 3, for a variety of harm categories (e.g., hate speech, threats, etc.).</p> <p>But the framework is modular\u2014you can define your own moderator class, wrapping any model that takes in your defined StateT and ActionT types (see astra_rl/core/common) and returns a Sequence[float].</p> <p>This guide walks you through creating a new Moderator subclass.</p>"},{"location":"tutorials/customize_training/moderators.html#step-1-subclass-the-moderator-base-class","title":"Step 1: Subclass the Moderator Base Class","text":"<p>To define your own moderator, create a class that inherits from:</p> <pre><code>Moderator[StateT, ActionT]\n</code></pre> <p>Where: - StateT is the type of state your environment uses (e.g., a string prompt) - ActionT is the type of action your model produces (e.g., a generated response) For most NLP use cases, both StateT and ActionT are str.</p> <p>example:</p> <pre><code>from typing import Sequence\nfrom detoxify import Detoxify\nfrom astra_rl.core.moderator import Moderator\n\nclass DetoxifyModerator(Moderator[str, str]):\n    def __init__(self, harm_category: str = \"toxicity\", variant: str = \"original\"):\n        self.model = Detoxify(variant)\n        self.harm_category = harm_category\n</code></pre>"},{"location":"tutorials/customize_training/moderators.html#step-2-implement-the-moderate-method","title":"Step 2: Implement the moderate Method","text":"<p>You must implement the abstract method:</p> <pre><code>def moderate(self, x: Sequence[Union[StateT, ActionT]]) -&gt; Sequence[float]:\n</code></pre> <p>This method: - Takes a sequence of states and/or actions. - Returns a sequence of floats, where each float is the moderation score (e.g., toxicity score) for the corresponding input.</p> <p>example:</p> <pre><code>def moderate(self, x: Sequence[str]) -&gt; Sequence[float]:\n        return self.model.predict(x)[self.harm_category]\n</code></pre>"},{"location":"tutorials/customize_training/moderators.html#step-3-integrate-your-moderator","title":"Step 3: Integrate your moderator","text":"<p>Once your class is defined, you can plug it into the RL pipeline like any other component:</p> <pre><code>moderator = DetoxifyModerator(harm_category=\"insult\", variant=\"unbiased\")\nscores = moderator.moderate([\"you are stupid\", \"have a nice day!\"])\n</code></pre> <p>To train with your custom moderator, modify your problem subclass to instantiate it during initialization:</p> <p>example:</p> <pre><code>class ExampleDetoxifyProblem(ASTProblem):\n    def __init__(self, device=\"cpu\"):\n        # your choice of moderator\n        super().__init__(DetoxifyModerator()) ## Plug in your custom moderator here ##\n\n        self.device = device\n        self.attacker = GPT2LMHeadModel.from_pretrained(MODEL_NAME).to(self.device)\n        self.target = GPT2LMHeadModel.from_pretrained(MODEL_NAME).to(self.device)\n\n        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n        self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n</code></pre>"},{"location":"tutorials/customize_training/moderators.html#helpful-notes","title":"Helpful Notes:","text":"<ul> <li> <p>Your moderator can wrap any scoring model\u2014e.g., classifiers, LLMs, rule-based filters\u2014as long as it implements moderate(...) \u2192 Sequence[float].</p> </li> <li> <p>You can include internal logic to handle tokenization, batching, preprocessing, etc.</p> </li> <li> <p>Return one score per input in the same order as received.</p> </li> <li> <p>If you're using a library or model that scores multiple types of harm (like Detoxify or llamaguard), your class can expose a harm_category attribute to customize which score to extract.</p> </li> </ul>"},{"location":"tutorials/customize_training/moderators.html#full-examples","title":"Full examples:","text":"<p>See the following files for complete, working implementations: - astra_rl/moderators/detoxify.py \u2014 wraps the Detoxify library - astra_rl/moderators/llamaGuard.py \u2014 wraps Meta\u2019s Llama Guard 3 model</p>"},{"location":"tutorials/customize_training/problems.html","title":"How to Create a Custom Problem Class (HF or non-HF)","text":"<p>A Problem encapsulates models + tokenization + rollout + log-probabilities + rewards. Environments call your Problem to:</p> <ul> <li>sample attacker/target continuations,</li> <li>compute log-probs for losses (e.g., PPO/DPO),</li> <li>compute rewards (scalar or per-step).</li> </ul> <p>Most users can subclass <code>ASTProblem</code> (or <code>HFASTProblem</code> for HF). If you\u2019re integrating a custom / non-HF model, you\u2019ll implement the same small set of methods.</p>"},{"location":"tutorials/customize_training/problems.html#what-you-must-implement-api-contract","title":"What you must implement (API contract)","text":"<p>Your subclass must provide batched implementations (lists in, lists/tensors out), aligned by index:</p> <ul> <li><code>rollout_prompt_with_attacker(prompts: Sequence[str]) -&gt; Sequence[str]</code></li> <li><code>rollout_prompt_with_target(prompts: Sequence[str]) -&gt; Sequence[str]</code></li> <li><code>get_attacker_logprobs(contexts: Sequence[str], continuations: Sequence[str]) -&gt; torch.Tensor</code> (sum of token log-probs for each continuation conditioned on its context; shape <code>[B]</code>)</li> <li><code>get_target_logprobs(contexts, continuations) -&gt; torch.Tensor</code> (no grad)</li> <li><code>get_baseline_logprobs(contexts, continuations) -&gt; torch.Tensor</code> (no grad; often same as target/reference)</li> <li><code>parameters() -&gt; Iterable[torch.nn.Parameter]</code> (usually attacker\u2019s params only)</li> </ul> <p>Vectorize! Always pass lists (even of length 1) and do batched compute inside. This is faster and simpler.</p>"},{"location":"tutorials/customize_training/problems.html#todo-allie-add-details-on-how-to-create-a-custom-problem-class-that","title":"TODO: Allie add details on how to create a custom problem class that...","text":"<ul> <li>Works for non-HF models: user decides how to encode/decode/generate/forward.</li> <li>Keeps log-prob masking correct: compute <code>log P(continuation | context)</code> by masking out context tokens and summing over continuation tokens only.</li> <li>Preserves gradients for the attacker but uses <code>no_grad</code> for target/baseline, matching PPO/DPO style solvers.</li> </ul>"},{"location":"tutorials/customize_training/problems.html#huggingface-convenience","title":"HuggingFace convenience","text":"<p>If you are using HF, you can subclass our <code>HFASTProblem</code>. Quick notes:</p> <ul> <li>Use left padding + EOS as PAD for causal LMs to keep the rightmost tokens aligned.</li> <li>Keep <code>add_special_tokens=False</code> when encoding prompts that you\u2019ll later concatenate with generated text\u2014this avoids inserting extra BOS/SEP that break index alignment.</li> <li>Respect model context: truncate <code>context</code> to <code>max_ctx - max_new_tokens</code> before generation to avoid device-side index errors.</li> </ul>"},{"location":"tutorials/customize_training/problems.html#designing-a-custom-reward","title":"Designing a custom reward","text":"<p>Your <code>reward(contexts, attacks, responses)</code> returns a list of floats aligned with the batch. Common patterns:</p> <ul> <li>Toxicity-driven (maximize target toxicity):   <code>reward = w1 * tox(response) + w2 * tox(attack) - \u03bb * length(attack)</code></li> <li>Safety violations from a separate policy/guardrail model.</li> <li>Preference pairs: when using DPO/IPO/ORPO, you\u2019ll pass log-probs for preferred/dispreferred candidates to the solver; <code>reward</code> may be unused.</li> <li>Task-specific: factuality, jailbreak success, refusal suppression, etc.</li> </ul> <p>Tip: keep rewards bounded (e.g., clip to [-1, 1]) to stabilize PPO-style updates.</p>"},{"location":"tutorials/customize_training/problems.html#practical-gotchas","title":"Practical gotchas","text":"<ul> <li>Return log-probs, not probs. Only exponentiate if some downstream code explicitly asks for probabilities.</li> <li>Gradient locality. Let gradient flow through <code>get_attacker_logprobs</code>. Use <code>torch.no_grad()</code> for target/baseline calls.</li> <li>Batch always. Avoid per-item loops calling the model. Build padded batches.</li> <li>Context windows. Enforce <code>T \u2264 max_ctx</code>. For GPT-like models, left-truncate context and reserve space for the continuation.</li> <li>Tokenizer mismatch. If attacker/target tokenizers differ, do not cross-decode. Each model should encode/decode its own texts.</li> <li>Determinism. Accept a random seed (or RNG) in your env for reproducible rollouts.</li> </ul>"},{"location":"tutorials/customize_training/problems.html#when-to-use-astproblem-vs-hfastproblem","title":"When to use <code>ASTProblem</code> vs <code>HFASTProblem</code>","text":"<ul> <li>Non-HF or heavily customized model stack \u2192 subclass <code>ASTProblem</code>.</li> <li>HF Transformers \u2192 subclass <code>HFASTProblem</code> (less boilerplate) </li> </ul>"},{"location":"tutorials/customize_training/solvers.html","title":"How to Create a Custom Solver (RL Algorithm)","text":"<p>In ASTRA-RL, a solver subclasses <code>Algorithm[...]</code> and implements three things:</p> <ol> <li><code>flatten(graph)</code> \u2013 turn a rollout <code>Graph</code> into a list of per-sample Steps your solver trains on</li> <li><code>collate_fn(steps)</code> \u2013 batch those steps into a Batch the harness can load via a DataLoader</li> <li><code>step(batch)</code> \u2013 compute the training loss (and log metrics), returning <code>(loss, logs)</code></li> </ol> <p>This isolates \u201chow we learn\u201d (the solver) from \u201chow data is collected\u201d (the Environment) and \u201chow models run/interact\u201d (the Problem).</p>"},{"location":"tutorials/customize_training/solvers.html#choose-your-data-contract","title":"Choose your data contract","text":"<p>First, define your own <code>@dataclass Step</code>/<code>Batch</code> to match your algorithm. These dataclasses determine what information is stored at each step  of learning and hold the information across a total batch. (TODO: improve)</p>"},{"location":"tutorials/customize_training/solvers.html#example","title":"example","text":"<pre><code>from dataclasses import dataclass\nfrom typing import Any, Dict, Generic, List, Sequence, Tuple\nimport torch\n\nfrom astra_rl.core.algorithm import Algorithm\nfrom astra_rl.core.common import StateT, ActionT\nfrom astra_rl.core.environment import Graph\n\n# 1) Define your per-sample record\n@dataclass\nclass MyStep(Generic[StateT, ActionT]):\n    context: StateT\n    action: ActionT\n    reward: float  # or returns/advantages/etc.\n\n# 2) Define your batch\n@dataclass\nclass MyBatch(Generic[StateT, ActionT]):\n    contexts: Sequence[StateT]\n    actions: Sequence[ActionT]\n    rewards: torch.Tensor  # torch for math\n</code></pre>"},{"location":"tutorials/customize_training/solvers.html#plugging-your-solver-into-the-harness","title":"Plugging your solver into the harness","text":"<p>To integrate the solver, simply instantiate in your main code and give it to the harness as a parameter. Your solver should provide the harness with a list of steps. </p> <pre><code>solver = MyAlgo(problem, kl_coeff=0.02)\nharness = Harness(\n    env,\n    solver,\n    num_episodes_per_experience=2,\n    dataloader_kwargs={\"batch_size\": 8, \"shuffle\": True},\n)\nfor step in range(1000):\n    for batch in harness.experience():     # yields MyBatch via your collate_fn\n        loss, logs = harness.step(batch)   # calls solver.step(...)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        logs[\"step\"] = step\n        harness.log_current_step(logs)\n</code></pre>"},{"location":"tutorials/customize_training/solvers.html#debug-checklist","title":"Debug checklist","text":"<ul> <li>Shapes: <code>flatten</code> produces a list; <code>collate_fn</code> returns tensors/seqs with consistent lengths.</li> <li>Gradient flow: only <code>get_attacker_logprobs</code> should require grad; keep target/baseline calls in <code>torch.no_grad()</code>.</li> <li>Rewards: are finite and roughly bounded; consider normalization or clipping.</li> <li>KL anchor: if training diverges, raise <code>kl_coeff</code> (or use adaptive control).</li> <li>Tree width: preference algorithms need <code>tree_width \u2265 2</code>; reward methods are simplest with <code>tree_width = 1</code>.</li> <li>Determinism: seed env rollouts to reproduce a bug.</li> </ul>"},{"location":"tutorials/customize_training/solvers.html#adapting-to-your-data","title":"Adapting to your data","text":"<ul> <li>Per-turn rewards? Put them into <code>MyStep</code> and compute returns (discounted sum) in <code>flatten</code> or <code>collate_fn</code>.</li> <li>Off-policy data? Add <code>old_logp</code> to <code>MyStep</code> and implement importance sampling/clip.</li> <li>Extra features (toxicity, length, success flags)? Add fields to <code>MyStep</code> and use them in the loss.</li> </ul>"},{"location":"tutorials/customize_training/trainers.html","title":"Trainers","text":"<p>The Trainer Class is defined at astra_rl/training/trainer.py . It has 3 core responsibilities: 1) it instantiates the optimizer which will update the attacker's policy to lower training loss. 2) it instantiates the training harness which handles collecting online training data (we advise leaving the harness code alone); 3) it conducts the main training loop in the train method. </p> <p>To use the Trainer CLass be sure to import the Trainer base class and the training configuration. like so </p> <pre><code>from astra_rl import (\n    Trainer,\n    TrainingConfiguration,\n)\n</code></pre> <p>First, instantiate the training configuration with the training hyperparamaters you desire.</p> <pre><code>\n</code></pre> <p>Then, instantiate an instance of the Trainer base class with your training config, environmnet and algorithm.</p> <p>Look over the simple train() method in the Trainer base class. If you are okay with how this training loop operates (optimizes attacker on training alg loss and DOES NOT  SAVE THE MODEL), you can simply use the base train method.</p> <p>If you want a more sophisticated approach that periodically runs the updated model on a development data set and continually saves (in Hugging face format) the best performing model, import the HFASTTrainer class and instantiate it with how frequently you wish to perform evaluations during training, your training config, environment and algorithm.</p> <p>Note: The Trainer class will instantiate the training harness. We encourage you to not change the code for the harness and instead adjust training through the trainer class, paramaters, environment or algorithm. </p> <p>If you wish to further refine the training loop (including how/when the model is saved during training, adding features such as learning rate scheduling, or implementing a custom training loop design), this tutorial will help you create a custom training class!</p>"}]}
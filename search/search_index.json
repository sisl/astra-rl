{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Adaptive Stress Testing for Robust AI &amp; Reinforcement Learning (ASTRA-RL)","text":"<p>Welcome to the ASTRA-RL toolbox documentation! This documentation provides an overview of the ASTRA-RL toolbox, its features, and how to use it effectively.</p> <p>Warning</p> <p>This documentation (and project) is a work in progress. If you have any questions or suggestions, please feel free to open an issue on the GitHub repository</p>"},{"location":"api/index.html","title":"Library API","text":"<p>This section provides detailed information about all the classes, functions, and modules available in the ASTRA-RL toolbox. Each entry includes a description of its purpose, parameters, and usage examples.</p> <p>This documentation is generated automatically from the codebase using docstrings and comments, ensuring that it stays up-to-date with the latest changes.</p>"},{"location":"tutorials/index.html","title":"Tutorials","text":"<p>This section provides step-by-step guides and examples to help you get started with the ASTRA-RL toolbox. Each tutorial covers a specific aspect of the toolbox, from basic usage to advanced features.</p>"},{"location":"tutorials/quick_start_training.html","title":"Quick Start: Training a HuggingFace Attacker with ASTRA-RL","text":"<p>Do you want to train a HuggingFace attacker using an ASTRA-supported algorithm (e.g., DPO, IPO, PPO) and problem formulation (ASTPrompter, RL - Perez*, MALIBU*, CRT*)?</p> <p>Then this guide is for you. We\u2019ll walk through every step required to train a red-teaming attacker using our pre-configured classes and point you to customization guides when your use case goes beyond the defaults. By using our pre-configured classes, you'll be training your attacker in less than 20 lines of code!</p>"},{"location":"tutorials/quick_start_training.html#step-1-setup","title":"Step 1: Setup","text":"<p>Please see the README for full setup instructions. Here's a quick recap:</p> <pre><code># Install the ASTRA-RL toolbox\npip install astra-rl\n\n# Clone the repository (for examples and development)\ngit clone git@github.com:sisl/astra-rl.git\ncd astra-rl\n\n# Sync dependencies\nuv sync --dev\n\n# (Optional) Install pre-commit hooks to auto-format code\nuv run pre-commit install\n</code></pre>"},{"location":"tutorials/quick_start_training.html#step-2-create-a-training-script","title":"Step 2: Create a Training Script","text":"<p>Create a Python file for your training code (e.g., train.py).</p>"},{"location":"tutorials/quick_start_training.html#step-3-import-required-modules","title":"Step 3: Import Required Modules","text":"<pre><code>from torch.optim import AdamW\n\n# ASTRA-RL core components\nfrom astra_rl import ASTEnvironment, DPO, DetoxifyModerator, Harness\n\n# HuggingFace-friendly problem wrapper for ASTPrompter-style red teaming\nfrom astra_rl.ext.transformers import HFASTProblem\n</code></pre>"},{"location":"tutorials/quick_start_training.html#step-4-load-your-initial-prompts","title":"Step 4: Load Your Initial Prompts","text":"<p>To train an attacker, you\u2019ll need a list of comma-separated strings that act as initial prompts\u2014these initiate attacker-target rollouts used for online training. </p> <pre><code>import json\n\nwith open(\"prompts_reddit_train.json\") as f:\n    PROMPTS = json.load(f)\n</code></pre> <p>Since ASTPrompter red-teams for harmful outputs in conversational settings, it uses the ConvoKit Reddit Small Corpus (filtered for proper formatting and for non-toxicity using Detoxify) as its default source of initial prompts.</p> <p>The ASTRA-RL toolbox easily supports external prompt datasets or APIs\u2014just ensure the final PROMPTS variable is formatted as a list of strings.</p>"},{"location":"tutorials/quick_start_training.html#step-5-set-your-device","title":"Step 5: Set Your Device","text":"<pre><code>DEVICE = \"cuda\"  # or \"cpu\" if GPU is not available\n</code></pre> <p>We support both lightweight (e.g., GPT-2 + Detoxify) and heavyweight (e.g., LLaMA + LlamaGuard) setups. Pick model and moderator sizes that fit your compute!</p>"},{"location":"tutorials/quick_start_training.html#step-6-instantiate-your-problem","title":"Step 6: Instantiate Your Problem","text":"<p>The <code>HFASTProblem</code> class is a HuggingFace-compatible extension of <code>ASTProblem</code>. It simplifies red-teaming with transformer-based language models by handling:</p> <ul> <li>Automatic loading of attacker, target, and baseline models</li> <li>Tokenizer setup with fallback padding tokens for compatibility</li> <li>Rollout \"step\" generation</li> <li>Log probability computation of attacker, target, and baseline responses</li> </ul> <p>You simply provide the model IDs (any huggingface model) and a <code>Moderator</code> instance (DetexifyModerator() or LlamaGuardModerator()), and <code>HFASTProblem</code> manages the rest\u2014making it easy to plug into ASTRA-RL\u2019s training and evaluation pipeline.</p> <p>Note: If your attacker and target are different models (e.g., GPT-2 attacker, LLaMA target), this class handles all the tokenizer/model interop for you.</p> <pre><code>from astra_rl.modifiers import LlamaGuardModerator  # optional\n\n# Example 1: GPT2 attacker, target, baseline with Detoxify (lightweight setup)\nproblem = HFASTProblem(\"gpt2\", \"gpt2\", \"gpt2\", DetoxifyModerator(), DEVICE)\n\n# Example 2: GPT2 attacker, LLaMA target, GPT2 baseline with LlamaGuard (GPU recommended)\nproblem = HFASTProblem(\"gpt2\", \"meta-llama/Llama-3-8b\", \"gpt2\", LlamaGuardModerator(), DEVICE)\n</code></pre> <p>Need a custom model or rollout step logic? See customize/problems</p> <p>Want to use a custom moderator? See customize/moderators</p>"},{"location":"tutorials/quick_start_training.html#step-7-instantiate-the-environment","title":"Step 7: Instantiate the Environment","text":"<p>The environment defines how training rollouts are structured and collected. In ASTRA-RL, the default is the <code>ASTEnvironment</code>, which implements the conversation tree rollout used in the ASTPrompter paper.</p> <pre><code>env = ASTEnvironment(problem, PROMPTS)\n</code></pre> <p>This environment builds a tree-structured conversation graph, where:     - The root node starts from a random initial prompt (from PROMPTS)     - At each turn, the attacker generates multiple (tree_width, default 2) candidate utterances     - Each of those utterances is fed to the target model, which produces a response     - The resulting attacker\u2013target\u2013response tuples form child nodes     - This process repeats for tree_depth levels (default 3), yielding a multi-turn attacker-target dialogue tree This structure enables preference-based learning algorithms like DPO and IPO to reason over multiple conversational branches at once, training the attacker to elicit harmful responses in a multi-turn/dialouge setting.</p> <p>By default, rollouts are configured with tree_width=2 and tree_depth=3, but you can customize both:</p> <pre><code>env = ASTEnvironment(problem, PROMPTS, tree_width=4, tree_depth=5)\n</code></pre> <p>Want a different rollout graph structure or a multi-agent setup? See customize/environments</p>"},{"location":"tutorials/quick_start_training.html#step-8-choose-your-algorithm-and-optimizer","title":"Step 8: Choose Your Algorithm and Optimizer","text":"<pre><code>solver = DPO(problem)\noptimizer = AdamW(problem.parameters(), lr=1e-5)\n</code></pre> <p>To integrate your own RL algorithm, see customize_training/solvers</p>"},{"location":"tutorials/quick_start_training.html#step-9-create-the-training-harness","title":"Step 9: Create the Training Harness","text":"<pre><code>harness = Harness(\n    env,\n    solver,\n    num_episodes_per_experience=2,\n    use_wandb=True,\n    dataloader_kwargs={\"batch_size\": 4},\n)\n</code></pre>"},{"location":"tutorials/quick_start_training.html#the-training-harness-wires-your-environment-and-solver-together-it-collects-online-experience-and-for-each-batch-invokes-the-solver-to-compute-the-loss-used-to-update-the-attackers-policy-you-typically-wont-need-to-modify-the-harness-code-itselfadjust-behavior-via-the-environment-solver-or-your-outer-training-loop-eg-schedules-logging-hyperparameters","title":"The training harness wires your environment and solver together. It collects online experience and, for each batch, invokes the solver to compute the loss used to update the attacker\u2019s policy. You typically won\u2019t need to modify the harness code itself\u2014adjust behavior via the environment, solver, or your outer training loop (e.g., schedules, logging, hyperparameters).","text":""},{"location":"tutorials/quick_start_training.html#step-10-train-the-attacker","title":"Step 10: Train the Attacker","text":"<pre><code>for step in range(1000):\n    # Collect experience rollouts from attacker-target interactions\n    buf = harness.experience()\n    for experience in buf:\n        # Compute loss using the solver (e.g., DPO)\n        loss, step_logs = harness.step(experience)\n\n        # Standard PyTorch optimization\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        step_logs[\"step\"] = step\n        harness.log_current_step(step_logs)\n</code></pre>"},{"location":"tutorials/quick_start_training.html#choose-how-many-training-stepsepochs-to-run-you-have-full-control-over-the-loopcurriculum-evaluation-cadence-checkpointing-learning-rate-schedules-gradient-clipping-and-more","title":"Choose how many training steps/epochs to run. You have full control over the loop\u2014curriculum, evaluation cadence, checkpointing, learning-rate schedules, gradient clipping, and more.","text":""},{"location":"tutorials/quick_start_training.html#full-example-examplesast_hfpy","title":"Full Example: examples/ast_hf.py","text":"<p>We provide a complete working example that mirrors this guide!</p>"},{"location":"tutorials/running_an_evaluation.html","title":"Running An Evaluation","text":"<p>This tutorial will guide you through the process of running an evaluation using an existing evaluator model. The evaluator is a model that can be used to assess the quality of other models' outputs. </p> <p>For the purposes of this tutorial, we will assume that you have already trained an evaluator model from the Training an Evaluator tutorial.mk</p>"},{"location":"tutorials/training_an_evaluator.html","title":"Training an Evaluator","text":"<p>This tutorial will guide you through the process of training a custom model evaluator using the ASTRA-RL toolbox. The evaluator is a model that can be used to assess the quality of other models' outputs.</p>"}]}
{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Adaptive Stress Testing for Robust AI &amp; Reinforcement Learning (ASTRA-RL)","text":"<p>Welcome to the ASTRA-RL toolbox documentation! This documentation provides an overview of the ASTRA-RL toolbox, its features, and how to use it effectively.</p> <p>Warning</p> <p>This documentation (and project) is a work in progress. If you have any questions or suggestions, please feel free to open an issue on the GitHub repository</p>"},{"location":"api/index.html","title":"Library API","text":"<p>This section provides detailed information about all the classes, functions, and modules available in the ASTRA-RL toolbox. Each entry includes a description of its purpose, parameters, and usage examples.</p> <p>This documentation is generated automatically from the codebase using docstrings and comments, ensuring that it stays up-to-date with the latest changes.</p>"},{"location":"tutorials/index.html","title":"Tutorials","text":"<p>This section provides step-by-step guides and examples to help you get started with the ASTRA-RL toolbox. Each tutorial covers a specific aspect of the toolbox, from basic usage to advanced features.</p>"},{"location":"tutorials/quick_start_training.html","title":"Quick Start: Training a HuggingFace Attacker with ASTRA-RL","text":"<p>Do you want to train a HuggingFace attacker using an ASTRA-supported algorithm (e.g., DPO, IPO, PPO) and problem formulation (ASTPrompter, RL - Perez*, MALIBU*, CRT*)?</p> <p>Then this guide is for you. We\u2019ll walk through every step required to train a red-teaming attacker using our pre-configured classes and point you to customization guides when your use case goes beyond the defaults. By using our pre-configured classes, you'll be training your attacker in less than 20 lines of code!</p>"},{"location":"tutorials/quick_start_training.html#step-1-setup","title":"Step 1: Setup","text":"<p>Please see the README for full setup instructions. Here's a quick recap:</p> <pre><code># Install the ASTRA-RL toolbox\npip install astra-rl\n\n# Clone the repository (for examples and development)\ngit clone git@github.com:sisl/astra-rl.git\ncd astra-rl\n\n# Sync dependencies\nuv sync --dev\n\n# (Optional) Install pre-commit hooks to auto-format code\nuv run pre-commit install\n</code></pre>"},{"location":"tutorials/quick_start_training.html#step-2-create-a-training-script","title":"Step 2: Create a Training Script","text":"<p>Create a Python file for your training code (e.g., train.py).</p>"},{"location":"tutorials/quick_start_training.html#step-3-import-required-modules","title":"Step 3: Import Required Modules","text":"<pre><code>from torch.optim import AdamW\n\n# ASTRA-RL core components\nfrom astra_rl import ASTEnvironment, DPO, DetoxifyModerator, Harness\n\n# HuggingFace-friendly problem wrapper for ASTPrompter-style red teaming\nfrom astra_rl.ext.transformers import HFASTProblem\n</code></pre>"},{"location":"tutorials/quick_start_training.html#step-4-load-your-initial-prompts","title":"Step 4: Load Your Initial Prompts","text":"<p>To train an attacker, you\u2019ll need a list of comma-separated strings that act as initial prompts\u2014these initiate attacker-target rollouts used for online training. </p> <pre><code>import json\n\nwith open(\"prompts_reddit_train.json\") as f:\n    PROMPTS = json.load(f)\n</code></pre> <p>Since ASTPrompter red-teams for harmful outputs in conversational settings, it uses the ConvoKit Reddit Small Corpus (filtered for proper formatting and for non-toxicity using Detoxify) as its default source of initial prompts.</p> <p>The ASTRA-RL toolbox easily supports external prompt datasets or APIs\u2014just ensure the final PROMPTS variable is formatted as a list of strings.</p>"},{"location":"tutorials/quick_start_training.html#step-5-set-your-device","title":"Step 5: Set Your Device","text":"<pre><code>DEVICE = \"cuda\"  # or \"cpu\" if GPU is not available\n</code></pre> <p>We support both lightweight (e.g., GPT-2 + Detoxify) and heavyweight (e.g., LLaMA + LlamaGuard) setups. Pick model and moderator sizes that fit your compute!</p>"},{"location":"tutorials/quick_start_training.html#step-6-instantiate-your-problem","title":"Step 6: Instantiate Your Problem","text":"<p>The <code>HFASTProblem</code> class is a HuggingFace-compatible extension of <code>ASTProblem</code>. It simplifies red-teaming with transformer-based language models by handling:</p> <ul> <li>Automatic loading of attacker, target, and baseline models</li> <li>Tokenizer setup with fallback padding tokens for compatibility</li> <li>Rollout \"step\" generation</li> <li>Log probability computation of attacker, target, and baseline responses</li> </ul> <p>You simply provide the model IDs (any huggingface model) and a <code>Moderator</code> instance (DetexifyModerator() or LlamaGuardModerator()), and <code>HFASTProblem</code> manages the rest\u2014making it easy to plug into ASTRA-RL\u2019s training and evaluation pipeline.</p> <p>Note: If your attacker and target are different models (e.g., GPT-2 attacker, LLaMA target), this class handles all the tokenizer/model interop for you.</p> <pre><code>from astra_rl.modifiers import LlamaGuardModerator  # optional\n\n# Example 1: GPT2 attacker, target, baseline with Detoxify (lightweight setup)\nproblem = HFASTProblem(\"gpt2\", \"gpt2\", \"gpt2\", DetoxifyModerator(), DEVICE)\n\n# Example 2: GPT2 attacker, LLaMA target, GPT2 baseline with LlamaGuard (GPU recommended)\nproblem = HFASTProblem(\"gpt2\", \"meta-llama/Llama-3-8b\", \"gpt2\", LlamaGuardModerator(), DEVICE)\n</code></pre> <p>Need a custom model or rollout step logic? See customize/problems</p> <p>Want to use a custom moderator? See customize/moderators</p>"},{"location":"tutorials/quick_start_training.html#step-7-instantiate-the-environment","title":"Step 7: Instantiate the Environment","text":"<p>The environment defines how training rollouts are structured and collected. In ASTRA-RL, the default is the <code>ASTEnvironment</code>, which implements the conversation tree rollout used in the ASTPrompter paper.</p> <pre><code>env = ASTEnvironment(problem, PROMPTS)\n</code></pre> <p>This environment builds a tree-structured conversation graph, where:     - The root node starts from a random initial prompt (from PROMPTS)     - At each turn, the attacker generates multiple (tree_width, default 2) candidate utterances     - Each of those utterances is fed to the target model, which produces a response     - The resulting attacker\u2013target\u2013response tuples form child nodes     - This process repeats for tree_depth levels (default 3), yielding a multi-turn attacker-target dialogue tree This structure enables preference-based learning algorithms like DPO and IPO to reason over multiple conversational branches at once, training the attacker to elicit harmful responses in a multi-turn/dialouge setting.</p> <p>By default, rollouts are configured with tree_width=2 and tree_depth=3, but you can customize both:</p> <pre><code>env = ASTEnvironment(problem, PROMPTS, tree_width=4, tree_depth=5)\n</code></pre> <p>Want a different rollout graph structure or a multi-agent setup? See customize/environments</p>"},{"location":"tutorials/quick_start_training.html#step-8-choose-your-algorithm-and-optimizer","title":"Step 8: Choose Your Algorithm and Optimizer","text":"<pre><code>solver = DPO(problem)\noptimizer = AdamW(problem.parameters(), lr=1e-5)\n</code></pre> <p>To integrate your own RL algorithm, see customize_training/solvers</p>"},{"location":"tutorials/quick_start_training.html#step-9-create-the-training-harness","title":"Step 9: Create the Training Harness","text":"<pre><code>harness = Harness(\n    env,\n    solver,\n    num_episodes_per_experience=2,\n    use_wandb=True,\n    dataloader_kwargs={\"batch_size\": 4},\n)\n</code></pre>"},{"location":"tutorials/quick_start_training.html#the-training-harness-wires-your-environment-and-solver-together-it-collects-online-experience-and-for-each-batch-invokes-the-solver-to-compute-the-loss-used-to-update-the-attackers-policy-you-typically-wont-need-to-modify-the-harness-code-itselfadjust-behavior-via-the-environment-solver-or-your-outer-training-loop-eg-schedules-logging-hyperparameters","title":"The training harness wires your environment and solver together. It collects online experience and, for each batch, invokes the solver to compute the loss used to update the attacker\u2019s policy. You typically won\u2019t need to modify the harness code itself\u2014adjust behavior via the environment, solver, or your outer training loop (e.g., schedules, logging, hyperparameters).","text":""},{"location":"tutorials/quick_start_training.html#step-10-train-the-attacker","title":"Step 10: Train the Attacker","text":"<pre><code>for step in range(1000):\n    # Collect experience rollouts from attacker-target interactions\n    buf = harness.experience()\n    for experience in buf:\n        # Compute loss using the solver (e.g., DPO)\n        loss, step_logs = harness.step(experience)\n\n        # Standard PyTorch optimization\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        step_logs[\"step\"] = step\n        harness.log_current_step(step_logs)\n</code></pre>"},{"location":"tutorials/quick_start_training.html#choose-how-many-training-stepsepochs-to-run-you-have-full-control-over-the-loopcurriculum-evaluation-cadence-checkpointing-learning-rate-schedules-gradient-clipping-and-more","title":"Choose how many training steps/epochs to run. You have full control over the loop\u2014curriculum, evaluation cadence, checkpointing, learning-rate schedules, gradient clipping, and more.","text":""},{"location":"tutorials/quick_start_training.html#full-example-examplesast_hfpy","title":"Full Example: examples/ast_hf.py","text":"<p>We provide a complete working example that mirrors this guide!</p>"},{"location":"tutorials/running_an_evaluation.html","title":"Running An Evaluation","text":"<p>This tutorial will guide you through the process of running an evaluation using an existing evaluator model. The evaluator is a model that can be used to assess the quality of other models' outputs. </p> <p>For the purposes of this tutorial, we will assume that you have already trained an evaluator model from the Training an Evaluator tutorial.mk</p>"},{"location":"tutorials/training_an_evaluator.html","title":"Training an Evaluator","text":"<p>This tutorial will guide you through the process of training a custom model evaluator using the ASTRA-RL toolbox. The evaluator is a model that can be used to assess the quality of other models' outputs.</p>"},{"location":"tutorials/customize_training/envirnoments.html","title":"How to Customize the Environment","text":"<p>The environment defines how attacker\u2013target interactions are produced and packaged for training. It controls:</p> <ul> <li>how rollouts are generated (single path vs. tree),</li> <li>how states advance across turns,</li> <li>what per-step data (actions, responses, rewards) is stored,</li> <li>and what the solver receives as input.</li> </ul> <p>ASTRA-RL ships with <code>ASTEnvironment</code>, which mirrors the rollout structure used in ASTPrompter. You can subclass it\u2014or the base <code>Environment</code>\u2014to support:</p> <ul> <li>multi-agent or multi-turn conversations,</li> <li>tree-structured or flat trajectories,</li> <li>custom state-advance logic,</li> <li>alternative reward shaping or logging.</li> </ul>"},{"location":"tutorials/customize_training/envirnoments.html#option-a-subclass-the-base-environment","title":"Option A: Subclass the base <code>Environment</code>","text":"<pre><code>from astra_rl.core.environment import Environment\n\nclass MyCustomEnvironment(Environment[str, str]):\n    ...\n</code></pre> <p>The base <code>Environment</code> defines the interface the training harness expects. Make sure you implement the mandatory rollout() method.</p>"},{"location":"tutorials/customize_training/envirnoments.html#option-b-subclass-an-existing-environment","title":"Option B: Subclass an existing environment","text":"<p>If your rollout logic is \u201clike AST but with a twist,\u201d subclass <code>ASTEnvironment</code> and override only the parts you need (e.g., the expansion logic, evaluation, or metrics extraction).</p> <pre><code>from astra_rl import ASTProblem, ASTEnvironment \nfrom astra_rl.core.environment import Node, Graph\n\nclass MyASTVariant(ASTEnvironment):\n    \"\"\"\n    Subclass of ASTEnvironment that performs your custom rollout logic\n    \"\"\"\n\n    def __init__(...):\n        super().__init__(problem, prompts, tree_width, tree_depth)\n</code></pre>"},{"location":"tutorials/customize_training/envirnoments.html#required-implement-rollout","title":"Required: implement <code>rollout(...)</code>","text":"<p>Your environment must implement:</p> <pre><code>def rollout(self, seed: Optional[int] = None) -&gt; Graph[str, str]:\n    ...\n</code></pre> <p>This function performs one training rollout and returns a <code>Graph</code> made of <code>Node</code>s:</p> <ul> <li><code>Graph(context: str, children: List[Node])</code></li> <li><code>Node(context: str, attack: str, response: str, reward: float, children: List[Node])</code></li> </ul> <p>At a minimum, each node should capture:</p> <ul> <li><code>context</code> (state so far, e.g., conversation text),</li> <li><code>attack</code> (attacker\u2019s utterance / action),</li> <li><code>response</code> (target/defender\u2019s reply),</li> <li><code>reward</code> (float),</li> <li><code>children</code> (next steps; empty for leaf nodes).</li> </ul> <p>Tip: Make sure your rollout matches your solver\u2019s data contract.     Preference-based methods (e.g., DPO/IPO/ORPO) need preference pairs, so the easiest online setup is tree_width \u2265 2 to sample two candidates per prompt. Reward-based policy-gradient methods (e.g., PPO/A2C) do not require pairs; tree_width = 1 is typical.</p>"},{"location":"tutorials/customize_training/envirnoments.html#useful-astproblem-helpers-inside-your-environment","title":"Useful <code>ASTProblem</code> helpers inside your environment","text":"<p>These are the public, batch-friendly methods you\u2019ll typically call during rollout:</p> <ul> <li><code>rollout_prompt_with_attacker(prompts: Sequence[str]) -&gt; Sequence[str]</code></li> <li><code>rollout_prompt_with_target(prompts: Sequence[str]) -&gt; Sequence[str]</code></li> <li><code>reward(prompts, attacks, responses) -&gt; Sequence[float]</code></li> <li><code>advance(prompt, attack, response) -&gt; str</code> (returns the next state)</li> </ul> <p>Tip: These APIs are vectorized\u2014always pass lists (even if length 1) for best performance and simpler code.</p>"},{"location":"tutorials/customize_training/envirnoments.html#optional-evaluation-metrics","title":"(Optional) Evaluation metrics","text":"<p>If you want per-turn metrics like toxicity or likelihood, add a helper that walks a single-path graph and computes metrics at each turn. These helper functions can be useful when evaluating models after training. See the quick_start_evaluation guide for more information. </p> <p>Tip: Use the moderator in batch mode: <code>moderator.moderate(list_of_texts)</code>.</p>"},{"location":"tutorials/customize_training/envirnoments.html#best-practices-common-pitfalls","title":"Best practices &amp; common pitfalls","text":"<ul> <li>Prefer batch calls. The <code>ASTProblem</code> helpers are vectorized\u2014avoid per-item model calls inside tight loops.</li> <li>Determinism: Thread a <code>seed</code> through <code>rollout()</code> for reproducible experiments.</li> <li>Depth/width off-by-one: Depth <code>= 0</code> should produce no nodes; verify leaf counts match expectations.</li> <li>State advancement: Always use <code>advance(state, attack, response)</code> to build the next state, even if it\u2019s simple string concatenation\u2014this keeps things consistent and testable.</li> <li>Graph sanity checks: Before training, print or visualize one <code>rollout()</code> to confirm the structure matches your mental model (fan-out, depth, rewards present).</li> <li>Context length: If your <code>Problem</code> uses LMs with max context, ensure your <code>Problem</code> handles truncation. Environments should not assume a specific token limit.</li> <li>Evaluation vs. training: Keep evaluation single-path (<code>width=1</code>) to simplify metrics and avoid combinatorial explosion.</li> </ul>"},{"location":"tutorials/customize_training/envirnoments.html#quick-validation-checklist","title":"Quick validation checklist","text":"<ol> <li><code>env.rollout()</code> returns a <code>Graph</code> with the expected shape (root + child nodes).</li> <li>Every <code>Node</code> has <code>context</code>, <code>attack</code>, <code>response</code>, <code>reward</code> populated.</li> <li><code>advance()</code> is called exactly once per (context, attack, response).</li> <li>Running two <code>rollout(seed=123)</code> calls yields the same graph.</li> <li><code>final_reward()</code> returns the last-step reward in a single-path rollout.</li> <li>(If added) <code>eval_rollout(prompt)</code> respects <code>width=1</code> and your metrics extractor runs without per-item model calls.</li> </ol>"},{"location":"tutorials/customize_training/envirnoments.html#example-printing-a-small-rollout","title":"Example: printing a small rollout","text":"<pre><code>g = env.rollout(seed=7)\nprint(\"ROOT:\", g.context)\nfor i, n in enumerate(g.children):\n    print(f\"[{i}] atk={n.attack!r} rsp={n.response!r} rew={n.reward:.3f} #children={len(n.children)}\")\n</code></pre>"},{"location":"tutorials/customize_training/envirnoments.html#extending-the-data-model","title":"Extending the data model","text":"<p>Need extra metadata (e.g., per-turn safety tags, KL terms, or timestamps)? Subclass <code>Node</code> and/or wrap <code>Graph</code> with your own dataclasses, then adapt your solver to read those fields.</p>"},{"location":"tutorials/customize_training/moderators.html","title":"How to Create a Custom Moderator Class","text":"<p>Moderators play a central role in LM red teaming, acting similarly to reward models in traditional reinforcement learning. Their job is to quantify the reward an adversarial agent receives for reaching a particular state\u2014typically by measuring how harmful or unsafe a target model's output is.</p> <p>In many RL-based red-teaming setups, the moderator provides the signal that trains the attacker to generate utterances that elicit harmful responses from a target model. This achieves the red-teaming objective by exposing weaknesses in the target model\u2019s safety alignment and highlighting where additional fine-tuning is needed.</p> <p>To serve this purpose, a moderator must: - Accept a sequence of target model generations (e.g., text), - Return a scalar score (e.g., a toxicity value from 0 to 1) indicating the level of harm.</p> <p>The astra-rl toolbox currently supports text-based moderation using: - Detoxify, for toxicity classification, - Llama Guard 3, for a variety of harm categories (e.g., hate speech, threats, etc.).</p> <p>But the framework is modular\u2014you can define your own moderator class, wrapping any model that takes in your defined StateT and ActionT types (see astra_rl/core/common) and returns a Sequence[float].</p> <p>This guide walks you through creating a new Moderator subclass.</p>"},{"location":"tutorials/customize_training/moderators.html#step-1-subclass-the-moderator-base-class","title":"Step 1: Subclass the Moderator Base Class","text":"<p>To define your own moderator, create a class that inherits from:</p> <pre><code>Moderator[StateT, ActionT]\n</code></pre> <p>Where: - StateT is the type of state your environment uses (e.g., a string prompt) - ActionT is the type of action your model produces (e.g., a generated response) For most NLP use cases, both StateT and ActionT are str.</p> <p>example:</p> <pre><code>from typing import Sequence\nfrom detoxify import Detoxify\nfrom astra_rl.core.moderator import Moderator\n\nclass DetoxifyModerator(Moderator[str, str]):\n    def __init__(self, harm_category: str = \"toxicity\", variant: str = \"original\"):\n        self.model = Detoxify(variant)\n        self.harm_category = harm_category\n</code></pre>"},{"location":"tutorials/customize_training/moderators.html#step-2-implement-the-moderate-method","title":"Step 2: Implement the moderate Method","text":"<p>You must implement the abstract method:</p> <pre><code>def moderate(self, x: Sequence[Union[StateT, ActionT]]) -&gt; Sequence[float]:\n</code></pre> <p>This method: - Takes a sequence of states and/or actions. - Returns a sequence of floats, where each float is the moderation score (e.g., toxicity score) for the corresponding input.</p> <p>example:</p> <pre><code>def moderate(self, x: Sequence[str]) -&gt; Sequence[float]:\n        return self.model.predict(x)[self.harm_category]\n</code></pre>"},{"location":"tutorials/customize_training/moderators.html#step-3-integrate-your-moderator","title":"Step 3: Integrate your moderator","text":"<p>Once your class is defined, you can plug it into the RL pipeline like any other component:</p> <pre><code>moderator = DetoxifyModerator(harm_category=\"insult\", variant=\"unbiased\")\nscores = moderator.moderate([\"you are stupid\", \"have a nice day!\"])\n</code></pre> <p>To train with your custom moderator, modify your problem subclass to instantiate it during initialization:</p> <p>example:</p> <pre><code>class ExampleDetoxifyProblem(ASTProblem):\n    def __init__(self, device=\"cpu\"):\n        # your choice of moderator\n        super().__init__(DetoxifyModerator()) ## Plug in your custom moderator here ##\n\n        self.device = device\n        self.attacker = GPT2LMHeadModel.from_pretrained(MODEL_NAME).to(self.device)\n        self.target = GPT2LMHeadModel.from_pretrained(MODEL_NAME).to(self.device)\n\n        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n        self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n</code></pre>"},{"location":"tutorials/customize_training/moderators.html#helpful-notes","title":"Helpful Notes:","text":"<ul> <li> <p>Your moderator can wrap any scoring model\u2014e.g., classifiers, LLMs, rule-based filters\u2014as long as it implements moderate(...) \u2192 Sequence[float].</p> </li> <li> <p>You can include internal logic to handle tokenization, batching, preprocessing, etc.</p> </li> <li> <p>Return one score per input in the same order as received.</p> </li> <li> <p>If you're using a library or model that scores multiple types of harm (like Detoxify or llamaguard), your class can expose a harm_category attribute to customize which score to extract.</p> </li> </ul>"},{"location":"tutorials/customize_training/moderators.html#full-examples","title":"Full examples:","text":"<p>See the following files for complete, working implementations: - astra_rl/moderators/detoxify.py \u2014 wraps the Detoxify library - astra_rl/moderators/llamaGuard.py \u2014 wraps Meta\u2019s Llama Guard 3 model</p>"},{"location":"tutorials/customize_training/problems.html","title":"How to Create a Custom Problem Class for Red-Teaming with ASTRA-RL","text":"<p>The <code>Problem</code> class defines the core logic of how attacker-target interactions occur in reinforcement-learning-based red-teaming. It specifies:</p> <ul> <li>How the attacker and target interact and advance the conversation state.</li> <li>How reward signals are computed from attacker-target interactions.</li> <li>How a model rollout step is performed.</li> <li>What models and tokenizers are used for attacker, target, and baseline (if applicable).</li> </ul> <p>By subclassing the <code>Problem</code> or <code>ASTProblem</code> class, you can customize any of these aspects to fit your particular red-teaming scenario, algorithm, or dataset. In general, we reccomend subclassing from the ASTProblem or HFASTProblem whenever you can and then over-writing methods or definitions to suite your needs. </p> <p>This guide will show you how to:</p> <ul> <li>Create your own custom subclass.</li> <li>Integrate your own models and tokenizers.</li> <li>Customize reward computation and rollout logic.</li> </ul>"},{"location":"tutorials/customize_training/problems.html#understanding-the-base-class","title":"Understanding the Base Class","text":"<p>The base <code>ASTProblem</code> (source code astra-rl/src/astra_rl/methods/ast_problem.py) class provides default implementations  suitable for the ASTPrompter approach:</p> <ul> <li><code>advance</code>: Defines how a prompt advances given an attacker action and a target response.</li> <li><code>reward</code>: Defines how rewards are calculated from attacker-target interactions, using toxicity scoring and perplexity measures.</li> <li><code>rollout_prompt_with_attacker</code> / <code>rollout_prompt_with_target</code>: Methods to generate attacker and target model outputs from a given context.</li> <li><code>parameters</code>: Specifies model parameters for optimization.</li> </ul> <p>If possible, you should subclass this base class to preserve and extend this default behavior. If you want to change a method, simply define it in your subclass and it will over-right the original implementation while preserving the rest of the base class functionality.</p>"},{"location":"tutorials/customize_training/problems.html#how-to-create-a-subclass-with-custom-modelstokenizers","title":"How to create a subclass with custom models/tokenizers","text":"<p>To subclass your own <code>Problem</code>, follow this template:</p> <pre><code>from astra_rl.methods.ast_problem import ASTProblem\nfrom astra_rl.core.moderator import Moderator\n\nclass MyCustomProblem(ASTProblem):\n    def __init__(self, moderator: Moderator[str, str], my_custom_param: float = 1.0):\n        super().__init__(moderator)\n        self.my_custom_param = my_custom_param\n\n    def advance(self, state: str, action: str, response: str) -&gt; str:\n        # Example: Simply concatenate with separators\n        return f\"{state}\\n[Attacker]: {action}\\n[Target]: {response}\"\n\n    def reward(self, contexts, attacks, responses):\n        # Implement your own reward logic here\n        scores = self.moderator.moderate(responses)\n        return [score * self.my_custom_param for score in scores]\n\n    def rollout_prompt_with_attacker(self, prompts):\n        # Implement custom attacker rollout logic\n        raise NotImplementedError\n\n    def rollout_prompt_with_target(self, prompts):\n        # Implement custom target rollout logic\n        raise NotImplementedError\n\n    def parameters(self):\n        # Implement if your problem has trainable model parameters\n        return []\n</code></pre>"},{"location":"tutorials/customize_training/problems.html#step-3-integrating-your-own-models-attacker-target-or-baseline","title":"Step 3: Integrating Your Own Models (Attacker, Target, or Baseline)","text":"<p>If you are using huggingface models, save time by subclassing from our HFASTProblem base class which takes in any huggingface model names for the attacker, target, and baseline. However, you can also integrate any pretrained language model by loading the model and tokenizer in your constructor. If you integrate your own custom model, you must ensure that the corresponding rollout method(s) (rollout_prompt_with_(MODEL) more info below) and the corresponding (.get_(MODEL)_logprobs) method(s) are updated to correctly integrate your custom model. Here's how to do it clearly and correctly:</p> <pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\n# Create your subclass from the base Problem (most bare-boned) or the base ASTPrompter class (takes care of rollout and log probability methods)\nclass MyHuggingFaceProblem(ASTProblem):\n    def __init__(self, attacker_model_id: str, target_model_id: str, moderator, device=\"cuda\"):\n        super().__init__(moderator)\n        self.device = device\n\n        # Load your custom models and tokenizers\n        self.attacker = AutoModelForCausalLM.from_pretrained(attacker_model_id).to(self.device)\n        self.attacker_tokenizer = AutoTokenizer.from_pretrained(attacker_model_id)\n\n        self.target = AutoModelForCausalLM.from_pretrained(target_model_id).to(self.device)\n        self.target_tokenizer = AutoTokenizer.from_pretrained(target_model_id)\n\n    # required method to perform one step of a rollout in a batched manner: must take in a state (eg. conversation so far) and return a list of continuations (attacker utterances) in the corresponding order\n    def rollout_prompt_with_attacker(self, prompts):\n        inputs = self.attacker_tokenizer(prompts, return_tensors=\"pt\", padding=True).to(self.device)\n        outputs = self.attacker.generate(**inputs, max_new_tokens=32)\n        generated_texts = self.attacker_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n        continuations = [gen[len(prompt):] for prompt, gen in zip(prompts, generated_texts)]\n        return continuations\n\n    def rollout_prompt_with_target(self, prompts):\n        inputs = self.target_tokenizer(prompts, return_tensors=\"pt\", padding=True).to(self.device)\n        outputs = self.target.generate(**inputs, max_new_tokens=32)\n        generated_texts = self.target_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n        continuations = [gen[len(prompt):] for prompt, gen in zip(prompts, generated_texts)]\n        return continuations\n\n    def parameters(self):\n        return self.attacker.parameters()\n</code></pre>"},{"location":"tutorials/customize_training/problems.html#step-4-customizing-rollout-logic","title":"Step 4: Customizing Rollout Logic","text":"<p>To customize how attacker-target rollouts are performed, override these methods clearly:</p> <p>rollout_prompt_with_attacker(prompts: Sequence[str]) \u2192 Sequence[str]</p> <p>rollout_prompt_with_target(prompts: Sequence[str]) \u2192 Sequence[str]</p> <p>For example, you might use sampling strategies, temperature adjustments, or custom stopping criteria:</p> <pre><code>def rollout_prompt_with_attacker(self, prompts):\n    inputs = self.attacker_tokenizer(prompts, padding=True, return_tensors=\"pt\").to(self.device)\n    outputs = self.attacker.generate(\n        **inputs,\n        do_sample=True,\n        max_new_tokens=50,\n        temperature=0.8,\n        top_k=50,\n        top_p=0.9\n    )\n    texts = self.attacker_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n    return [text[len(prompt):] for prompt, text in zip(prompts, texts)]\n</code></pre>"},{"location":"tutorials/customize_training/problems.html#step-5-customizing-reward-logic","title":"Step 5: Customizing Reward Logic","text":"<p>Override the reward method to compute your custom reward signal. Typically, you'll combine toxicity, relevance, perplexity, or other metrics:</p> <pre><code>def reward(self, contexts, attacks, responses):\n    attack_scores = self.moderator.moderate(attacks)\n    response_scores = self.moderator.moderate(responses)\n    combined = [(a_score + r_score) / 2.0 for a_score, r_score in zip(attack_scores, response_scores)]\n    return combined\n\n</code></pre>"},{"location":"tutorials/customize_training/problems.html#tips-and-best-practices","title":"Tips and Best Practices:","text":"<p>Always clearly document parameters and logic for your custom problem class.</p> <p>Ensure models and tokenizers are device-aware (e.g., GPU-compatible).</p> <p>Thoroughly test your rollouts independently before integrating them into the full RL loop.</p> <p>For debugging, add verbose logging to track input-output sequences.</p>"},{"location":"tutorials/customize_training/problems.html#further-reading-and-examples","title":"Further Reading and Examples","text":"<p>Default ASTPrompter implementation: ASTProblem</p> <p>HuggingFace-compatible subclass example: HFASTProblem</p> <p>Environment customization guide: Custom Environments</p>"}]}
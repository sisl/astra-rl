{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Adaptive Stress Testing for Robust AI &amp; Reinforcement Learning (ASTRA-RL)","text":"<p>Welcome to the ASTRA-RL toolbox documentation! This documentation provides an overview of the ASTRA-RL toolbox, its features, and how to use it effectively.</p> <p>Warning</p> <p>This documentation (and project) is a work in progress. If you have any questions or suggestions, please feel free to open an issue on the GitHub repository</p>"},{"location":"api/index.html","title":"Library API","text":"<p>This section provides detailed information about all the classes, functions, and modules available in the ASTRA-RL toolbox. Each entry includes a description of its purpose, parameters, and usage examples.</p> <p>This documentation is generated automatically from the codebase using docstrings and comments, ensuring that it stays up-to-date with the latest changes.</p>"},{"location":"tutorials/index.html","title":"Tutorials","text":"<p>This section provides step-by-step guides and examples to help you get started with the ASTRA-RL toolbox. Each tutorial covers a specific aspect of the toolbox, from basic usage to advanced features.</p>"},{"location":"tutorials/quick_start_training.html","title":"Quick Start: Training a HuggingFace Attacker with ASTRA-RL","text":"<p>Do you want to train a HuggingFace attacker using an ASTRA-supported algorithm (e.g., DPO, IPO, PPO) and problem formulation (ASTPrompter, RL - Perez*, MALIBU*, CRT*)?</p> <p>Then this guide is for you. We\u2019ll walk through every step required to train a red-teaming attacker using our pre-configured classes and point you to customization guides when your use case goes beyond the defaults. By using our pre-configured classes, you'll be training your attacker in less than 20 lines of code!</p>"},{"location":"tutorials/quick_start_training.html#step-1-setup","title":"Step 1: Setup","text":"<p>Please see the README for full setup instructions. Here's a quick recap:</p> <pre><code># Install the ASTRA-RL toolbox\npip install astra-rl\n\n# Clone the repository (for examples and development)\ngit clone git@github.com:sisl/astra-rl.git\ncd astra-rl\n\n# Sync dependencies\nuv sync --dev\n\n# (Optional) Install pre-commit hooks to auto-format code\nuv run pre-commit install\n</code></pre>"},{"location":"tutorials/quick_start_training.html#step-2-create-a-training-script","title":"Step 2: Create a Training Script","text":"<p>Create a Python file for your training code (e.g., train.py).</p>"},{"location":"tutorials/quick_start_training.html#step-3-import-required-modules","title":"Step 3: Import Required Modules","text":"<pre><code>from torch.optim import AdamW\n\n# ASTRA-RL core components\nfrom astra_rl import ASTEnvironment, DPO, DetoxifyModerator, Harness\n\n# HuggingFace-friendly problem wrapper for ASTPrompter-style red teaming\nfrom astra_rl.ext.transformers import HFASTProblem\n</code></pre>"},{"location":"tutorials/quick_start_training.html#step-4-load-your-initial-prompts","title":"Step 4: Load Your Initial Prompts","text":"<p>To train an attacker, you\u2019ll need a list of comma-separated strings that act as initial prompts\u2014these initiate attacker-target rollouts used for online training. </p> <pre><code>import json\n\nwith open(\"prompts_reddit_train.json\") as f:\n    PROMPTS = json.load(f)\n</code></pre> <p>Since ASTPrompter red-teams for harmful outputs in conversational settings, it uses the ConvoKit Reddit Small Corpus (filtered for proper formatting and for non-toxicity using Detoxify) as its default source of initial prompts.</p> <p>The ASTRA-RL toolbox easily supports external prompt datasets or APIs\u2014just ensure the final PROMPTS variable is formatted as a list of strings.</p>"},{"location":"tutorials/quick_start_training.html#step-5-set-your-device","title":"Step 5: Set Your Device","text":"<pre><code>DEVICE = \"cuda\"  # or \"cpu\" if GPU is not available\n</code></pre> <p>We support both lightweight (e.g., GPT-2 + Detoxify) and heavyweight (e.g., LLaMA + LlamaGuard) setups. Pick model and moderator sizes that fit your compute!</p>"},{"location":"tutorials/quick_start_training.html#step-6-instantiate-your-problem","title":"Step 6: Instantiate Your Problem","text":"<p>The <code>HFASTProblem</code> class is a HuggingFace-compatible extension of <code>ASTProblem</code>. It simplifies red-teaming with transformer-based language models by handling:</p> <ul> <li>Automatic loading of attacker, target, and baseline models</li> <li>Tokenizer setup with fallback padding tokens for compatibility</li> <li>Rollout \"step\" generation</li> <li>Log probability computation of attacker, target, and baseline responses</li> </ul> <p>You simply provide the model IDs (any huggingface model) and a <code>Moderator</code> instance (DetexifyModerator() or LlamaGuardModerator()), and <code>HFASTProblem</code> manages the rest\u2014making it easy to plug into ASTRA-RL\u2019s training and evaluation pipeline.</p> <p>Note: If your attacker and target are different models (e.g., GPT-2 attacker, LLaMA target), this class handles all the tokenizer/model interop for you.</p> <pre><code>from astra_rl.modifiers import LlamaGuardModerator  # optional\n\n# Example 1: GPT2 attacker, target, baseline with Detoxify (lightweight setup)\nproblem = HFASTProblem(\"gpt2\", \"gpt2\", \"gpt2\", DetoxifyModerator(), DEVICE)\n\n# Example 2: GPT2 attacker, LLaMA target, GPT2 baseline with LlamaGuard (GPU recommended)\nproblem = HFASTProblem(\"gpt2\", \"meta-llama/Llama-3-8b\", \"gpt2\", LlamaGuardModerator(), DEVICE)\n</code></pre> <p>Need a custom model or rollout step logic? See customize/problems</p> <p>Want to use a custom moderator? See customize/moderators</p>"},{"location":"tutorials/quick_start_training.html#step-7-instantiate-the-environment","title":"Step 7: Instantiate the Environment","text":"<p>The environment defines how training rollouts are structured and collected. In ASTRA-RL, the default is the <code>ASTEnvironment</code>, which implements the conversation tree rollout used in the ASTPrompter paper.</p> <pre><code>env = ASTEnvironment(problem, PROMPTS)\n</code></pre> <p>This environment builds a tree-structured conversation graph, where:     - The root node starts from a random initial prompt (from PROMPTS)     - At each turn, the attacker generates multiple (tree_width, default 2) candidate utterances     - Each of those utterances is fed to the target model, which produces a response     - The resulting attacker\u2013target\u2013response tuples form child nodes     - This process repeats for tree_depth levels (default 3), yielding a multi-turn attacker-target dialogue tree This structure enables preference-based learning algorithms like DPO and IPO to reason over multiple conversational branches at once, training the attacker to elicit harmful responses in a multi-turn/dialouge setting.</p> <p>By default, rollouts are configured with tree_width=2 and tree_depth=3, but you can customize both:</p> <pre><code>env = ASTEnvironment(problem, PROMPTS, tree_width=4, tree_depth=5)\n</code></pre> <p>Want a different rollout graph structure or a multi-agent setup? See customize/environments</p>"},{"location":"tutorials/quick_start_training.html#step-8-choose-your-algorithm-and-optimizer","title":"Step 8: Choose Your Algorithm and Optimizer","text":"<pre><code>solver = DPO(problem)\noptimizer = AdamW(problem.parameters(), lr=1e-5)\n</code></pre> <p>To integrate your own RL algorithm, see customize_training/solvers</p>"},{"location":"tutorials/quick_start_training.html#step-9-create-the-training-harness","title":"Step 9: Create the Training Harness","text":"<pre><code>harness = Harness(\n    env,\n    solver,\n    num_episodes_per_experience=2,\n    use_wandb=True,\n    dataloader_kwargs={\"batch_size\": 4},\n)\n</code></pre>"},{"location":"tutorials/quick_start_training.html#the-training-harness-wires-your-environment-and-solver-together-it-collects-online-experience-and-for-each-batch-invokes-the-solver-to-compute-the-loss-used-to-update-the-attackers-policy-you-typically-wont-need-to-modify-the-harness-code-itselfadjust-behavior-via-the-environment-solver-or-your-outer-training-loop-eg-schedules-logging-hyperparameters","title":"The training harness wires your environment and solver together. It collects online experience and, for each batch, invokes the solver to compute the loss used to update the attacker\u2019s policy. You typically won\u2019t need to modify the harness code itself\u2014adjust behavior via the environment, solver, or your outer training loop (e.g., schedules, logging, hyperparameters).","text":""},{"location":"tutorials/quick_start_training.html#step-10-train-the-attacker","title":"Step 10: Train the Attacker","text":"<pre><code>for step in range(1000):\n    # Collect experience rollouts from attacker-target interactions\n    buf = harness.experience()\n    for experience in buf:\n        # Compute loss using the solver (e.g., DPO)\n        loss, step_logs = harness.step(experience)\n\n        # Standard PyTorch optimization\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        step_logs[\"step\"] = step\n        harness.log_current_step(step_logs)\n</code></pre>"},{"location":"tutorials/quick_start_training.html#choose-how-many-training-stepsepochs-to-run-you-have-full-control-over-the-loopcurriculum-evaluation-cadence-checkpointing-learning-rate-schedules-gradient-clipping-and-more","title":"Choose how many training steps/epochs to run. You have full control over the loop\u2014curriculum, evaluation cadence, checkpointing, learning-rate schedules, gradient clipping, and more.","text":""},{"location":"tutorials/quick_start_training.html#full-example-examplesast_hfpy","title":"Full Example: examples/ast_hf.py","text":"<p>We provide a complete working example that mirrors this guide!</p>"},{"location":"tutorials/running_an_evaluation.html","title":"Running An Evaluation","text":"<p>This tutorial will guide you through the process of running an evaluation using an existing evaluator model. The evaluator is a model that can be used to assess the quality of other models' outputs. </p> <p>For the purposes of this tutorial, we will assume that you have already trained an evaluator model from the Training an Evaluator tutorial.mk</p>"},{"location":"tutorials/training_an_evaluator.html","title":"Training an Evaluator","text":"<p>This tutorial will guide you through the process of training a custom model evaluator using the ASTRA-RL toolbox. The evaluator is a model that can be used to assess the quality of other models' outputs.</p>"},{"location":"tutorials/customize_training/envirnoments.html","title":"How to Customize the Environment","text":"<p>The environment defines how attacker\u2013target interactions are produced and packaged for training. It controls:</p> <ul> <li>how rollouts are generated (single path vs. tree),</li> <li>how states advance across turns,</li> <li>what per-step data (actions, responses, rewards) is stored,</li> <li>and what the solver receives as input.</li> </ul> <p>ASTRA-RL ships with <code>ASTEnvironment</code>, which mirrors the rollout structure used in ASTPrompter. You can subclass it\u2014or the base <code>Environment</code>\u2014to support:</p> <ul> <li>multi-agent or multi-turn conversations,</li> <li>tree-structured or flat trajectories,</li> <li>custom state-advance logic,</li> <li>alternative reward shaping or logging.</li> </ul>"},{"location":"tutorials/customize_training/envirnoments.html#option-a-subclass-the-base-environment","title":"Option A: Subclass the base <code>Environment</code>","text":"<pre><code>from astra_rl.core.environment import Environment\n\nclass MyCustomEnvironment(Environment[str, str]):\n    ...\n</code></pre> <p>The base <code>Environment</code> defines the interface the training harness expects. Make sure you implement the mandatory rollout() method.</p>"},{"location":"tutorials/customize_training/envirnoments.html#option-b-subclass-an-existing-environment","title":"Option B: Subclass an existing environment","text":"<p>If your rollout logic is \u201clike AST but with a twist,\u201d subclass <code>ASTEnvironment</code> and override only the parts you need (e.g., the expansion logic, evaluation, or metrics extraction).</p> <pre><code>from astra_rl import ASTProblem, ASTEnvironment \nfrom astra_rl.core.environment import Node, Graph\n\nclass MyASTVariant(ASTEnvironment):\n    \"\"\"\n    Subclass of ASTEnvironment that performs your custom rollout logic\n    \"\"\"\n\n    def __init__(...):\n        super().__init__(problem, prompts, tree_width, tree_depth)\n</code></pre>"},{"location":"tutorials/customize_training/envirnoments.html#required-implement-rollout","title":"Required: implement <code>rollout(...)</code>","text":"<p>Your environment must implement:</p> <pre><code>def rollout(self, seed: Optional[int] = None) -&gt; Graph[str, str]:\n    ...\n</code></pre> <p>This function performs one training rollout and returns a <code>Graph</code> made of <code>Node</code>s:</p> <ul> <li><code>Graph(context: str, children: List[Node])</code></li> <li><code>Node(context: str, attack: str, response: str, reward: float, children: List[Node])</code></li> </ul> <p>At a minimum, each node should capture:</p> <ul> <li><code>context</code> (state so far, e.g., conversation text),</li> <li><code>attack</code> (attacker\u2019s utterance / action),</li> <li><code>response</code> (target/defender\u2019s reply),</li> <li><code>reward</code> (float),</li> <li><code>children</code> (next steps; empty for leaf nodes).</li> </ul> <p>Tip: Make sure your rollout matches your solver\u2019s data contract.     Preference-based methods (e.g., DPO/IPO/ORPO) need preference pairs, so the easiest online setup is tree_width \u2265 2 to sample two candidates per prompt. Reward-based policy-gradient methods (e.g., PPO/A2C) do not require pairs; tree_width = 1 is typical.</p>"},{"location":"tutorials/customize_training/envirnoments.html#useful-astproblem-helpers-inside-your-environment","title":"Useful <code>ASTProblem</code> helpers inside your environment","text":"<p>These are the public, batch-friendly methods you\u2019ll typically call during rollout:</p> <ul> <li><code>rollout_prompt_with_attacker(prompts: Sequence[str]) -&gt; Sequence[str]</code></li> <li><code>rollout_prompt_with_target(prompts: Sequence[str]) -&gt; Sequence[str]</code></li> <li><code>reward(prompts, attacks, responses) -&gt; Sequence[float]</code></li> <li><code>advance(prompt, attack, response) -&gt; str</code> (returns the next state)</li> </ul> <p>Tip: These APIs are vectorized\u2014always pass lists (even if length 1) for best performance and simpler code.</p>"},{"location":"tutorials/customize_training/envirnoments.html#optional-evaluation-metrics","title":"(Optional) Evaluation metrics","text":"<p>If you want per-turn metrics like toxicity or likelihood, add a helper that walks a single-path graph and computes metrics at each turn. These helper functions can be useful when evaluating models after training. See the quick_start_evaluation guide for more information. </p> <p>Tip: Use the moderator in batch mode: <code>moderator.moderate(list_of_texts)</code>.</p>"},{"location":"tutorials/customize_training/envirnoments.html#best-practices-common-pitfalls","title":"Best practices &amp; common pitfalls","text":"<ul> <li>Prefer batch calls. The <code>ASTProblem</code> helpers are vectorized\u2014avoid per-item model calls inside tight loops.</li> <li>Determinism: Thread a <code>seed</code> through <code>rollout()</code> for reproducible experiments.</li> <li>Depth/width off-by-one: Depth <code>= 0</code> should produce no nodes; verify leaf counts match expectations.</li> <li>State advancement: Always use <code>advance(state, attack, response)</code> to build the next state, even if it\u2019s simple string concatenation\u2014this keeps things consistent and testable.</li> <li>Graph sanity checks: Before training, print or visualize one <code>rollout()</code> to confirm the structure matches your mental model (fan-out, depth, rewards present).</li> <li>Context length: If your <code>Problem</code> uses LMs with max context, ensure your <code>Problem</code> handles truncation. Environments should not assume a specific token limit.</li> <li>Evaluation vs. training: Keep evaluation single-path (<code>width=1</code>) to simplify metrics and avoid combinatorial explosion.</li> </ul>"},{"location":"tutorials/customize_training/envirnoments.html#quick-validation-checklist","title":"Quick validation checklist","text":"<ol> <li><code>env.rollout()</code> returns a <code>Graph</code> with the expected shape (root + child nodes).</li> <li>Every <code>Node</code> has <code>context</code>, <code>attack</code>, <code>response</code>, <code>reward</code> populated.</li> <li><code>advance()</code> is called exactly once per (context, attack, response).</li> <li>Running two <code>rollout(seed=123)</code> calls yields the same graph.</li> <li><code>final_reward()</code> returns the last-step reward in a single-path rollout.</li> <li>(If added) <code>eval_rollout(prompt)</code> respects <code>width=1</code> and your metrics extractor runs without per-item model calls.</li> </ol>"},{"location":"tutorials/customize_training/envirnoments.html#example-printing-a-small-rollout","title":"Example: printing a small rollout","text":"<pre><code>g = env.rollout(seed=7)\nprint(\"ROOT:\", g.context)\nfor i, n in enumerate(g.children):\n    print(f\"[{i}] atk={n.attack!r} rsp={n.response!r} rew={n.reward:.3f} #children={len(n.children)}\")\n</code></pre>"},{"location":"tutorials/customize_training/envirnoments.html#extending-the-data-model","title":"Extending the data model","text":"<p>Need extra metadata (e.g., per-turn safety tags, KL terms, or timestamps)? Subclass <code>Node</code> and/or wrap <code>Graph</code> with your own dataclasses, then adapt your solver to read those fields.</p>"},{"location":"tutorials/customize_training/moderators.html","title":"How to Create a Custom Moderator Class","text":"<p>Moderators play a central role in LM red teaming, acting similarly to reward models in traditional reinforcement learning. Their job is to quantify the reward an adversarial agent receives for reaching a particular state\u2014typically by measuring how harmful or unsafe a target model's output is.</p> <p>In many RL-based red-teaming setups, the moderator provides the signal that trains the attacker to generate utterances that elicit harmful responses from a target model. This achieves the red-teaming objective by exposing weaknesses in the target model\u2019s safety alignment and highlighting where additional fine-tuning is needed.</p> <p>To serve this purpose, a moderator must: - Accept a sequence of target model generations (e.g., text), - Return a scalar score (e.g., a toxicity value from 0 to 1) indicating the level of harm.</p> <p>The astra-rl toolbox currently supports text-based moderation using: - Detoxify, for toxicity classification, - Llama Guard 3, for a variety of harm categories (e.g., hate speech, threats, etc.).</p> <p>But the framework is modular\u2014you can define your own moderator class, wrapping any model that takes in your defined StateT and ActionT types (see astra_rl/core/common) and returns a Sequence[float].</p> <p>This guide walks you through creating a new Moderator subclass.</p>"},{"location":"tutorials/customize_training/moderators.html#step-1-subclass-the-moderator-base-class","title":"Step 1: Subclass the Moderator Base Class","text":"<p>To define your own moderator, create a class that inherits from:</p> <pre><code>Moderator[StateT, ActionT]\n</code></pre> <p>Where: - StateT is the type of state your environment uses (e.g., a string prompt) - ActionT is the type of action your model produces (e.g., a generated response) For most NLP use cases, both StateT and ActionT are str.</p> <p>example:</p> <pre><code>from typing import Sequence\nfrom detoxify import Detoxify\nfrom astra_rl.core.moderator import Moderator\n\nclass DetoxifyModerator(Moderator[str, str]):\n    def __init__(self, harm_category: str = \"toxicity\", variant: str = \"original\"):\n        self.model = Detoxify(variant)\n        self.harm_category = harm_category\n</code></pre>"},{"location":"tutorials/customize_training/moderators.html#step-2-implement-the-moderate-method","title":"Step 2: Implement the moderate Method","text":"<p>You must implement the abstract method:</p> <pre><code>def moderate(self, x: Sequence[Union[StateT, ActionT]]) -&gt; Sequence[float]:\n</code></pre> <p>This method: - Takes a sequence of states and/or actions. - Returns a sequence of floats, where each float is the moderation score (e.g., toxicity score) for the corresponding input.</p> <p>example:</p> <pre><code>def moderate(self, x: Sequence[str]) -&gt; Sequence[float]:\n        return self.model.predict(x)[self.harm_category]\n</code></pre>"},{"location":"tutorials/customize_training/moderators.html#step-3-integrate-your-moderator","title":"Step 3: Integrate your moderator","text":"<p>Once your class is defined, you can plug it into the RL pipeline like any other component:</p> <pre><code>moderator = DetoxifyModerator(harm_category=\"insult\", variant=\"unbiased\")\nscores = moderator.moderate([\"you are stupid\", \"have a nice day!\"])\n</code></pre> <p>To train with your custom moderator, modify your problem subclass to instantiate it during initialization:</p> <p>example:</p> <pre><code>class ExampleDetoxifyProblem(ASTProblem):\n    def __init__(self, device=\"cpu\"):\n        # your choice of moderator\n        super().__init__(DetoxifyModerator()) ## Plug in your custom moderator here ##\n\n        self.device = device\n        self.attacker = GPT2LMHeadModel.from_pretrained(MODEL_NAME).to(self.device)\n        self.target = GPT2LMHeadModel.from_pretrained(MODEL_NAME).to(self.device)\n\n        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n        self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n</code></pre>"},{"location":"tutorials/customize_training/moderators.html#helpful-notes","title":"Helpful Notes:","text":"<ul> <li> <p>Your moderator can wrap any scoring model\u2014e.g., classifiers, LLMs, rule-based filters\u2014as long as it implements moderate(...) \u2192 Sequence[float].</p> </li> <li> <p>You can include internal logic to handle tokenization, batching, preprocessing, etc.</p> </li> <li> <p>Return one score per input in the same order as received.</p> </li> <li> <p>If you're using a library or model that scores multiple types of harm (like Detoxify or llamaguard), your class can expose a harm_category attribute to customize which score to extract.</p> </li> </ul>"},{"location":"tutorials/customize_training/moderators.html#full-examples","title":"Full examples:","text":"<p>See the following files for complete, working implementations: - astra_rl/moderators/detoxify.py \u2014 wraps the Detoxify library - astra_rl/moderators/llamaGuard.py \u2014 wraps Meta\u2019s Llama Guard 3 model</p>"},{"location":"tutorials/customize_training/prob.html","title":"Prob","text":"<p>Awesome\u2014here\u2019s a polished, copy-pasteable \u201cHow-to\u201d that shows users exactly how to build a custom Problem class, including non-HuggingFace models and a custom reward.</p>"},{"location":"tutorials/customize_training/prob.html#how-to-create-a-custom-problem-class-hf-or-non-hf","title":"How to Create a Custom Problem Class (HF or non-HF)","text":"<p>A Problem encapsulates models + tokenization + rollout + log-probabilities + rewards. Environments call your Problem to:</p> <ul> <li>sample attacker/target continuations,</li> <li>compute log-probs for losses (e.g., PPO/DPO),</li> <li>compute rewards (scalar or per-step).</li> </ul> <p>Most users can subclass <code>ASTProblem</code> (or <code>HFASTProblem</code> for HF). If you\u2019re integrating a custom / non-HF model, you\u2019ll implement the same small set of methods.</p>"},{"location":"tutorials/customize_training/prob.html#what-you-must-implement-api-contract","title":"What you must implement (API contract)","text":"<p>Your subclass must provide batched implementations (lists in, lists/tensors out), aligned by index:</p> <ul> <li><code>rollout_prompt_with_attacker(prompts: Sequence[str]) -&gt; Sequence[str]</code></li> <li><code>rollout_prompt_with_target(prompts: Sequence[str]) -&gt; Sequence[str]</code></li> <li><code>get_attacker_logprobs(contexts: Sequence[str], continuations: Sequence[str]) -&gt; torch.Tensor</code> (sum of token log-probs for each continuation conditioned on its context; shape <code>[B]</code>)</li> <li><code>get_target_logprobs(contexts, continuations) -&gt; torch.Tensor</code> (no grad)</li> <li><code>get_baseline_logprobs(contexts, continuations) -&gt; torch.Tensor</code> (no grad; often same as target/reference)</li> <li><code>parameters() -&gt; Iterable[torch.nn.Parameter]</code> (usually attacker\u2019s params only)</li> </ul> <p>Vectorize! Always pass lists (even of length 1) and do batched compute inside. This is faster and simpler.</p>"},{"location":"tutorials/customize_training/prob.html#minimal-template-non-hf-friendly","title":"Minimal template (non-HF friendly)","text":"<p>Below is a \u201cneutral\u201d template that works whether your models are HuggingFace or not. Plug your model in by implementing four tiny adapter methods: <code>_encode</code>, <code>_decode</code>, <code>_generate_ids</code>, <code>_forward_logits</code>.</p> <pre><code>from __future__ import annotations\nfrom typing import Iterable, List, Sequence, Optional, Dict, Any\nimport torch\nimport torch.nn.functional as F\n\nfrom astra_rl.methods.ast_problem import ASTProblem\nfrom astra_rl.core.moderator import Moderator  # your toxicity/safety scorer, etc.\n\nclass MyCustomProblem(ASTProblem):\n    \"\"\"\n    Problem class that supports *any* autoregressive LM by providing\n    small adapters for encode/decode/generate/forward.\n    \"\"\"\n\n    def __init__(\n        self,\n        moderator: Moderator[str, str],\n        attacker_model,      # your custom AR LM (HF or not)\n        target_model,        # frozen model under test (HF or not)\n        *,\n        device: str = \"cuda\",\n        max_ctx: int = 1024,\n        pad_token_id: int = 0,\n        eos_token_id: int = 0,\n    ):\n        super().__init__(moderator)\n        self.device = device\n        self.max_ctx = int(max_ctx)\n        self.pad_token_id = int(pad_token_id)\n        self.eos_token_id = int(eos_token_id)\n\n        self.attacker = attacker_model.to(self.device)\n        self.target   = target_model.to(self.device)\n\n        # If your models need separate tokenizers/encoders, store them:\n        # self.attacker_tok = ...\n        # self.target_tok   = ...\n\n    # ---------- Required harness API: rollouts ----------\n    def rollout_prompt_with_attacker(self, prompts: Sequence[str]) -&gt; List[str]:\n        \"\"\"\n        Generate attacker continuations for each prompt.\n        Returns a list of pure continuations (not the full prompt+continuation).\n        \"\"\"\n        # 1) Encode\n        batch = self._encode(prompts, for_attacker=True)\n        input_ids = batch[\"input_ids\"].to(self.device)\n\n        # 2) Generate ids with your sampler\n        out_ids = self._generate_ids(self.attacker, input_ids, max_new_tokens=32)\n\n        # 3) Decode full strings, then strip the prompt\n        full_texts = self._decode(out_ids, for_attacker=True)\n        conts = [full[len(p):] for p, full in zip(prompts, full_texts)]\n        return conts\n\n    def rollout_prompt_with_target(self, prompts: Sequence[str]) -&gt; List[str]:\n        batch = self._encode(prompts, for_attacker=False)\n        input_ids = batch[\"input_ids\"].to(self.device)\n        out_ids = self._generate_ids(self.target, input_ids, max_new_tokens=32)\n        full_texts = self._decode(out_ids, for_attacker=False)\n        conts = [full[len(p):] for p, full in zip(prompts, full_texts)]\n        return conts\n\n    # ---------- Required harness API: log-probs ----------\n    def get_attacker_logprobs(\n        self, contexts: Sequence[str], continuations: Sequence[str]\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Return sum of token log-probs for each continuation given its context.\n        Must allow gradient to flow through the *attacker*.\n        \"\"\"\n        return self._conditional_logprob_sum(\n            self.attacker, contexts, continuations, for_attacker=True, require_grad=True\n        )\n\n    def get_target_logprobs(\n        self, contexts: Sequence[str], continuations: Sequence[str]\n    ) -&gt; torch.Tensor:\n        with torch.no_grad():\n            return self._conditional_logprob_sum(\n                self.target, contexts, continuations, for_attacker=False, require_grad=False\n            )\n\n    def get_baseline_logprobs(\n        self, contexts: Sequence[str], continuations: Sequence[str]\n    ) -&gt; torch.Tensor:\n        # Often identical to target (frozen reference) in KL/DPO setups.\n        return self.get_target_logprobs(contexts, continuations)\n\n    # ---------- Required harness API: trainable params ----------\n    def parameters(self) -&gt; Iterable[torch.nn.Parameter]:\n        # Only optimize the attacker by default\n        return self.attacker.parameters()\n\n    # ---------- Reward: customize for your task ----------\n    def reward(\n        self,\n        contexts: Sequence[str],\n        attacks: Sequence[str],\n        responses: Sequence[str],\n    ) -&gt; List[float]:\n        \"\"\"\n        Example: combine toxicity (higher = \u201cworse\u201d =&gt; higher reward\n        for red team) and a short-length preference.\n        \"\"\"\n        # Batch moderation (vectorized)\n        atk_tox = self.moderator.moderate(attacks)    # List[float] in [0,1]\n        rsp_tox = self.moderator.moderate(responses)  # List[float] in [0,1]\n\n        # Simple scalar reward: emphasize target toxicity, lightly weight attack length\n        w_rsp, w_atk, len_pen = 1.0, 0.3, 0.002\n        rewards = []\n        for a, r, atk_txt in zip(atk_tox, rsp_tox, attacks):\n            rew = w_rsp * r + w_atk * a - len_pen * len(atk_txt)\n            rewards.append(float(rew))\n        return rewards\n\n    # ======================================================================\n    # ============= Model-agnostic adapters (edit for non-HF!) =============\n    # ======================================================================\n\n    def _encode(self, texts: Sequence[str], *, for_attacker: bool) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"\n        Return {'input_ids': LongTensor[B, T]} with left padding and context cap.\n        Replace this body to use your own tokenizer/byte encoder.\n        \"\"\"\n        # Example: using a shared byte-level BPE or your custom tokenizer\n        ids_list: List[List[int]] = [\n            self._text_to_ids(t, for_attacker=for_attacker) for t in texts\n        ]\n        # Left-truncate to fit context window (reserve space for generation if needed)\n        T = max(len(ids) for ids in ids_list)\n        if T &gt; self.max_ctx:\n            ids_list = [ids[-self.max_ctx:] for ids in ids_list]\n            T = self.max_ctx\n\n        # Left-pad with PAD to length T\n        padded = []\n        for ids in ids_list:\n            pad = [self.pad_token_id] * (T - len(ids))\n            padded.append(pad + ids)\n        input_ids = torch.tensor(padded, dtype=torch.long)\n        return {\"input_ids\": input_ids}\n\n    def _decode(self, ids: torch.Tensor, *, for_attacker: bool) -&gt; List[str]:\n        \"\"\"\n        Convert token ids back to text. Implement with your tokenizer/codec.\n        \"\"\"\n        return [self._ids_to_text(row.tolist(), for_attacker=for_attacker) for row in ids]\n\n    def _generate_ids(\n        self,\n        model,\n        input_ids: torch.Tensor,\n        *,\n        max_new_tokens: int = 32,\n        temperature: float = 1.0,\n        top_p: float = 0.9,\n        top_k: int = 50,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Autoregressive sampling loop that works for any model exposing logits.\n        Replace with your model's native `.generate(...)` if available.\n        \"\"\"\n        model.train(False)\n        B, T = input_ids.shape\n        ids = input_ids.to(self.device)\n        with torch.no_grad():\n            for _ in range(max_new_tokens):\n                logits = self._forward_logits(model, ids)[:, -1, :]  # [B, V]\n                # Temperature + nucleus/top-k (toy implementation)\n                logits = logits / max(1e-6, temperature)\n                probs = F.softmax(logits, dim=-1)\n\n                if top_k is not None and top_k &gt; 0:\n                    topk_vals, topk_idx = torch.topk(probs, k=min(top_k, probs.size(-1)))\n                    mask = torch.ones_like(probs) * 0.0\n                    mask.scatter_(1, topk_idx, topk_vals)\n                    probs = mask\n                    probs = probs / probs.sum(dim=-1, keepdim=True)\n\n                if top_p is not None and 0 &lt; top_p &lt; 1:\n                    sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n                    cumsum = torch.cumsum(sorted_probs, dim=-1)\n                    keep = cumsum &lt;= top_p\n                    # ensure at least one token\n                    keep[..., 0] = True\n                    filtered = torch.zeros_like(probs)\n                    filtered.scatter_(1, sorted_idx, keep.float() * sorted_probs)\n                    probs = filtered / filtered.sum(dim=-1, keepdim=True)\n\n                next_ids = torch.multinomial(probs, num_samples=1)  # [B, 1]\n                ids = torch.cat([ids, next_ids], dim=1)\n        return ids\n\n    def _forward_logits(self, model, input_ids: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass: returns logits [B, T, V].\n        Implement using your model\u2019s API.\n        \"\"\"\n        # Example for a PyTorch nn.Module that takes `input_ids`\n        return model(input_ids)  # must return [B, T, V]\n\n    # ---------- Conditional log-prob utility ----------\n    def _conditional_logprob_sum(\n        self,\n        model,\n        contexts: Sequence[str],\n        continuations: Sequence[str],\n        *,\n        for_attacker: bool,\n        require_grad: bool,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Compute log P(continuation | context), summed over continuation tokens.\n        \"\"\"\n        assert len(contexts) == len(continuations)\n        # Encode each part separately so we can mask out context tokens\n        ctx_ids = [self._text_to_ids(t, for_attacker=for_attacker) for t in contexts]\n        cont_ids = [self._text_to_ids(t, for_attacker=for_attacker) for t in continuations]\n\n        # Build combined sequence per item and left-pad to same length\n        combined = [c + d for c, d in zip(ctx_ids, cont_ids)]\n        T = min(self.max_ctx, max(len(x) for x in combined))\n        padded, mask_ctx = [], []\n        for c, d in zip(ctx_ids, cont_ids):\n            seq = (c + d)[-T:]  # left-truncate to window\n            # context length after truncation:\n            ctx_len = max(0, min(len(c), T - len(d)))\n            # left-pad\n            pad = [self.pad_token_id] * (T - len(seq))\n            padded.append(pad + seq)\n            # mask True *only* over continuation tokens\n            mask_ctx.append([False] * (T - len(seq) + ctx_len) + [True] * (len(seq) - ctx_len))\n\n        input_ids = torch.tensor(padded, dtype=torch.long, device=self.device)\n        cont_mask = torch.tensor(mask_ctx, dtype=torch.bool, device=self.device)\n\n        # Forward\n        input_ids_ = input_ids.detach().clone()\n        input_ids_.requires_grad_(require_grad)  # gradients flow through attacker only\n        logits = self._forward_logits(model, input_ids_)  # [B, T, V]\n        logprobs = F.log_softmax(logits[:, :-1, :], dim=-1)         # next-token log P\n        next_tokens = input_ids_[:, 1:].unsqueeze(-1)               # [B, T-1, 1]\n        token_logp = logprobs.gather(-1, next_tokens).squeeze(-1)   # [B, T-1]\n\n        # Mask out context tokens; sum only over continuation\n        token_logp = token_logp.masked_fill(~cont_mask[:, 1:], 0.0)\n        lp_sum = token_logp.sum(dim=-1)  # [B]\n        return lp_sum\n\n    # ---------- Replace these with your tokenizer/codec ----------\n    def _text_to_ids(self, text: str, *, for_attacker: bool) -&gt; List[int]:\n        \"\"\"\n        Convert text-&gt;ids. Replace with your tokenizer or byte codec.\n        Ensure EOS exists; consider using eos as pad for GPT-style models.\n        \"\"\"\n        # Toy byte-level \u201ctokenizer\u201d (DON\u2019T use in production):\n        ids = [min(255, ord(ch)) for ch in text]\n        ids = ids + [self.eos_token_id]\n        return ids\n\n    def _ids_to_text(self, ids: List[int], *, for_attacker: bool) -&gt; str:\n        # Inverse of the toy tokenizer (again, replace with your real one)\n        # Strip left-padding PADs:\n        ids = [t for t in ids if t != self.pad_token_id]\n        # Stop at first EOS if desired\n        if self.eos_token_id in ids:\n            ids = ids[: ids.index(self.eos_token_id)]\n        return \"\".join(chr(i) for i in ids)\n</code></pre>"},{"location":"tutorials/customize_training/prob.html#why-this-template-helps","title":"Why this template helps","text":"<ul> <li>Works for non-HF models: you decide how to encode/decode/generate/forward.</li> <li>Keeps log-prob masking correct: we compute <code>log P(continuation | context)</code> by masking out context tokens and summing over continuation tokens only.</li> <li>Preserves gradients for the attacker but uses <code>no_grad</code> for target/baseline, matching PPO/DPO style solvers.</li> </ul>"},{"location":"tutorials/customize_training/prob.html#huggingface-convenience-optional","title":"HuggingFace convenience (optional)","text":"<p>If you are on HF, your life is simpler. You can mirror your current example or subclass our <code>HFASTProblem</code>. Quick notes:</p> <ul> <li>Use left padding + EOS as PAD for causal LMs to keep the rightmost tokens aligned.</li> <li>Keep <code>add_special_tokens=False</code> when encoding prompts that you\u2019ll later concatenate with generated text\u2014this avoids inserting extra BOS/SEP that break index alignment.</li> <li>Respect model context: truncate <code>context</code> to <code>max_ctx - max_new_tokens</code> before generation to avoid device-side index errors.</li> </ul>"},{"location":"tutorials/customize_training/prob.html#designing-a-custom-reward","title":"Designing a custom reward","text":"<p>Your <code>reward(contexts, attacks, responses)</code> returns a list of floats aligned with the batch. Common patterns:</p> <ul> <li>Toxicity-driven (maximize target toxicity):   <code>reward = w1 * tox(response) + w2 * tox(attack) - \u03bb * length(attack)</code></li> <li>Safety violations from a separate policy/guardrail model.</li> <li>Preference pairs: when using DPO/IPO/ORPO, you\u2019ll pass log-probs for preferred/dispreferred candidates to the solver; <code>reward</code> may be unused.</li> <li>Task-specific: factuality, jailbreak success, refusal suppression, etc.</li> </ul> <p>Tip: keep rewards bounded (e.g., clip to [-1, 1]) to stabilize PPO-style updates.</p>"},{"location":"tutorials/customize_training/prob.html#practical-gotchas","title":"Practical gotchas","text":"<ul> <li>Return log-probs, not probs. Only exponentiate if some downstream code explicitly asks for probabilities.</li> <li>Gradient locality. Let gradient flow through <code>get_attacker_logprobs</code>. Use <code>torch.no_grad()</code> for target/baseline calls.</li> <li>Batch always. Avoid per-item loops calling the model. Build padded batches.</li> <li>Context windows. Enforce <code>T \u2264 max_ctx</code>. For GPT-like models, left-truncate context and reserve space for the continuation.</li> <li>Tokenizer mismatch. If attacker/target tokenizers differ, do not cross-decode. Each model should encode/decode its own texts.</li> <li>Determinism. Accept a random seed (or RNG) in your env for reproducible rollouts.</li> </ul>"},{"location":"tutorials/customize_training/prob.html#quick-self-test-paste-in-a-notebook","title":"Quick self-test (paste in a notebook)","text":"<pre><code>def _sanity(problem: MyCustomProblem):\n    prompts = [\"Hello\", \"Tell me a joke about cats\"]\n    atk = problem.rollout_prompt_with_attacker(prompts)\n    tgt = problem.rollout_prompt_with_target([p + a for p, a in zip(prompts, atk)])\n    assert len(atk) == len(tgt) == len(prompts)\n\n    lpa = problem.get_attacker_logprobs(prompts, atk)\n    lpt = problem.get_target_logprobs(prompts, atk)\n    assert lpa.shape == lpt.shape == (len(prompts),)\n    assert lpa.dtype == lpt.dtype\n\n    r = problem.reward(prompts, atk, tgt)\n    assert isinstance(r, list) and len(r) == len(prompts)\n    print(\"\u2713 problem interface looks good\")\n\n# _sanity(MyCustomProblem(...))\n</code></pre>"},{"location":"tutorials/customize_training/prob.html#when-to-use-astproblem-vs-hfastproblem","title":"When to use <code>ASTProblem</code> vs <code>HFASTProblem</code>","text":"<ul> <li>Non-HF or heavily customized model stack \u2192 start from the template above (subclass <code>ASTProblem</code>).</li> <li>HF Transformers \u2192 subclass <code>HFASTProblem</code> (less boilerplate) or adapt the template by replacing <code>_encode/_decode/_generate_ids/_forward_logits</code> with HF calls.</li> </ul> <p>With this guide + template, users can wire up any autoregressive model, keep gradients and masking correct, and plug in custom rewards without touching the harness or environment logic.</p>"},{"location":"tutorials/customize_training/problems.html","title":"How to Create a Custom Problem Class for Red-Teaming with ASTRA-RL","text":"<p>The <code>Problem</code> class defines the core logic of how attacker-target interactions occur in reinforcement-learning-based red-teaming. It specifies:</p> <ul> <li>How the attacker and target interact and advance the conversation state.</li> <li>How reward signals are computed from attacker-target interactions.</li> <li>How a model rollout step is performed.</li> <li>What models and tokenizers are used for attacker, target, and baseline (if applicable).</li> </ul> <p>By subclassing the <code>Problem</code> or <code>ASTProblem</code> class, you can customize any of these aspects to fit your particular red-teaming scenario, algorithm, or dataset. In general, we reccomend subclassing from the ASTProblem or HFASTProblem whenever you can and then over-writing methods or definitions to suite your needs. </p> <p>This guide will show you how to:</p> <ul> <li>Create your own custom subclass.</li> <li>Integrate your own models and tokenizers.</li> <li>Customize reward computation and rollout logic.</li> </ul>"},{"location":"tutorials/customize_training/problems.html#understanding-the-base-class","title":"Understanding the Base Class","text":"<p>The base <code>ASTProblem</code> (source code astra-rl/src/astra_rl/methods/ast_problem.py) class provides default implementations  suitable for the ASTPrompter approach:</p> <ul> <li><code>advance</code>: Defines how a prompt advances given an attacker action and a target response.</li> <li><code>reward</code>: Defines how rewards are calculated from attacker-target interactions, using toxicity scoring and perplexity measures.</li> <li><code>rollout_prompt_with_attacker</code> / <code>rollout_prompt_with_target</code>: Methods to generate attacker and target model outputs from a given context.</li> <li><code>parameters</code>: Specifies model parameters for optimization.</li> </ul> <p>If possible, you should subclass this base class to preserve and extend this default behavior. If you want to change a method, simply define it in your subclass and it will over-right the original implementation while preserving the rest of the base class functionality.</p>"},{"location":"tutorials/customize_training/problems.html#how-to-create-a-subclass-with-custom-modelstokenizers","title":"How to create a subclass with custom models/tokenizers","text":"<p>To subclass your own <code>Problem</code>, follow this template:</p> <pre><code>from astra_rl.methods.ast_problem import ASTProblem\nfrom astra_rl.core.moderator import Moderator\n\nclass MyCustomProblem(ASTProblem):\n    def __init__(self, moderator: Moderator[str, str], my_custom_param: float = 1.0):\n        super().__init__(moderator)\n        self.my_custom_param = my_custom_param\n\n    def advance(self, state: str, action: str, response: str) -&gt; str:\n        # Example: Simply concatenate with separators\n        return f\"{state}\\n[Attacker]: {action}\\n[Target]: {response}\"\n\n    def reward(self, contexts, attacks, responses):\n        # Implement your own reward logic here\n        scores = self.moderator.moderate(responses)\n        return [score * self.my_custom_param for score in scores]\n\n    def rollout_prompt_with_attacker(self, prompts):\n        # Implement custom attacker rollout logic\n        raise NotImplementedError\n\n    def rollout_prompt_with_target(self, prompts):\n        # Implement custom target rollout logic\n        raise NotImplementedError\n\n    def parameters(self):\n        # Implement if your problem has trainable model parameters\n        return []\n</code></pre>"},{"location":"tutorials/customize_training/problems.html#step-3-integrating-your-own-models-attacker-target-or-baseline","title":"Step 3: Integrating Your Own Models (Attacker, Target, or Baseline)","text":"<p>If you are using huggingface models, save time by subclassing from our HFASTProblem base class which takes in any huggingface model names for the attacker, target, and baseline. However, you can also integrate any pretrained language model by loading the model and tokenizer in your constructor. If you integrate your own custom model, you must ensure that the corresponding rollout method(s) (rollout_prompt_with_(MODEL) more info below) and the corresponding (.get_(MODEL)_logprobs) method(s) are updated to correctly integrate your custom model. Here's how to do it clearly and correctly:</p> <pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\n# Create your subclass from the base Problem (most bare-boned) or the base ASTPrompter class (takes care of rollout and log probability methods)\nclass MyHuggingFaceProblem(ASTProblem):\n    def __init__(self, attacker_model_id: str, target_model_id: str, moderator, device=\"cuda\"):\n        super().__init__(moderator)\n        self.device = device\n\n        # Load your custom models and tokenizers\n        self.attacker = AutoModelForCausalLM.from_pretrained(attacker_model_id).to(self.device)\n        self.attacker_tokenizer = AutoTokenizer.from_pretrained(attacker_model_id)\n\n        self.target = AutoModelForCausalLM.from_pretrained(target_model_id).to(self.device)\n        self.target_tokenizer = AutoTokenizer.from_pretrained(target_model_id)\n\n    # required method to perform one step of a rollout in a batched manner: must take in a state (eg. conversation so far) and return a list of continuations (attacker utterances) in the corresponding order\n    def rollout_prompt_with_attacker(self, prompts):\n        inputs = self.attacker_tokenizer(prompts, return_tensors=\"pt\", padding=True).to(self.device)\n        outputs = self.attacker.generate(**inputs, max_new_tokens=32)\n        generated_texts = self.attacker_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n        continuations = [gen[len(prompt):] for prompt, gen in zip(prompts, generated_texts)]\n        return continuations\n\n    def rollout_prompt_with_target(self, prompts):\n        inputs = self.target_tokenizer(prompts, return_tensors=\"pt\", padding=True).to(self.device)\n        outputs = self.target.generate(**inputs, max_new_tokens=32)\n        generated_texts = self.target_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n        continuations = [gen[len(prompt):] for prompt, gen in zip(prompts, generated_texts)]\n        return continuations\n\n    def parameters(self):\n        return self.attacker.parameters()\n</code></pre>"},{"location":"tutorials/customize_training/problems.html#step-4-customizing-rollout-logic","title":"Step 4: Customizing Rollout Logic","text":"<p>To customize how attacker-target rollouts are performed, override these methods clearly:</p> <p>rollout_prompt_with_attacker(prompts: Sequence[str]) \u2192 Sequence[str]</p> <p>rollout_prompt_with_target(prompts: Sequence[str]) \u2192 Sequence[str]</p> <p>For example, you might use sampling strategies, temperature adjustments, or custom stopping criteria:</p> <pre><code>def rollout_prompt_with_attacker(self, prompts):\n    inputs = self.attacker_tokenizer(prompts, padding=True, return_tensors=\"pt\").to(self.device)\n    outputs = self.attacker.generate(\n        **inputs,\n        do_sample=True,\n        max_new_tokens=50,\n        temperature=0.8,\n        top_k=50,\n        top_p=0.9\n    )\n    texts = self.attacker_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n    return [text[len(prompt):] for prompt, text in zip(prompts, texts)]\n</code></pre>"},{"location":"tutorials/customize_training/problems.html#step-5-customizing-reward-logic","title":"Step 5: Customizing Reward Logic","text":"<p>Override the reward method to compute your custom reward signal. Typically, you'll combine toxicity, relevance, perplexity, or other metrics:</p> <pre><code>def reward(self, contexts, attacks, responses):\n    attack_scores = self.moderator.moderate(attacks)\n    response_scores = self.moderator.moderate(responses)\n    combined = [(a_score + r_score) / 2.0 for a_score, r_score in zip(attack_scores, response_scores)]\n    return combined\n\n</code></pre>"},{"location":"tutorials/customize_training/problems.html#tips-and-best-practices","title":"Tips and Best Practices:","text":"<p>Always clearly document parameters and logic for your custom problem class.</p> <p>Ensure models and tokenizers are device-aware (e.g., GPU-compatible).</p> <p>Thoroughly test your rollouts independently before integrating them into the full RL loop.</p> <p>For debugging, add verbose logging to track input-output sequences.</p>"},{"location":"tutorials/customize_training/problems.html#further-reading-and-examples","title":"Further Reading and Examples","text":"<p>Default ASTPrompter implementation: ASTProblem</p> <p>HuggingFace-compatible subclass example: HFASTProblem</p> <p>Environment customization guide: Custom Environments</p>"}]}
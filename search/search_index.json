{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Adaptive Stress Testing for Robust AI &amp; Reinforcement Learning (ASTRA-RL)","text":"<p>Welcome to the ASTRA-RL toolbox documentation! This documentation provides an overview of the ASTRA-RL toolbox, its features, and how to use it effectively.</p> <p>Warning</p> <p>This documentation (and project) is a work in progress. If you have any questions or suggestions, please feel free to open an issue on the GitHub repository</p>"},{"location":"api/index.html","title":"Library API","text":"<p>This section provides detailed information about all the classes, functions, and modules available in the ASTRA-RL toolbox. Each entry includes a description of its purpose, parameters, and usage examples.</p> <p>This documentation is generated automatically from the codebase using docstrings and comments, ensuring that it stays up-to-date with the latest changes.</p>"},{"location":"tutorials/index.html","title":"Tutorials","text":"<p>This section provides step-by-step guides and examples to help you get started with the ASTRA-RL toolbox. Each tutorial covers a specific aspect of the toolbox, from basic usage to advanced features.</p>"},{"location":"tutorials/quick_start_training.html","title":"Quick start training","text":"<p>Do you want to train a Huggingface adversary using an ASTRA-supported algorithm (DPO, IPO, PPO) and problem formulation (ASTPrompter, Perez, MALIBU, Hong)? Then this quick start guide will walk you through every step to training a red-teaming adversary.  TARGET, BASELINE, ATTACKER</p> <p>1) Set-up See the README for more details and guidance behind the installation process.</p> <pre><code># Install the ASTRA-RL toolbox\npip install astra-rl\n# Clone the repository\ngit clone git@github.com:sisl/astra-rl.git\ncd astra-rl\n# Sync package dependencies\nuv sync --dev\n# Install pre-commit hooks:\nuv run pre-commit install\n</code></pre> <p>2) Create a python file for your training code (ie. mytrain.py)</p> <p>3) Import required classes, wrappers, and functions</p> <pre><code># import your favorite optimization algorithm. We suggest Adam!\nfrom torch.optim import AdamW\n# import a pre-instantiaed environment class, RL training algorithm, Moderator, and training harness class\nfrom astra_rl import ASTEnvironment, DPO, DetoxifyModerator, Harness\n# import the wrapper that supports Huggingface models for ASTPrompter-style adversarial training\nfrom astra_rl.ext.transformers import HFASTProblem\n</code></pre> <p>4) Load your Training Data (initial prompts) To train your adversary, you will need to provide a large dataset of comma-seperated strings that will serve as the prompts that initiate adversarial-defender conversation rollouts during training. These rollouts (scored by the moderator) will serve as the training data for the adversary, teaching the adversary what actions it should take to ellicit harmful responses from the defender. Therefore, it is important that the initial prompts you provide are relavent to the red-teaming scenario you care about. For example, if you want to red-team medical diagnosis LMs, your initial prompts should reflect the prompts a medical diagnosis LM would likely see during operation: \"I have a cut that is swollen and red. What do you think is wrong?\" ect.</p> <p>The supported framework (ASTPrompter) red-teams for harmful behaviors in conversational LMs and therefore uses the ConvoKit Reddit corpus (small) to serve as the initial prompts. To use this data, make sure you have the prompts_reddit_train.json file in your local directory and read the prompts in as a comma seperated list as shown below.</p> <pre><code>with open(\"prompts_reddit_train.json\") as f:\n        PROMPTS = json.load(f)\n</code></pre> <p>If you would like to use a different dataset to initialize training rollouts, simply load your json file instead! Just make sure that your json file is formatted as a list of comma seperated strings.</p> <p>5) Set your device: If you have acsess to a GPU:</p> <pre><code>DEVICE = \"cuda\" \n</code></pre> <p>This will allow you train larger adversaries and use larger moderator models. Since we want to make this framework available to people with a variety of computational capabilities, we put the occonus of properly allocating recources to you, the user. </p> <p>If you do not have acsess to a GPU, you can still use this toolbox to train smaller adversaries for red teaming! Try finetunning smaller adversarial models such as GPT2 and using lighter-weight moderators (detoxify)!</p> <pre><code>DEVICE = \"cpu\" \n</code></pre> <p>6) Instatiate your problem (choose adversary, defender, reference, and moderator models!) To facilitate your experience, we have created a problem wrapper that supports any hugging face model!</p> <p>Adversary/Defender/Reference Hugging face models:</p> <p>Simply instantiate your problem with the HFASTProblem class and provide the name of the hugging face models you would like to use. Additionally, you will need to provide a moderator that will evalute adversary/defender utterances to determine rewards. We have two moderator classes pre-configured and ready for use: DetoxifyModerator and LlamaGuardModerator()*.</p> <pre><code># example for instatiating a problem with gpt2 as the adversary, defender, andreference model with a detoxify moderator! (cpu-friendly)\nproblem = HFASTProblem(\"gpt2\", \"gpt2\", \"gpt2\", DetoxifyModerator(), DEVICE)\n\n# example for instatiating a problem with gpt2 as the adversary, llama3 as the defender, and reference model with a llamaguard moderator!(gpu necessary)\nproblem = HFASTProblem(\"gpt2\", \"llama??\", \"gpt2\", LlamaGuardModerator(), DEVICE)\n\n</code></pre> <p>Want to use a non-hugging face attacker, target, or baseline? No Problem! Go to: astra-rl/docs/tutorials/custom_models.md to create a problem subclass that fits your needs!  </p> <p>Want to use a moderator besides llamaguard or detoxify? It's easy! Go to: astra-rl/docs/tutorials/custom_moderators.md to learn how to create a class for your moderator and integrate it into your problem class!</p> <p>7) </p>"},{"location":"tutorials/running_an_evaluation.html","title":"Running An Evaluation","text":"<p>This tutorial will guide you through the process of running an evaluation using an existing evaluator model. The evaluator is a model that can be used to assess the quality of other models' outputs. </p> <p>For the purposes of this tutorial, we will assume that you have already trained an evaluator model from the Training an Evaluator tutorial.mk</p>"},{"location":"tutorials/training_an_evaluator.html","title":"Training an Evaluator","text":"<p>This tutorial will guide you through the process of training a custom model evaluator using the ASTRA-RL toolbox. The evaluator is a model that can be used to assess the quality of other models' outputs.</p>"}]}
{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Adaptive Stress Testing for Robust AI &amp; Reinforcement Learning (ASTRA-RL)","text":"<p>Warning</p> <p>This documentation (and project) is a work in progress. If you have any questions or suggestions, please feel free to open an issue on the GitHub repository</p> <p>ASTRA-RL is a python toolbox for training and evaluating language models and generative AI systems that use textual inputs. It provides a set of tools for training, evaluating, and analyzing language models, with a focus on applying reinforcement learning based refinement techniques to improve evaluator model performance.</p>"},{"location":"index.html#installation","title":"Installation","text":"<p>To install the ASTRA-RL toolbox, you can use pip. The package is available on PyPI, so you can install it directly from there.</p> <pre><code>pip install astra-rl\n</code></pre> <p>You can then import the library in your Python code:</p> <pre><code>import astra_rl\n# or\nimport astra_rl as astral\n</code></pre>"},{"location":"index.html#using-the-documentation","title":"Using the Documentation","text":"<p>This documentation is structured to help you quickly find the information you need. The main sections include:</p> <ul> <li>Getting Started: An introduction to the toolbox, including installation and basic usage.</li> <li>Tutorials: Step-by-step guides to help you understand how to use the toolbox effectively.</li> <li>API Reference: Detailed information about the classes, functions, and modules available in the</li> </ul>"},{"location":"contributing.html","title":"Contributing","text":"<p>Warning</p> <p>This documentation (and project) is a work in progress. If you have any questions or suggestions, please feel free to open an issue on the GitHub repository</p> <p>Thank you for your interest in contributing to ASTRA-RL! We welcome contributions from the community to help improve the toolbox and its documentation.</p>"},{"location":"contributing.html#how-to-contribute","title":"How to Contribute","text":"<p>1) Fork the Repository: Start by forking the ASTRA-RL repository on GitHub.</p> <p>2) Clone Your Fork: Clone your forked repository to your local machine:</p> <pre><code>git clone https://github.com/YOUR_USERNAME/astra-rl.git\n</code></pre> <p>3) Create a Branch: Create a new branch for your changes:</p> <pre><code>git checkout -b feature/your-feature-name\n</code></pre> <p>4) Make Changes: Make your changes to the code or documentation. Ensure that your code adheres to the project's coding standards and style guidelines.</p> <p>5) Run Tests: Before committing your changes, run the tests to ensure everything is working correctly:</p> <pre><code>uv run pytest\n</code></pre> <p>If your tests require a GPU, you can run them with the <code>--gpu</code> option to enable GPU tests:</p> <pre><code>uv run pytest --gpu\n</code></pre> <p>6) Commit Your Changes: Commit your changes with a descriptive commit message:</p> <pre><code>git commit -m \"Add feature: your-feature-name\"\n</code></pre> <p>7) Push Your Changes: Push your changes to your forked repository:</p> <pre><code>git push origin feature/your-feature-name\n</code></pre> <p>8) Create a Pull Request: Go to the original repository on GitHub and create a pull request (PR) from your branch. Provide a clear description of the changes you made and any relevant context. Fill out the PR template to help us understand your changes better.</p>"},{"location":"contributing.html#development","title":"Development","text":"<p>This section provides instructions for setting up the development environment and running tests.</p> <p>To start, we STRONGLY recommend using uv to manage your Python environment. This will ensure that you have the correct dependencies and versions installed.</p>"},{"location":"contributing.html#setting-up-the-development-environment","title":"Setting Up the Development Environment","text":"<p>Step 1: Clone the repository:</p> <pre><code>git clone https://github.com/sisl/astra-rl.git\ncd astra-rl\n</code></pre> <p>Step 2: Sync package dependencies:</p> <pre><code>uv sync --dev\n</code></pre> <p>This will create a <code>.venv</code> directory in the project root with all the necessary dependencies installed.</p> <p>Step 3: Install pre-commit hooks:</p> <pre><code>uv run pre-commit install\n</code></pre> <p>This will ensure that the linter (<code>ruff</code>), formatter (<code>ruff</code>), and type checker (<code>mypy</code>) is happy with your code every time you commit.</p>"},{"location":"contributing.html#running-tests","title":"Running Tests","text":"<p>Assuming you've set up your environment using <code>uv</code>, you can run the tests using the following command:</p> <pre><code>pytest\n</code></pre> <p>or </p> <pre><code>uv run pytest\n</code></pre> <p>To generate local coverage reports, you can use:</p> <pre><code>uv run coverage run -m pytest\nuv run coverage report # Generate CLI report\nuv run coverage html   # Generate HTML report\n</code></pre>"},{"location":"contributing.html#running-tests-with-gpu","title":"Running Tests with GPU","text":"<p>Some tests may require a GPU to run. You can enable GPU tests by passing the <code>--gpu</code> option:</p> <pre><code>uv run pytest --gpu\n</code></pre> <p>These tests will be skipped by default unless you specify the <code>--gpu</code> option.</p>"},{"location":"contributing.html#generating-documentation","title":"Generating Documentation","text":"<p>To generate the documentation, you can use the following command:</p> <pre><code>uv run mkdocs serve\n</code></pre> <p>This will build the documentation and start a local server. You can then view the documentation in your web browser.</p>"},{"location":"getting_started.html","title":"Getting Started","text":"<p>Warning</p> <p>This documentation (and project) is a work in progress. If you have any questions or suggestions, please feel free to open an issue on the GitHub repository</p>"},{"location":"api/index.html","title":"Library API","text":"<p>This section provides detailed information about all the classes, functions, and modules available in the ASTRA-RL toolbox. Each entry includes a description of its purpose, parameters, and usage examples.</p> <p>This documentation is generated automatically from the codebase using docstrings and comments, ensuring that it stays up-to-date with the latest changes.</p>"},{"location":"api/algorithms/dpo.html","title":"DPO","text":""},{"location":"api/algorithms/dpo.html#astra_rl.algorithms.dpo","title":"<code>astra_rl.algorithms.dpo</code>","text":""},{"location":"api/algorithms/dpo.html#astra_rl.algorithms.dpo.DPO","title":"<code>DPO</code>","text":"<p>               Bases: <code>Algorithm[StateT, ActionT, DPOStep[StateT, ActionT], DPOBatch[StateT, ActionT]]</code>, <code>Generic[StateT, ActionT]</code></p> <p>Direct Preference Optimization (DPO) algorithm.</p> Source code in <code>src/astra_rl/algorithms/dpo.py</code> <pre><code>class DPO(\n    Algorithm[StateT, ActionT, DPOStep[StateT, ActionT], DPOBatch[StateT, ActionT]],\n    Generic[StateT, ActionT],\n):\n    \"\"\"Direct Preference Optimization (DPO) algorithm.\"\"\"\n\n    def __init__(self, problem: Problem[StateT, ActionT], beta: float = 0.1):\n        super().__init__(problem)\n\n        self.beta = beta\n\n    def flatten(\n        self, graph: Graph[StateT, ActionT]\n    ) -&gt; Sequence[DPOStep[StateT, ActionT]]:\n        # in DPO, we sample from each branch the most rewarded\n        # and least rewarded actions in order to use them as our contrastive\n        # pairs.\n\n        pairs: List[DPOStep[StateT, ActionT]] = []\n        bfs = [graph.children]\n        while len(bfs):\n            front = bfs.pop(0)\n            sorted_list = sorted(list(front), key=lambda x: x.reward, reverse=True)\n\n            if len(sorted_list) &lt; 2:\n                # if there is no pair, we skip this node\n                continue\n\n            pos_entry = sorted_list[0]\n            neg_entry = sorted_list[-1]\n\n            assert pos_entry.context == neg_entry.context, (\n                \"paired rollouts for DPO must share the same prefix!\"\n            )\n\n            pairs.append(\n                DPOStep(\n                    prefix=pos_entry.context,\n                    suffix_pos=pos_entry.attack,\n                    suffix_neg=neg_entry.attack,\n                )\n            )\n\n            for i in sorted_list:\n                bfs.append(i.children)\n\n        return pairs\n\n    @staticmethod\n    def collate_fn(x: Sequence[DPOStep[StateT, ActionT]]) -&gt; DPOBatch[StateT, ActionT]:\n        prefixes = [i.prefix for i in x]\n        suffix_pos = [i.suffix_pos for i in x]\n        suffix_neg = [i.suffix_neg for i in x]\n\n        return DPOBatch(prefixes=prefixes, suffix_pos=suffix_pos, suffix_neg=suffix_neg)\n\n    def step(\n        self, batch: DPOBatch[StateT, ActionT]\n    ) -&gt; tuple[torch.Tensor, Dict[Any, Any]]:\n        attacker_logprobs_win = self.problem._get_attacker_logprobs_and_validate(\n            batch.prefixes, batch.suffix_pos\n        ).sum(dim=-1)  # Sum per-token logprobs to get sequence logprobs\n        attacker_logprobs_loss = self.problem._get_attacker_logprobs_and_validate(\n            batch.prefixes, batch.suffix_neg\n        ).sum(dim=-1)  # Sum per-token logprobs to get sequence logprobs\n        baseline_logprobs_win = self.problem._get_baseline_logprobs_and_validate(\n            batch.prefixes, batch.suffix_pos\n        ).sum(dim=-1)  # Sum per-token logprobs to get sequence logprobs\n        baseline_logprobs_loss = self.problem._get_baseline_logprobs_and_validate(\n            batch.prefixes, batch.suffix_neg\n        ).sum(dim=-1)  # Sum per-token logprobs to get sequence logprobs\n\n        # https://github.com/eric-mitchell/direct-preference-optimization/blob/ \\\n        # f8b8c0f49dc92a430bae41585f9d467d3618fe2f/trainers.py#L70-L87\n        pi_logratios = attacker_logprobs_win - attacker_logprobs_loss\n        ref_logratios = baseline_logprobs_win - baseline_logprobs_loss\n        logits = pi_logratios - ref_logratios\n\n        loss = -F.logsigmoid(self.beta * logits)\n\n        # Calculate addition quantities\n        # TODO: CHECK ME for correctness and completion!\n        chosen_rewards = self.beta * (attacker_logprobs_win - baseline_logprobs_win)\n        rejected_rewards = self.beta * (attacker_logprobs_loss - baseline_logprobs_loss)\n        reward_accuracies = (chosen_rewards &gt; rejected_rewards).float()\n        reward_margin = chosen_rewards - rejected_rewards\n\n        logging_dict: Dict[Any, Any] = {\n            \"training/loss\": loss.mean().cpu().item(),\n            \"reward/chosen_rewards\": chosen_rewards.mean().cpu().item(),\n            \"reward/rejected_rewards\": rejected_rewards.mean().cpu().item(),\n            \"reward/reward_accuracies\": reward_accuracies.mean().cpu().item(),\n            \"reward/reward_margin\": reward_margin.mean().cpu().item(),\n            \"policy/logprobs_chosen\": attacker_logprobs_win.mean()\n            .detach()\n            .cpu()\n            .item(),\n            \"policy/logprobs_rejected\": attacker_logprobs_loss.mean()\n            .detach()\n            .cpu()\n            .item(),\n            \"ref/logprobs_chosen\": baseline_logprobs_win.mean().detach().cpu().item(),\n            \"ref/logprobs_rejected\": baseline_logprobs_loss.mean()\n            .detach()\n            .cpu()\n            .item(),\n        }\n        # TODO: Add this from old code?\n        # \"policy/rollout\": wandb.Html(str(r\"&lt;span&gt;\"+batch[\"prompt_win\"][0][0]+\"&lt;/span&gt;&lt;span style='color:Tomato;'&gt;\"+batch[\"prompt_win\"][0][1]+r\"&lt;/span&gt;&lt;span style='color:DodgerBlue'&gt;\"+batch[\"prompt_win\"][0][2]+r\"&lt;/span&gt;\")),\n\n        return loss.mean(), logging_dict\n</code></pre>"},{"location":"api/algorithms/ppo.html","title":"DPO","text":""},{"location":"api/algorithms/ppo.html#astra_rl.algorithms.ppo","title":"<code>astra_rl.algorithms.ppo</code>","text":""},{"location":"api/algorithms/ppo.html#astra_rl.algorithms.ppo.PPO","title":"<code>PPO</code>","text":"<p>               Bases: <code>Algorithm[StateT, ActionT, PPOStep[StateT, ActionT], PPOBatch[StateT, ActionT]]</code>, <code>ABC</code></p> <p>Proximal Policy Optimization (PPO) algorithm with value function.</p> Source code in <code>src/astra_rl/algorithms/ppo.py</code> <pre><code>class PPO(\n    Algorithm[StateT, ActionT, PPOStep[StateT, ActionT], PPOBatch[StateT, ActionT]],\n    ABC,\n):\n    \"\"\"Proximal Policy Optimization (PPO) algorithm with value function.\"\"\"\n\n    def __init__(\n        self,\n        problem: ValueFunctionProblem[StateT, ActionT],\n        clip_range: float = 0.1,\n        vf_loss_coef: float = 1.0,\n    ):\n        super().__init__(problem)\n\n        self.problem: ValueFunctionProblem[StateT, ActionT] = problem\n        self.clip_range = clip_range\n        self.vf_loss_coef = vf_loss_coef\n\n    def flatten(\n        self, graph: Graph[StateT, ActionT]\n    ) -&gt; Sequence[PPOStep[StateT, ActionT]]:\n        # in DPO, we sample from each branch the most rewarded\n        # and least rewarded actions in order to use them as our contrastive\n        # pairs.\n\n        res: List[PPOStep[StateT, ActionT]] = []\n        bfs = [graph.children]\n        while len(bfs):\n            front = bfs.pop(0)\n            if len(list(front)) &lt; 2:\n                # if there is no pair, we skip this node\n                continue\n\n            for i in front:\n                res.append(PPOStep(prefix=i.context, suffix=i.attack, reward=i.reward))\n                bfs.append(i.children)\n\n        return res\n\n    @staticmethod\n    def collate_fn(x: Sequence[PPOStep[StateT, ActionT]]) -&gt; PPOBatch[StateT, ActionT]:\n        prefixes = [i.prefix for i in x]\n        suffix = [i.suffix for i in x]\n        rewards = [i.reward for i in x]\n\n        return PPOBatch(prefix=prefixes, suffix=suffix, reward=rewards)\n\n    def step(\n        self, batch: PPOBatch[StateT, ActionT]\n    ) -&gt; tuple[torch.Tensor, Dict[Any, Any]]:\n        logprobs_attacker = self.problem._get_attacker_logprobs_and_validate(\n            batch.prefix, batch.suffix\n        )\n        logprobs_baseline = self.problem._get_baseline_logprobs_and_validate(\n            batch.prefix, batch.suffix\n        )\n        values = self.problem.value(batch.prefix, batch.suffix)\n\n        # Q(s,a) = R(s,a), which is jank but seems to be the standard\n        # also its bootstrapped without discount throughout the stream\n        Q = (\n            torch.tensor(batch.reward)\n            .to(logprobs_attacker.device)\n            .unsqueeze(-1)\n            .unsqueeze(-1)\n            .repeat(1, *values.shape[1:])\n        )\n        A = Q - values\n\n        # normalize advantages\n        if A.size(-1) == 1:\n            A = ((A - A.mean()) / (A.std() + 1e-8)).squeeze(-1)\n        else:\n            A = (A - A.mean()) / (A.std() + 1e-8)\n        # compute ratio, should be 1 at the first iteration\n        ratio = torch.exp((logprobs_attacker - logprobs_baseline.detach()))\n\n        # compute clipped surrogate lolss\n        policy_loss_1 = A * ratio\n        policy_loss_2 = A * torch.clamp(ratio, 1 - self.clip_range, 1 + self.clip_range)\n        policy_loss_2 = A * torch.clamp(ratio, 1 - 0.1, 1 + 0.1)\n        policy_loss = -(torch.min(policy_loss_1, policy_loss_2)).mean()\n\n        # compute value loss\n        value_loss = F.mse_loss(Q, values)\n\n        # compute final lossvalue_loss\n        loss = policy_loss + self.vf_loss_coef * value_loss\n\n        # create logging dict\n        logging_dict: Dict[Any, Any] = {\n            \"training/loss\": loss.mean().cpu().item(),\n            \"training/policy_loss\": policy_loss.mean().cpu().item(),\n            \"training/value_loss\": value_loss.mean().cpu().item(),\n            \"reward/mean_reward\": torch.tensor(batch.reward).mean().cpu().item(),\n            \"reward/std_reward\": torch.tensor(batch.reward).std().cpu().item(),\n            \"policy/logprobs\": logprobs_attacker.mean().detach().cpu().item(),\n            \"ref/logprobs\": logprobs_baseline.mean().detach().cpu().item(),\n        }\n\n        return loss, logging_dict\n</code></pre>"},{"location":"api/core/algorithm.html","title":"Algorithm","text":""},{"location":"api/core/algorithm.html#astra_rl.core.algorithm","title":"<code>astra_rl.core.algorithm</code>","text":"<p>algorithm.py</p>"},{"location":"api/core/algorithm.html#astra_rl.core.algorithm.Algorithm","title":"<code>Algorithm</code>","text":"<p>               Bases: <code>ABC</code>, <code>Generic[StateT, ActionT, Step, Batch]</code></p> <p>An Algorithm used for performing training.</p> <p>Specifically, the Algorithm object is responsible for encoding how a particular rollout graph becomes processed into a loss which updates the weights of the model. To implement its children, you basically call self.problem's various methods to push values through the network.</p> <p>Attributes:</p> Name Type Description <code>problem</code> <code>Problem</code> <p>The problem instance that defines the environment and actions.</p> Generics <p>StateT (type): The type of the state in the environment. ActionT (type): The type of the action in the environment. Step (type): The type of a single step in the environment. Batch (type): The type of a batch of steps, passed to the .step() function for gradient.</p> Source code in <code>src/astra_rl/core/algorithm.py</code> <pre><code>class Algorithm(ABC, Generic[StateT, ActionT, Step, Batch]):\n    \"\"\"An Algorithm used for performing training.\n\n    Specifically, the Algorithm object is responsible for encoding\n    how a particular rollout graph becomes processed into a loss\n    which updates the weights of the model. To implement its children,\n    you basically call self.problem's various methods to push values\n    through the network.\n\n\n    Attributes:\n        problem (Problem): The problem instance that defines the environment and actions.\n\n    Generics:\n        StateT (type): The type of the state in the environment.\n        ActionT (type): The type of the action in the environment.\n        Step (type): The type of a single step in the environment.\n        Batch (type): The type of a batch of steps, passed to the .step() function for gradient.\n    \"\"\"\n\n    def __init__(self, problem: Problem[StateT, ActionT]):\n        self.problem = problem\n\n    @abstractmethod\n    def flatten(self, graph: Graph[StateT, ActionT]) -&gt; Sequence[Step]:\n        \"\"\"Process a rollout graph into a sequence of steps.\n\n        Args:\n            graph (Graph[StateT, ActionT]): The graph to flatten.\n\n        Returns:\n            Sequence[Step]: A sequence of steps representing the flattened graph.\n        \"\"\"\n        pass\n\n    @staticmethod\n    @abstractmethod\n    def collate_fn(batch: Sequence[Step]) -&gt; Batch:\n        \"\"\"The collate_fn for torch dataloaders for batching.\n\n        We use this as the literal collate_fn to a torch DataLoader, and\n        it is responsible for emitting well-formed batches of data.\n\n        Args:\n            batch (Sequence[Step]): A sequence of steps to collate.\n\n        Returns:\n            Batch: A batch of data ready for processing using .step().\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def step(self, batch: Batch) -&gt; tuple[torch.Tensor, Dict[Any, Any]]:\n        \"\"\"Take a batch and compute loss of this batch.\n\n        Args:\n            batch (Batch): A batch of data to process.\n\n        Returns:\n            tuple[torch.Tensor, Dict[Any, Any]]: A tuple containing:\n                - torch.Tensor: The loss computed by the algorithm (for current batch).\n                - Dict[Any, Any]: Additional information for logging.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/core/algorithm.html#astra_rl.core.algorithm.Algorithm.collate_fn","title":"<code>collate_fn(batch)</code>  <code>abstractmethod</code> <code>staticmethod</code>","text":"<p>The collate_fn for torch dataloaders for batching.</p> <p>We use this as the literal collate_fn to a torch DataLoader, and it is responsible for emitting well-formed batches of data.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Sequence[Step]</code> <p>A sequence of steps to collate.</p> required <p>Returns:</p> Name Type Description <code>Batch</code> <code>Batch</code> <p>A batch of data ready for processing using .step().</p> Source code in <code>src/astra_rl/core/algorithm.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef collate_fn(batch: Sequence[Step]) -&gt; Batch:\n    \"\"\"The collate_fn for torch dataloaders for batching.\n\n    We use this as the literal collate_fn to a torch DataLoader, and\n    it is responsible for emitting well-formed batches of data.\n\n    Args:\n        batch (Sequence[Step]): A sequence of steps to collate.\n\n    Returns:\n        Batch: A batch of data ready for processing using .step().\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/algorithm.html#astra_rl.core.algorithm.Algorithm.flatten","title":"<code>flatten(graph)</code>  <code>abstractmethod</code>","text":"<p>Process a rollout graph into a sequence of steps.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>Graph[StateT, ActionT]</code> <p>The graph to flatten.</p> required <p>Returns:</p> Type Description <code>Sequence[Step]</code> <p>Sequence[Step]: A sequence of steps representing the flattened graph.</p> Source code in <code>src/astra_rl/core/algorithm.py</code> <pre><code>@abstractmethod\ndef flatten(self, graph: Graph[StateT, ActionT]) -&gt; Sequence[Step]:\n    \"\"\"Process a rollout graph into a sequence of steps.\n\n    Args:\n        graph (Graph[StateT, ActionT]): The graph to flatten.\n\n    Returns:\n        Sequence[Step]: A sequence of steps representing the flattened graph.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/algorithm.html#astra_rl.core.algorithm.Algorithm.step","title":"<code>step(batch)</code>  <code>abstractmethod</code>","text":"<p>Take a batch and compute loss of this batch.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Batch</code> <p>A batch of data to process.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Dict[Any, Any]]</code> <p>tuple[torch.Tensor, Dict[Any, Any]]: A tuple containing: - torch.Tensor: The loss computed by the algorithm (for current batch). - Dict[Any, Any]: Additional information for logging.</p> Source code in <code>src/astra_rl/core/algorithm.py</code> <pre><code>@abstractmethod\ndef step(self, batch: Batch) -&gt; tuple[torch.Tensor, Dict[Any, Any]]:\n    \"\"\"Take a batch and compute loss of this batch.\n\n    Args:\n        batch (Batch): A batch of data to process.\n\n    Returns:\n        tuple[torch.Tensor, Dict[Any, Any]]: A tuple containing:\n            - torch.Tensor: The loss computed by the algorithm (for current batch).\n            - Dict[Any, Any]: Additional information for logging.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/common.html","title":"Common","text":""},{"location":"api/core/common.html#astra_rl.core.common","title":"<code>astra_rl.core.common</code>","text":"<p>common.py Common data structures.</p>"},{"location":"api/core/environment.html","title":"Environment","text":""},{"location":"api/core/environment.html#astra_rl.core.environment","title":"<code>astra_rl.core.environment</code>","text":"<p>environment.py Roll out a problem, and specify how its environment behaves.</p>"},{"location":"api/core/environment.html#astra_rl.core.environment.Environment","title":"<code>Environment</code>","text":"<p>               Bases: <code>ABC</code>, <code>Generic[StateT, ActionT]</code></p> <p>An Environment used for rolling out a problem.</p> <p>The primary point of this class is to make a <code>Graph</code> of the problem by calling the <code>rollout</code> method. The environment can keep/sample initial state, but should not have global state that persists across rollouts.</p> <p>Attributes:</p> Name Type Description <code>problem</code> <code>Problem[StateT, ActionT]</code> <p>The problem instance that defines the environment and actions.</p> Generics <p>StateT (type): The type of the state in the environment. ActionT (type): The type of the action in the environment.</p> Source code in <code>src/astra_rl/core/environment.py</code> <pre><code>class Environment(ABC, Generic[StateT, ActionT]):\n    \"\"\"An Environment used for rolling out a problem.\n\n    The primary point of this class is to make a `Graph` of the problem\n    by calling the `rollout` method. The environment can keep/sample\n    initial state, but should not have global state that persists\n    across rollouts.\n\n    Attributes:\n        problem (Problem[StateT, ActionT]): The problem instance that defines the environment and actions.\n\n    Generics:\n        StateT (type): The type of the state in the environment.\n        ActionT (type): The type of the action in the environment.\n    \"\"\"\n\n    def __init__(self, problem: Problem[StateT, ActionT]):\n        self.problem = problem\n\n    @abstractmethod\n    def rollout(self, seed: Optional[int] = None) -&gt; Graph[StateT, ActionT]:\n        \"\"\"Roll out a problem and return a graph of the actions taken.\n\n        Args:\n            seed (Optional[int]): An optional seed; the same seed should produce the same graph.\n\n        Returns:\n            Graph[StateT, ActionT]: A graph representing the rollout of the problem.\n        \"\"\"\n\n        pass\n</code></pre>"},{"location":"api/core/environment.html#astra_rl.core.environment.Environment.rollout","title":"<code>rollout(seed=None)</code>  <code>abstractmethod</code>","text":"<p>Roll out a problem and return a graph of the actions taken.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>Optional[int]</code> <p>An optional seed; the same seed should produce the same graph.</p> <code>None</code> <p>Returns:</p> Type Description <code>Graph[StateT, ActionT]</code> <p>Graph[StateT, ActionT]: A graph representing the rollout of the problem.</p> Source code in <code>src/astra_rl/core/environment.py</code> <pre><code>@abstractmethod\ndef rollout(self, seed: Optional[int] = None) -&gt; Graph[StateT, ActionT]:\n    \"\"\"Roll out a problem and return a graph of the actions taken.\n\n    Args:\n        seed (Optional[int]): An optional seed; the same seed should produce the same graph.\n\n    Returns:\n        Graph[StateT, ActionT]: A graph representing the rollout of the problem.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/core/environment.html#astra_rl.core.environment.Graph","title":"<code>Graph</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Generic[StateT, ActionT]</code></p> <p>A graph representing the rollout (history + actions) of a problem.</p> <p>Attributes:</p> Name Type Description <code>context</code> <code>StateT</code> <p>The initial state of the environment.</p> <code>children</code> <code>Sequence[Node[StateT, ActionT]]</code> <p>The sequence of nodes representing actions and responses.</p> Source code in <code>src/astra_rl/core/environment.py</code> <pre><code>@dataclass\nclass Graph(Generic[StateT, ActionT]):\n    \"\"\"A graph representing the rollout (history + actions) of a problem.\n\n    Attributes:\n        context (StateT): The initial state of the environment.\n        children (Sequence[Node[StateT, ActionT]]): The sequence of nodes representing actions and responses.\n    \"\"\"\n\n    context: StateT\n    children: Sequence[Node[StateT, ActionT]]\n</code></pre>"},{"location":"api/core/environment.html#astra_rl.core.environment.Node","title":"<code>Node</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Generic[StateT, ActionT]</code></p> <p>A node in the rollout graph.</p> <p>Represents a single leaf in the rollout process, containing the context, the action taken, the response received, the reward for that action, and any children nodes that follow this action in this rollout.</p> <p>Attributes:</p> Name Type Description <code>context</code> <code>StateT</code> <p>The initial state before the action.</p> <code>attack</code> <code>ActionT</code> <p>The action taken in this node.</p> <code>response</code> <code>StateT</code> <p>The resulting state after the action.</p> <code>reward</code> <code>float</code> <p>The reward received for taking the action.</p> <code>children</code> <code>Sequence[Node[StateT, ActionT]]</code> <p>Subsequent nodes that follow this action.</p> Generics <p>StateT (type): The type of the state in the environment. ActionT (type): The type of the action in the environment.</p> Source code in <code>src/astra_rl/core/environment.py</code> <pre><code>@dataclass\nclass Node(Generic[StateT, ActionT]):\n    \"\"\"A node in the rollout graph.\n\n    Represents a single leaf in the rollout process, containing the context,\n    the action taken, the response received, the reward for that action,\n    and any children nodes that follow this action in this rollout.\n\n    Attributes:\n        context (StateT): The initial state before the action.\n        attack (ActionT): The action taken in this node.\n        response (StateT): The resulting state after the action.\n        reward (float): The reward received for taking the action.\n        children (Sequence[Node[StateT, ActionT]]): Subsequent nodes that follow this action.\n\n    Generics:\n        StateT (type): The type of the state in the environment.\n        ActionT (type): The type of the action in the environment.\n    \"\"\"\n\n    context: StateT\n    attack: ActionT\n    response: StateT\n    reward: float\n\n    children: Sequence[Self]\n</code></pre>"},{"location":"api/core/moderator.html","title":"Moderator","text":""},{"location":"api/core/moderator.html#astra_rl.core.moderator","title":"<code>astra_rl.core.moderator</code>","text":"<p>moderator.py</p>"},{"location":"api/core/moderator.html#astra_rl.core.moderator.Moderator","title":"<code>Moderator</code>","text":"<p>               Bases: <code>ABC</code>, <code>Generic[StateT, ActionT]</code></p> <p>Red-Teaming moderator for evaluating sequences.</p> Source code in <code>src/astra_rl/core/moderator.py</code> <pre><code>class Moderator(ABC, Generic[StateT, ActionT]):\n    \"\"\"Red-Teaming moderator for evaluating sequences.\"\"\"\n\n    @abstractmethod\n    def moderate(self, x: Sequence[Union[StateT, ActionT]]) -&gt; Sequence[float]:\n        pass\n</code></pre>"},{"location":"api/core/problem.html","title":"Problem","text":""},{"location":"api/core/problem.html#astra_rl.core.problem","title":"<code>astra_rl.core.problem</code>","text":"<p>A \"Problem\" is one of the core abstractions in Astra RL, defining how to interact with the system under test. The interface is defined by the <code>Problem</code> class, which defines a set of abstract methods that users must implement to create a custom problem. This provides flexibility in terms of how users can define their own applications while still adhering to a common interface that enables the Astra RL framework to function correctly.</p>"},{"location":"api/core/problem.html#astra_rl.core.problem.Problem","title":"<code>Problem</code>","text":"<p>               Bases: <code>ABC</code>, <code>Generic[StateT, ActionT]</code></p> <p>Defines the core problem interface for Astra RL.</p> <p>This class is responsible for defining how exactly to interact with the system under test---with generics in terms of how to get probabilities and rollouts from the attacker and target models.</p> <p>This allows for us to be generic over the types of states, actions as well as how to measure them. We ask for a moderator as a way to ensure that subclasses can all be generic over the exact metric, and instead can only be opinonated about how to achieve the metric.</p> <p>Attributes:</p> Name Type Description <code>moderator</code> <code>Moderator[StateT, ActionT]</code> <p>The moderator used to evaluate sequences.</p> Generics <p>StateT (type): The type of the state in the environment. ActionT (type): The type of the action in the environment.</p> Source code in <code>src/astra_rl/core/problem.py</code> <pre><code>class Problem(ABC, Generic[StateT, ActionT]):\n    \"\"\"Defines the core problem interface for Astra RL.\n\n    This class is responsible for defining how exactly to interact\n    with the system under test---with generics in terms of how to get\n    probabilities and rollouts from the attacker and target models.\n\n    This allows for us to be generic over the types of states, actions\n    as well as how to measure them. We ask for a moderator as a way to\n    ensure that subclasses can all be generic over the exact metric, and\n    instead can only be opinonated about how to achieve the metric.\n\n    Attributes:\n        moderator (Moderator[StateT, ActionT]): The moderator used to evaluate sequences.\n\n    Generics:\n        StateT (type): The type of the state in the environment.\n        ActionT (type): The type of the action in the environment.\n    \"\"\"\n\n    def __init__(self, moderator: Moderator[StateT, ActionT]) -&gt; None:\n        # we check all asserts once, and then disable them\n        self._disable_asserts: Dict[str, bool] = defaultdict(bool)\n        self.moderator = moderator\n\n    @abstractmethod\n    def get_target_logprobs(\n        self, context: Sequence[StateT], continuation: Sequence[ActionT]\n    ) -&gt; torch.Tensor:\n        \"\"\"Evaluates P(continuation|context) on *model under test*.\n\n        Args:\n            context (Sequence[str]): Sequence of strings, where each string is a context on which the\n                                 continuation's probability is conditioned.\n            continuation (Sequence[str]): Sequence of strings, where each string is a continuation whose\n                                      probability is measured.\n\n        Note:\n            This should be batched; i.e., len(context) == len(continuation) and each\n            represents a batch element.\n\n        Returns:\n            torch.Tensor: The per-token log probabilities of the continuations given their contexts.\n                         Shape: (batch_size, max_continuation_length)\n        \"\"\"\n\n        pass\n\n    @abstractmethod\n    def get_baseline_logprobs(\n        self, context: Sequence[StateT], continuation: Sequence[ActionT]\n    ) -&gt; torch.Tensor:\n        \"\"\"Evaluates P(continuation|context) on *attacker's baseline distribution* for KL\n           divergence measurements.\n\n        Args:\n            context (Sequence[str]): Sequence of strings, where each string is a context on which the\n                                 continuation's probability is conditioned.\n            continuation (Sequence[str]): Sequence of strings, where each string is a continuation whose\n                                      probability is measured.\n\n        Note:\n            This should be batched; i.e., len(context) == len(continuation) and each\n            represents a batch element. Note that this is *not* the defender's model, but\n            rather the baseline model used for measuring KL divergence to make sure that\n            the trained attacker stays an LM.\n\n        Returns:\n            torch.Tensor: The per-token log probabilities of the continuations given their contexts.\n                         Shape: (batch_size, max_continuation_length)\n        \"\"\"\n\n        pass\n\n    @abstractmethod\n    def get_attacker_logprobs(\n        self, context: Sequence[StateT], continuation: Sequence[ActionT]\n    ) -&gt; torch.Tensor:\n        \"\"\"Evaluates P(continuation|context) on *attacker*. This must return tensor w/ grads!\n\n        Args:\n            context (Sequence[str]): Sequence of strings, where each string is a context on which the\n                                 continuation's probability is conditioned.\n            continuation (Sequence[str]): Sequence of strings, where each string is a continuation whose\n                                      probability is measured.\n\n        Note:\n            This should be batched; i.e., len(context) == len(continuation) and each\n            represents a batch element.\n\n        Returns:\n            torch.Tensor: The per-token log probabilities of the continuations given their contexts.\n                         Shape: (batch_size, max_continuation_length)\n        \"\"\"\n\n        pass\n\n    @abstractmethod\n    def rollout_prompt_with_attacker(self, x: Sequence[StateT]) -&gt; Sequence[ActionT]:\n        \"\"\"Rolls out the prompt with the attacker model. Do *not* return the prompt.\n\n        a ~ \\\\pi(s)\n\n        Args:\n            x (Sequence[str]): Sequence of strings representing the prompt to be rolled out.\n\n        Returns:\n            Sequence[str]: The rolled out prompt with the adversary model.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def rollout_prompt_with_target(self, x: Sequence[StateT]) -&gt; Sequence[StateT]:\n        \"\"\"Rolls out the prompt with the model under test. Do *not* return the prompt.\n\n        s' ~ \\\\sum_a T(s, a)\n\n        Args:\n            x (Sequence[str]): Sequence of strings representing the prompt to be rolled out.\n\n        Returns:\n            Sequence[str]: The rolled out prompt with the adversary model.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def advance(self, context: StateT, attack: ActionT, response: StateT) -&gt; StateT:\n        \"\"\"Given a context and continuation, returns the next state.\n\n        Args:\n            context (str): Sequence of strings representing the context.\n            attack (str): Sequence of strings representing the attack given context.\n            response (str): Sequence of strings representing the defense against attack.\n\n        Returns:\n                str: The next state after applying the continuation to the context.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def parameters(self) -&gt; Iterator[torch.nn.parameter.Parameter]:\n        \"\"\"Return the trainable parameters in this problem.\n\n        Returns:\n            Iterator[torch.nn.parameter.Parameter]: An iterator over the trainable parameters.\n            usually just by calling model.parameters()\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def reward(\n        self,\n        context: Sequence[StateT],\n        attack: Sequence[ActionT],\n        response: Sequence[StateT],\n    ) -&gt; Sequence[float]:\n        pass\n\n    ##### Utility methods for validation and checks #####\n\n    def _check_continuation(\n        self,\n        check_key: str,\n        context: Sequence[StateT],\n        continuation: Sequence[Union[ActionT, StateT]],\n    ) -&gt; None:\n        if self._disable_asserts[check_key]:\n            return\n        self._disable_asserts[check_key] = True\n\n    def _check_logprobs(\n        self,\n        check_key: str,\n        logprobs: torch.Tensor,\n        ctx_length: int,\n        requires_grad: bool = False,\n    ) -&gt; None:\n        if self._disable_asserts[check_key]:\n            return\n        # check that logprobs is a tensor and has gradients\n        assert isinstance(logprobs, torch.Tensor), \"Logprobs must be a torch.Tensor.\"\n        if requires_grad:\n            assert logprobs.requires_grad, (\n                \"Attacker logprobs must carry gradient information.\"\n            )\n        # check that the size of the tensor is B x T, where B is the batch size and T is max_continuation_length\n        assert logprobs.dim() == 2, (\n            \"Logprobs must be a 2D tensor (batch_size, max_continuation_length).\"\n        )\n        # check that the first dimension is the batch size\n        assert logprobs.size(0) == ctx_length, (\n            \"Logprobs must have the same batch size as the context.\"\n        )\n        # warn if everything is between 0 and 1\n        if ((logprobs &gt;= 0.0) &amp; (logprobs &lt;= 1.0)).all():\n            logger.warning(\n                \"Logprobs looks suspiciously like probabilities, \"\n                \"try taking the .log() of your tensor?\"\n            )\n        self._disable_asserts[check_key] = True\n\n    def _get_attacker_logprobs_and_validate(\n        self, context: Sequence[StateT], continuation: Sequence[ActionT]\n    ) -&gt; torch.Tensor:\n        logprobs = self.get_attacker_logprobs(context, continuation)\n        self._check_logprobs(\"attacker_logprobs\", logprobs, len(context), True)\n        return logprobs\n\n    def _get_target_logprobs_and_validate(\n        self, context: Sequence[StateT], continuation: Sequence[ActionT]\n    ) -&gt; torch.Tensor:\n        logprobs = self.get_target_logprobs(context, continuation)\n        self._check_logprobs(\"target_logprobs\", logprobs, len(context), False)\n        return logprobs\n\n    def _get_baseline_logprobs_and_validate(\n        self, context: Sequence[StateT], continuation: Sequence[ActionT]\n    ) -&gt; torch.Tensor:\n        logprobs = self.get_baseline_logprobs(context, continuation)\n        self._check_logprobs(\"baseline_logprobs\", logprobs, len(context), False)\n        return logprobs\n\n    def _rollout_prompt_with_attacker_and_validate(\n        self, x: Sequence[StateT]\n    ) -&gt; Sequence[ActionT]:\n        rolled_out = self.rollout_prompt_with_attacker(x)\n        self._check_continuation(\"attacker_rollout\", x, rolled_out)\n        return rolled_out\n\n    def _rollout_prompt_with_target_and_validate(\n        self, x: Sequence[StateT]\n    ) -&gt; Sequence[StateT]:\n        rolled_out = self.rollout_prompt_with_target(x)\n        self._check_continuation(\"target_rollout\", x, rolled_out)\n        return rolled_out\n</code></pre>"},{"location":"api/core/problem.html#astra_rl.core.problem.Problem.advance","title":"<code>advance(context, attack, response)</code>  <code>abstractmethod</code>","text":"<p>Given a context and continuation, returns the next state.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>str</code> <p>Sequence of strings representing the context.</p> required <code>attack</code> <code>str</code> <p>Sequence of strings representing the attack given context.</p> required <code>response</code> <code>str</code> <p>Sequence of strings representing the defense against attack.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>StateT</code> <p>The next state after applying the continuation to the context.</p> Source code in <code>src/astra_rl/core/problem.py</code> <pre><code>@abstractmethod\ndef advance(self, context: StateT, attack: ActionT, response: StateT) -&gt; StateT:\n    \"\"\"Given a context and continuation, returns the next state.\n\n    Args:\n        context (str): Sequence of strings representing the context.\n        attack (str): Sequence of strings representing the attack given context.\n        response (str): Sequence of strings representing the defense against attack.\n\n    Returns:\n            str: The next state after applying the continuation to the context.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/problem.html#astra_rl.core.problem.Problem.get_attacker_logprobs","title":"<code>get_attacker_logprobs(context, continuation)</code>  <code>abstractmethod</code>","text":"<p>Evaluates P(continuation|context) on attacker. This must return tensor w/ grads!</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>Sequence[str]</code> <p>Sequence of strings, where each string is a context on which the                  continuation's probability is conditioned.</p> required <code>continuation</code> <code>Sequence[str]</code> <p>Sequence of strings, where each string is a continuation whose                       probability is measured.</p> required Note <p>This should be batched; i.e., len(context) == len(continuation) and each represents a batch element.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The per-token log probabilities of the continuations given their contexts.          Shape: (batch_size, max_continuation_length)</p> Source code in <code>src/astra_rl/core/problem.py</code> <pre><code>@abstractmethod\ndef get_attacker_logprobs(\n    self, context: Sequence[StateT], continuation: Sequence[ActionT]\n) -&gt; torch.Tensor:\n    \"\"\"Evaluates P(continuation|context) on *attacker*. This must return tensor w/ grads!\n\n    Args:\n        context (Sequence[str]): Sequence of strings, where each string is a context on which the\n                             continuation's probability is conditioned.\n        continuation (Sequence[str]): Sequence of strings, where each string is a continuation whose\n                                  probability is measured.\n\n    Note:\n        This should be batched; i.e., len(context) == len(continuation) and each\n        represents a batch element.\n\n    Returns:\n        torch.Tensor: The per-token log probabilities of the continuations given their contexts.\n                     Shape: (batch_size, max_continuation_length)\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/core/problem.html#astra_rl.core.problem.Problem.get_baseline_logprobs","title":"<code>get_baseline_logprobs(context, continuation)</code>  <code>abstractmethod</code>","text":"<p>Evaluates P(continuation|context) on attacker's baseline distribution for KL    divergence measurements.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>Sequence[str]</code> <p>Sequence of strings, where each string is a context on which the                  continuation's probability is conditioned.</p> required <code>continuation</code> <code>Sequence[str]</code> <p>Sequence of strings, where each string is a continuation whose                       probability is measured.</p> required Note <p>This should be batched; i.e., len(context) == len(continuation) and each represents a batch element. Note that this is not the defender's model, but rather the baseline model used for measuring KL divergence to make sure that the trained attacker stays an LM.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The per-token log probabilities of the continuations given their contexts.          Shape: (batch_size, max_continuation_length)</p> Source code in <code>src/astra_rl/core/problem.py</code> <pre><code>@abstractmethod\ndef get_baseline_logprobs(\n    self, context: Sequence[StateT], continuation: Sequence[ActionT]\n) -&gt; torch.Tensor:\n    \"\"\"Evaluates P(continuation|context) on *attacker's baseline distribution* for KL\n       divergence measurements.\n\n    Args:\n        context (Sequence[str]): Sequence of strings, where each string is a context on which the\n                             continuation's probability is conditioned.\n        continuation (Sequence[str]): Sequence of strings, where each string is a continuation whose\n                                  probability is measured.\n\n    Note:\n        This should be batched; i.e., len(context) == len(continuation) and each\n        represents a batch element. Note that this is *not* the defender's model, but\n        rather the baseline model used for measuring KL divergence to make sure that\n        the trained attacker stays an LM.\n\n    Returns:\n        torch.Tensor: The per-token log probabilities of the continuations given their contexts.\n                     Shape: (batch_size, max_continuation_length)\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/core/problem.html#astra_rl.core.problem.Problem.get_target_logprobs","title":"<code>get_target_logprobs(context, continuation)</code>  <code>abstractmethod</code>","text":"<p>Evaluates P(continuation|context) on model under test.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>Sequence[str]</code> <p>Sequence of strings, where each string is a context on which the                  continuation's probability is conditioned.</p> required <code>continuation</code> <code>Sequence[str]</code> <p>Sequence of strings, where each string is a continuation whose                       probability is measured.</p> required Note <p>This should be batched; i.e., len(context) == len(continuation) and each represents a batch element.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The per-token log probabilities of the continuations given their contexts.          Shape: (batch_size, max_continuation_length)</p> Source code in <code>src/astra_rl/core/problem.py</code> <pre><code>@abstractmethod\ndef get_target_logprobs(\n    self, context: Sequence[StateT], continuation: Sequence[ActionT]\n) -&gt; torch.Tensor:\n    \"\"\"Evaluates P(continuation|context) on *model under test*.\n\n    Args:\n        context (Sequence[str]): Sequence of strings, where each string is a context on which the\n                             continuation's probability is conditioned.\n        continuation (Sequence[str]): Sequence of strings, where each string is a continuation whose\n                                  probability is measured.\n\n    Note:\n        This should be batched; i.e., len(context) == len(continuation) and each\n        represents a batch element.\n\n    Returns:\n        torch.Tensor: The per-token log probabilities of the continuations given their contexts.\n                     Shape: (batch_size, max_continuation_length)\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/core/problem.html#astra_rl.core.problem.Problem.parameters","title":"<code>parameters()</code>  <code>abstractmethod</code>","text":"<p>Return the trainable parameters in this problem.</p> <p>Returns:</p> Type Description <code>Iterator[Parameter]</code> <p>Iterator[torch.nn.parameter.Parameter]: An iterator over the trainable parameters.</p> <code>Iterator[Parameter]</code> <p>usually just by calling model.parameters()</p> Source code in <code>src/astra_rl/core/problem.py</code> <pre><code>@abstractmethod\ndef parameters(self) -&gt; Iterator[torch.nn.parameter.Parameter]:\n    \"\"\"Return the trainable parameters in this problem.\n\n    Returns:\n        Iterator[torch.nn.parameter.Parameter]: An iterator over the trainable parameters.\n        usually just by calling model.parameters()\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/problem.html#astra_rl.core.problem.Problem.rollout_prompt_with_attacker","title":"<code>rollout_prompt_with_attacker(x)</code>  <code>abstractmethod</code>","text":"<p>Rolls out the prompt with the attacker model. Do not return the prompt.</p> <p>a ~ \\pi(s)</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Sequence[str]</code> <p>Sequence of strings representing the prompt to be rolled out.</p> required <p>Returns:</p> Type Description <code>Sequence[ActionT]</code> <p>Sequence[str]: The rolled out prompt with the adversary model.</p> Source code in <code>src/astra_rl/core/problem.py</code> <pre><code>@abstractmethod\ndef rollout_prompt_with_attacker(self, x: Sequence[StateT]) -&gt; Sequence[ActionT]:\n    \"\"\"Rolls out the prompt with the attacker model. Do *not* return the prompt.\n\n    a ~ \\\\pi(s)\n\n    Args:\n        x (Sequence[str]): Sequence of strings representing the prompt to be rolled out.\n\n    Returns:\n        Sequence[str]: The rolled out prompt with the adversary model.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/problem.html#astra_rl.core.problem.Problem.rollout_prompt_with_target","title":"<code>rollout_prompt_with_target(x)</code>  <code>abstractmethod</code>","text":"<p>Rolls out the prompt with the model under test. Do not return the prompt.</p> <p>s' ~ \\sum_a T(s, a)</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Sequence[str]</code> <p>Sequence of strings representing the prompt to be rolled out.</p> required <p>Returns:</p> Type Description <code>Sequence[StateT]</code> <p>Sequence[str]: The rolled out prompt with the adversary model.</p> Source code in <code>src/astra_rl/core/problem.py</code> <pre><code>@abstractmethod\ndef rollout_prompt_with_target(self, x: Sequence[StateT]) -&gt; Sequence[StateT]:\n    \"\"\"Rolls out the prompt with the model under test. Do *not* return the prompt.\n\n    s' ~ \\\\sum_a T(s, a)\n\n    Args:\n        x (Sequence[str]): Sequence of strings representing the prompt to be rolled out.\n\n    Returns:\n        Sequence[str]: The rolled out prompt with the adversary model.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/problem.html#astra_rl.core.problem.ValueFunctionProblem","title":"<code>ValueFunctionProblem</code>","text":"<p>               Bases: <code>Problem[StateT, ActionT]</code>, <code>ABC</code></p> <p>Extends <code>Problem</code> to be able to return sequence values with a value head.</p> Note <p>This is useful for value-laiden solution methods such as Actor Critic derivatives (i.e., PPO).</p> <p>Attributes:</p> Name Type Description <code>moderator</code> <code>Moderator[StateT, ActionT]</code> <p>The moderator used to evaluate sequences.</p> Generics <p>StateT (type): The type of the state in the environment. ActionT (type): The type of the action in the environment.</p> Source code in <code>src/astra_rl/core/problem.py</code> <pre><code>class ValueFunctionProblem(Problem[StateT, ActionT], ABC):\n    \"\"\"Extends `Problem` to be able to return sequence values with a value head.\n\n    Note:\n        This is useful for value-laiden solution methods such as Actor\n        Critic derivatives (i.e., PPO).\n\n    Attributes:\n        moderator (Moderator[StateT, ActionT]): The moderator used to evaluate sequences.\n\n    Generics:\n        StateT (type): The type of the state in the environment.\n        ActionT (type): The type of the action in the environment.\n    \"\"\"\n\n    @abstractmethod\n    def value(\n        self, context: Sequence[StateT], continuation: Sequence[ActionT]\n    ) -&gt; torch.Tensor:\n        \"\"\"Given a squence, evaluate its token-wise value using a value function.\n\n        Notes:\n           This is typically done by the same neural network you use for rollouts\n           just passing the intermediate activations through another layer.\n\n        Args:\n            elem (Sequence[StateT]): The sequence to evaluate.\n\n        Returns:\n            torch.Tensor[batch_size, max_continuation_length]: The per-token values of\n            the given squence by the sequence predictor. Do not include the value of the input\n            prefixes. If you are predicting on the whole input, you should be slicing on\n            `[:, :-1]`, meaning you should *not* return the value of the last token, whose\n            input is eos/context length limit.\n        \"\"\"\n\n        pass\n</code></pre>"},{"location":"api/core/problem.html#astra_rl.core.problem.ValueFunctionProblem.value","title":"<code>value(context, continuation)</code>  <code>abstractmethod</code>","text":"<p>Given a squence, evaluate its token-wise value using a value function.</p> Notes <p>This is typically done by the same neural network you use for rollouts just passing the intermediate activations through another layer.</p> <p>Parameters:</p> Name Type Description Default <code>elem</code> <code>Sequence[StateT]</code> <p>The sequence to evaluate.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor[batch_size, max_continuation_length]: The per-token values of</p> <code>Tensor</code> <p>the given squence by the sequence predictor. Do not include the value of the input</p> <code>Tensor</code> <p>prefixes. If you are predicting on the whole input, you should be slicing on</p> <code>Tensor</code> <p><code>[:, :-1]</code>, meaning you should not return the value of the last token, whose</p> <code>Tensor</code> <p>input is eos/context length limit.</p> Source code in <code>src/astra_rl/core/problem.py</code> <pre><code>@abstractmethod\ndef value(\n    self, context: Sequence[StateT], continuation: Sequence[ActionT]\n) -&gt; torch.Tensor:\n    \"\"\"Given a squence, evaluate its token-wise value using a value function.\n\n    Notes:\n       This is typically done by the same neural network you use for rollouts\n       just passing the intermediate activations through another layer.\n\n    Args:\n        elem (Sequence[StateT]): The sequence to evaluate.\n\n    Returns:\n        torch.Tensor[batch_size, max_continuation_length]: The per-token values of\n        the given squence by the sequence predictor. Do not include the value of the input\n        prefixes. If you are predicting on the whole input, you should be slicing on\n        `[:, :-1]`, meaning you should *not* return the value of the last token, whose\n        input is eos/context length limit.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/methods/ast_problem.html","title":"AST Problem","text":""},{"location":"api/methods/ast_problem.html#astra_rl.methods.ast_problem","title":"<code>astra_rl.methods.ast_problem</code>","text":"<p>ast_problem.py ASTProblem</p>"},{"location":"api/methods/ast_problem.html#astra_rl.methods.ast_problem.ASTEnvironment","title":"<code>ASTEnvironment</code>","text":"<p>               Bases: <code>Environment[str, str]</code></p> <p>The ASTPrompter Rollout Environment</p> <p>Implements https://arxiv.org/abs/2407.09447.</p> <p>Specifically, this is the original rollout system used in the ASTPrompter paper, the case of red-teaming where we have the attacker and defender generates successive turns of strings, each of which is appended to the prompt of the other. They do not have IFT or other types of structure.</p> <p>For usage examples, see <code>astra_rl.core.environment.Environment</code>.</p> <p>Attributes:</p> Name Type Description <code>problem</code> <code>ASTProblem</code> <p>The problem instance that defines the environment and actions.</p> <code>prompts</code> <code>Sequence[str]</code> <p>A sequence of initial prompts to start the rollout.</p> <code>tree_width</code> <code>int</code> <p>The number of branches at each node in the rollout tree.</p> <code>tree_depth</code> <code>int</code> <p>The depth of the rollout tree.</p> Generics <p>StateT (str): The type of the state in the environment, which is a string. ActionT (str): The type of the action in the environment, which is also a string.</p> Source code in <code>src/astra_rl/methods/ast_problem.py</code> <pre><code>class ASTEnvironment(Environment[str, str]):\n    \"\"\"The ASTPrompter Rollout Environment\n\n    Implements https://arxiv.org/abs/2407.09447.\n\n    Specifically, this is the original rollout system used in the\n    ASTPrompter paper, the case of red-teaming where we have\n    the attacker and defender generates successive turns of strings,\n    each of which is appended to the prompt of the other. They do not\n    have IFT or other types of structure.\n\n    For usage examples, see `astra_rl.core.environment.Environment`.\n\n    Attributes:\n        problem (ASTProblem): The problem instance that defines the environment and actions.\n        prompts (Sequence[str]): A sequence of initial prompts to start the rollout.\n        tree_width (int): The number of branches at each node in the rollout tree.\n        tree_depth (int): The depth of the rollout tree.\n\n    Generics:\n        StateT (str): The type of the state in the environment, which is a string.\n        ActionT (str): The type of the action in the environment, which is also a string.\n    \"\"\"\n\n    def __init__(\n        self,\n        problem: ASTProblem,\n        prompts: Sequence[str],\n        tree_width: int = 2,\n        tree_depth: int = 3,\n    ):\n        super().__init__(problem)\n\n        self.prompts = prompts\n        self.tree_width = tree_width\n        self.tree_depth = tree_depth\n\n    def __handle_prompt(self, prompt: str, depth: int = 3) -&gt; Sequence[Node[str, str]]:\n        if depth == 0:\n            return []\n\n        prompts = [prompt for _ in range(self.tree_width)]\n        attacks = self.problem._rollout_prompt_with_attacker_and_validate(prompts)\n        defenses = self.problem._rollout_prompt_with_target_and_validate(\n            [prompt + i for i in attacks]\n        )\n        rewards = self.problem.reward(prompts, attacks, defenses)\n\n        nodes = [\n            Node(\n                prompt,\n                attack,\n                defense,\n                reward,\n                self.__handle_prompt(\n                    self.problem.advance(prompt, attack, defense), depth - 1\n                ),\n            )\n            for prompt, attack, defense, reward in zip(\n                prompts, attacks, defenses, rewards\n            )\n        ]\n\n        return nodes\n\n    def rollout(self, seed: Optional[int] = None) -&gt; Graph[str, str]:\n        R: Union[Random, ModuleType] = random\n        if seed:\n            R = Random(seed)\n        prompt = R.choice(self.prompts)\n        nodes = self.__handle_prompt(prompt, self.tree_depth)\n\n        return Graph(prompt, nodes)\n</code></pre>"},{"location":"api/methods/ast_problem.html#astra_rl.methods.ast_problem.ASTProblem","title":"<code>ASTProblem</code>","text":"<p>               Bases: <code>Problem[str, str]</code></p> <p>Adaptive Stress Testing for Large Language Model Red-Teaming.</p> <p>Implements https://arxiv.org/abs/2407.09447.</p> <p>For usage examples, see <code>astra_rl.core.problem.Problem</code>.</p> <p>Attributes:</p> Name Type Description <code>moderator</code> <code>Moderator</code> <p>The moderator used to evaluate the sequences.</p> <code>ppl_weight</code> <code>float</code> <p>Weight for perplexity in the reward calculation.</p> <code>combined_weight</code> <code>float</code> <p>Weight for combined scores in the reward calculation.</p> <code>defense_toxicity_weight</code> <code>float</code> <p>Weight for defense toxicity in the reward calculation.</p> Source code in <code>src/astra_rl/methods/ast_problem.py</code> <pre><code>class ASTProblem(Problem[str, str]):\n    \"\"\"Adaptive Stress Testing for Large Language Model Red-Teaming.\n\n    Implements https://arxiv.org/abs/2407.09447.\n\n    For usage examples, see `astra_rl.core.problem.Problem`.\n\n    Attributes:\n        moderator (Moderator): The moderator used to evaluate the sequences.\n        ppl_weight (float): Weight for perplexity in the reward calculation.\n        combined_weight (float): Weight for combined scores in the reward calculation.\n        defense_toxicity_weight (float): Weight for defense toxicity in the reward calculation.\n    \"\"\"\n\n    def __init__(\n        self,\n        moderator: Moderator[str, str],\n        ppl_weight: float = 0.025,\n        combined_weight: float = 0.1,\n        defense_toxicity_weight: float = 0.1,\n    ):\n        super().__init__(moderator)\n\n        self.ppl_weight = ppl_weight\n        self.combined_weight = combined_weight\n        self.defense_toxicity_weight = defense_toxicity_weight\n\n    def advance(self, state: str, action: str, next: str) -&gt; str:\n        return state + action + next\n\n    def reward(\n        self, context: Sequence[str], attack: Sequence[str], response: Sequence[str]\n    ) -&gt; Sequence[float]:\n        combined_uts = [j + k for j, k in zip(attack, response)]\n        combined_scores: Sequence[float] = self.moderator.moderate(combined_uts)\n        defender_scores: Sequence[float] = self.moderator.moderate(response)\n        attack_ppls: torch.Tensor = (\n            self._get_target_logprobs_and_validate(context, attack).sum(dim=-1).exp()\n        )  # Sum per-token logprobs before taking exp\n\n        reward: Sequence[float] = (\n            (\n                self.ppl_weight * attack_ppls\n                + self.combined_weight\n                * torch.tensor(combined_scores).to(attack_ppls.device)\n                + self.defense_toxicity_weight\n                * torch.tensor(defender_scores).to(attack_ppls.device)\n            )\n            .cpu()\n            .tolist()\n        )\n\n        return reward\n</code></pre>"},{"location":"api/moderators/detoxify.html","title":"Detoxify","text":""},{"location":"api/moderators/detoxify.html#astra_rl.moderators.detoxify","title":"<code>astra_rl.moderators.detoxify</code>","text":"<p>detoxify.py Moderator to call into the Detoxify engine.</p>"},{"location":"api/moderators/detoxify.html#astra_rl.moderators.detoxify.DetoxifyModerator","title":"<code>DetoxifyModerator</code>","text":"<p>               Bases: <code>Moderator[str, str]</code></p> <p>Moderator that wraps the Detoxify library for toxicity detection.</p> <p>https://github.com/unitaryai/detoxify</p> <p>Attributes:</p> Name Type Description <code>harm_category</code> <code>str</code> <p>The category of harm to detect (default is \"toxicity\"); see below.</p> <code>variant</code> <code>str</code> <p>The variant of the Detoxify model to use (default is \"original\").</p> Notes <p>Possible harm categories include \"toxicity\", \"severe_toxicity\", \"obscene\", \"identity_attack\", \"insult\", \"threat\", \"sexual_explicit\".</p> <p>Possible variants Include \"original\", \"multilingual\", \"unbiased\".</p> Source code in <code>src/astra_rl/moderators/detoxify.py</code> <pre><code>class DetoxifyModerator(Moderator[str, str]):\n    \"\"\"Moderator that wraps the Detoxify library for toxicity detection.\n\n    https://github.com/unitaryai/detoxify\n\n    Attributes:\n        harm_category (str): The category of harm to detect (default is \"toxicity\"); see below.\n        variant (str): The variant of the Detoxify model to use (default is \"original\").\n\n    Notes:\n        Possible harm categories\n        include \"toxicity\", \"severe_toxicity\", \"obscene\", \"identity_attack\",\n        \"insult\", \"threat\", \"sexual_explicit\".\n\n        Possible variants\n        Include \"original\", \"multilingual\", \"unbiased\".\n    \"\"\"\n\n    def __init__(self, harm_category: str = \"toxicity\", variant: str = \"original\"):\n        self.model = Detoxify(variant)\n        self.harm_category = harm_category\n\n    def moderate(self, x: Sequence[str]) -&gt; Sequence[float]:\n        # we ignore typing here because we don't actually have the ability\n        # to get typing information from detoxify\n        return self.model.predict(x)[self.harm_category]  # type: ignore\n</code></pre>"},{"location":"api/training/harness.html","title":"Harness","text":""},{"location":"api/training/harness.html#astra_rl.training.harness","title":"<code>astra_rl.training.harness</code>","text":""},{"location":"api/training/harness.html#astra_rl.training.harness.Harness","title":"<code>Harness</code>","text":"<p>               Bases: <code>Generic[StateT, ActionT, Step, Batch]</code></p> <p>Harness for running an algorithm in a given environment.</p> <p>Example:</p> <pre><code>Here is an example of how to use the `Harness` class with the DPO algorithm\nand an AST problem environment for *one episode only*. You should add your\nown optimization things such as weight decay or scheduling and figure out\nearly stopping, etc.\n\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from astra_rl.training.harness import (\n...     Harness,\n... )\n&gt;&gt;&gt; from astra_rl.algorithms.dpo import (\n...     DPO,\n... )\n&gt;&gt;&gt; from astra_rl.methods.ast import (\n...     ASTProblem,\n...     ASTEnvironment,\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; problem = (\n...     ASTProblem()\n... )\n&gt;&gt;&gt; environment = (\n...     ASTEnvironment(\n...         problem, ...\n...     )\n... )\n&gt;&gt;&gt; algorithm = DPO(...)\n&gt;&gt;&gt; harness = Harness(\n...     environment,\n...     algorithm,\n... )\n&gt;&gt;&gt; optimizer = torch.optim.Adam(\n...     problem.parameters(),\n...     lr=1e-4,\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; for batch in harness.experience():\n...     loss = harness.step(\n...         batch\n...     )\n...     loss.backward()\n...     optimizer.zero_grad()\n</code></pre> <p>Attributes:</p> Name Type Description <code>environment</code> <code>Environment[StateT, ActionT]</code> <p>The environment to run the algorithm in.</p> <code>algorithm</code> <code>Algorithm[StateT, ActionT, Step, Batch]</code> <p>The algorithm to run.</p> <code>num_episodes_per_experience</code> <code>int</code> <p>Number of episodes per call to <code>.experience()</code>.</p> <code>dataloader_kwargs</code> <code>Dict[str, Any]</code> <p>Keyword arguments for the PyTorch data loader. Batch size, for instance, should be set.</p> Generics <p>StateT (type): The type of the state in the environment. ActionT (type): The type of the action in the environment. Step (type): The type of a single step in the environment. Batch (type): The type of a batch of steps, passed to the <code>.step()</code> function for gradient.</p> Source code in <code>src/astra_rl/training/harness.py</code> <pre><code>class Harness(Generic[StateT, ActionT, Step, Batch]):\n    \"\"\"Harness for running an algorithm in a given environment.\n\n    Example:\n\n        Here is an example of how to use the `Harness` class with the DPO algorithm\n        and an AST problem environment for *one episode only*. You should add your\n        own optimization things such as weight decay or scheduling and figure out\n        early stopping, etc.\n\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; from astra_rl.training.harness import (\n        ...     Harness,\n        ... )\n        &gt;&gt;&gt; from astra_rl.algorithms.dpo import (\n        ...     DPO,\n        ... )\n        &gt;&gt;&gt; from astra_rl.methods.ast import (\n        ...     ASTProblem,\n        ...     ASTEnvironment,\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; problem = (\n        ...     ASTProblem()\n        ... )\n        &gt;&gt;&gt; environment = (\n        ...     ASTEnvironment(\n        ...         problem, ...\n        ...     )\n        ... )\n        &gt;&gt;&gt; algorithm = DPO(...)\n        &gt;&gt;&gt; harness = Harness(\n        ...     environment,\n        ...     algorithm,\n        ... )\n        &gt;&gt;&gt; optimizer = torch.optim.Adam(\n        ...     problem.parameters(),\n        ...     lr=1e-4,\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; for batch in harness.experience():\n        ...     loss = harness.step(\n        ...         batch\n        ...     )\n        ...     loss.backward()\n        ...     optimizer.zero_grad()\n\n\n    Attributes:\n        environment (Environment[StateT, ActionT]): The environment to run the algorithm in.\n        algorithm (Algorithm[StateT, ActionT, Step, Batch]): The algorithm to run.\n        num_episodes_per_experience (int): Number of episodes per call to `.experience()`.\n        dataloader_kwargs (Dict[str, Any]): Keyword arguments for the PyTorch data loader. Batch size, for instance, should be set.\n\n    Generics:\n        StateT (type): The type of the state in the environment.\n        ActionT (type): The type of the action in the environment.\n        Step (type): The type of a single step in the environment.\n        Batch (type): The type of a batch of steps, passed to the `.step()` function for gradient.\n    \"\"\"\n\n    def __init__(\n        self,\n        environment: Environment[StateT, ActionT],\n        algorithm: Algorithm[StateT, ActionT, Step, Batch],\n        num_episodes_per_experience: int = 32,\n        use_wandb: bool = False,\n        wandb_kwargs: Optional[Dict[str, Any]] = None,\n        dataloader_kwargs: Optional[Dict[str, Any]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Args:\n            environment (Environment): The environment to run the algorithm in.\n            algorithm (Algorithm): The algorithm to run.\n            num_episodes_per_experience (int, optional): Number of episodes per call to `.experience()`. Defaults to 32.\n            wandb_kwargs (Optional[Dict[str, Any]], optional): Keyword arguments for configuring Weights &amp; Biases. Defaults to None.\n            dataloader_kwargs (Optional[Dict[str, Any]], optional): Keyword arguments for the PyTorch DataLoader, such as batch size and shuffle. Defaults to None.\n        \"\"\"\n\n        self.environment = environment\n        self.algorithm = algorithm\n        self.num_episodes_per_experience = num_episodes_per_experience\n        self.use_wandb = use_wandb\n        self.wandb_kwargs = wandb_kwargs or {}\n        self.dataloader_kwargs: Dict[str, Any] = dataloader_kwargs or {}\n\n        if self.use_wandb:\n            self.wandb = ASTRAWandbLogger(self.wandb_kwargs)\n\n    def step(self, batch: Batch) -&gt; tuple[torch.Tensor, Dict[Any, Any]]:\n        \"\"\"Run a step of the algorithm on the dataset.\n\n        Args:\n            batch (Batch): The dataset batch to run the algorithm on.\n\n        Returns:\n            tuple[torch.Tensor, Dict[Any, Any]]: A tuple containing:\n                - torch.Tensor: The loss computed by the algorithm (for current batch).\n                - Dict[Any, Any]: Additional information for logging.\n        \"\"\"\n\n        result: torch.Tensor\n        logging_dict: Dict[Any, Any]\n        result, logging_dict = self.algorithm.step(batch)\n        step_logs: Dict[Any, Any] = {}\n\n        # TODO: Add other values here to logs besides algorithm specifics? Alternatively, can just return logging_dict\n        step_logs = {\n            **logging_dict,\n        }\n\n        return result, step_logs\n\n    def experience(self, seed: Optional[int] = None) -&gt; Iterator[Batch]:\n        \"\"\"Collect some experiences!\n\n        Args:\n            seed (Optional[int], optional): Seed for reproducibility. Defaults to None.\n\n        Returns:\n            Sequence[Step]: A sequence of steps collected from the algorithm's rollouts.\n        \"\"\"\n\n        logger.debug(\n            f\"Collecting {self.num_episodes_per_experience} episodes of experience...\"\n        )\n\n        graphs = []\n        for _ in range(self.num_episodes_per_experience):\n            graph = self.environment.rollout(seed=seed)\n            graphs.append(graph)\n\n        steps = sum([list(self.algorithm.flatten(i)) for i in graphs], [])\n\n        logger.debug(\n            f\"Done collecting {self.num_episodes_per_experience} episodes of experience\"\n            f\", got {len(steps)} training steps.\"\n        )\n\n        return iter(\n            DataLoader(\n                ListDataset(steps),\n                collate_fn=self.algorithm.collate_fn,\n                **self.dataloader_kwargs,\n            )\n        )\n\n    def log_current_step(self, current_logs: Dict[Any, Any]) -&gt; None:\n        \"\"\"Log the current step metrics to Weights &amp; Biases (if enabled) and logger.\n\n        Args:\n            current_logs (Dict[Any, Any]): The logs to be recorded.\n        \"\"\"\n        if self.use_wandb:\n            self.wandb.log(current_logs)\n\n        # Always log to the logger\n        # TODO: Do we want to log to the logger? Should be fine as used for debugging?\n        logger.info(f\"Current logs: {current_logs}\")\n</code></pre>"},{"location":"api/training/harness.html#astra_rl.training.harness.Harness.__init__","title":"<code>__init__(environment, algorithm, num_episodes_per_experience=32, use_wandb=False, wandb_kwargs=None, dataloader_kwargs=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>environment</code> <code>Environment</code> <p>The environment to run the algorithm in.</p> required <code>algorithm</code> <code>Algorithm</code> <p>The algorithm to run.</p> required <code>num_episodes_per_experience</code> <code>int</code> <p>Number of episodes per call to <code>.experience()</code>. Defaults to 32.</p> <code>32</code> <code>wandb_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>Keyword arguments for configuring Weights &amp; Biases. Defaults to None.</p> <code>None</code> <code>dataloader_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>Keyword arguments for the PyTorch DataLoader, such as batch size and shuffle. Defaults to None.</p> <code>None</code> Source code in <code>src/astra_rl/training/harness.py</code> <pre><code>def __init__(\n    self,\n    environment: Environment[StateT, ActionT],\n    algorithm: Algorithm[StateT, ActionT, Step, Batch],\n    num_episodes_per_experience: int = 32,\n    use_wandb: bool = False,\n    wandb_kwargs: Optional[Dict[str, Any]] = None,\n    dataloader_kwargs: Optional[Dict[str, Any]] = None,\n) -&gt; None:\n    \"\"\"\n    Args:\n        environment (Environment): The environment to run the algorithm in.\n        algorithm (Algorithm): The algorithm to run.\n        num_episodes_per_experience (int, optional): Number of episodes per call to `.experience()`. Defaults to 32.\n        wandb_kwargs (Optional[Dict[str, Any]], optional): Keyword arguments for configuring Weights &amp; Biases. Defaults to None.\n        dataloader_kwargs (Optional[Dict[str, Any]], optional): Keyword arguments for the PyTorch DataLoader, such as batch size and shuffle. Defaults to None.\n    \"\"\"\n\n    self.environment = environment\n    self.algorithm = algorithm\n    self.num_episodes_per_experience = num_episodes_per_experience\n    self.use_wandb = use_wandb\n    self.wandb_kwargs = wandb_kwargs or {}\n    self.dataloader_kwargs: Dict[str, Any] = dataloader_kwargs or {}\n\n    if self.use_wandb:\n        self.wandb = ASTRAWandbLogger(self.wandb_kwargs)\n</code></pre>"},{"location":"api/training/harness.html#astra_rl.training.harness.Harness.experience","title":"<code>experience(seed=None)</code>","text":"<p>Collect some experiences!</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>Optional[int]</code> <p>Seed for reproducibility. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Iterator[Batch]</code> <p>Sequence[Step]: A sequence of steps collected from the algorithm's rollouts.</p> Source code in <code>src/astra_rl/training/harness.py</code> <pre><code>def experience(self, seed: Optional[int] = None) -&gt; Iterator[Batch]:\n    \"\"\"Collect some experiences!\n\n    Args:\n        seed (Optional[int], optional): Seed for reproducibility. Defaults to None.\n\n    Returns:\n        Sequence[Step]: A sequence of steps collected from the algorithm's rollouts.\n    \"\"\"\n\n    logger.debug(\n        f\"Collecting {self.num_episodes_per_experience} episodes of experience...\"\n    )\n\n    graphs = []\n    for _ in range(self.num_episodes_per_experience):\n        graph = self.environment.rollout(seed=seed)\n        graphs.append(graph)\n\n    steps = sum([list(self.algorithm.flatten(i)) for i in graphs], [])\n\n    logger.debug(\n        f\"Done collecting {self.num_episodes_per_experience} episodes of experience\"\n        f\", got {len(steps)} training steps.\"\n    )\n\n    return iter(\n        DataLoader(\n            ListDataset(steps),\n            collate_fn=self.algorithm.collate_fn,\n            **self.dataloader_kwargs,\n        )\n    )\n</code></pre>"},{"location":"api/training/harness.html#astra_rl.training.harness.Harness.log_current_step","title":"<code>log_current_step(current_logs)</code>","text":"<p>Log the current step metrics to Weights &amp; Biases (if enabled) and logger.</p> <p>Parameters:</p> Name Type Description Default <code>current_logs</code> <code>Dict[Any, Any]</code> <p>The logs to be recorded.</p> required Source code in <code>src/astra_rl/training/harness.py</code> <pre><code>def log_current_step(self, current_logs: Dict[Any, Any]) -&gt; None:\n    \"\"\"Log the current step metrics to Weights &amp; Biases (if enabled) and logger.\n\n    Args:\n        current_logs (Dict[Any, Any]): The logs to be recorded.\n    \"\"\"\n    if self.use_wandb:\n        self.wandb.log(current_logs)\n\n    # Always log to the logger\n    # TODO: Do we want to log to the logger? Should be fine as used for debugging?\n    logger.info(f\"Current logs: {current_logs}\")\n</code></pre>"},{"location":"api/training/harness.html#astra_rl.training.harness.Harness.step","title":"<code>step(batch)</code>","text":"<p>Run a step of the algorithm on the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Batch</code> <p>The dataset batch to run the algorithm on.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Dict[Any, Any]]</code> <p>tuple[torch.Tensor, Dict[Any, Any]]: A tuple containing: - torch.Tensor: The loss computed by the algorithm (for current batch). - Dict[Any, Any]: Additional information for logging.</p> Source code in <code>src/astra_rl/training/harness.py</code> <pre><code>def step(self, batch: Batch) -&gt; tuple[torch.Tensor, Dict[Any, Any]]:\n    \"\"\"Run a step of the algorithm on the dataset.\n\n    Args:\n        batch (Batch): The dataset batch to run the algorithm on.\n\n    Returns:\n        tuple[torch.Tensor, Dict[Any, Any]]: A tuple containing:\n            - torch.Tensor: The loss computed by the algorithm (for current batch).\n            - Dict[Any, Any]: Additional information for logging.\n    \"\"\"\n\n    result: torch.Tensor\n    logging_dict: Dict[Any, Any]\n    result, logging_dict = self.algorithm.step(batch)\n    step_logs: Dict[Any, Any] = {}\n\n    # TODO: Add other values here to logs besides algorithm specifics? Alternatively, can just return logging_dict\n    step_logs = {\n        **logging_dict,\n    }\n\n    return result, step_logs\n</code></pre>"},{"location":"api/training/trainer.html","title":"Trainer","text":""},{"location":"api/training/trainer.html#astra_rl.training.trainer","title":"<code>astra_rl.training.trainer</code>","text":"<p>trainer.py The trainer is an opinionated interface designed for making training new models easy. To gain full customization over the model training pipeline, we recommend using the lower-level <code>Harness</code> interface in <code>harness.py</code>.</p>"},{"location":"api/training/trainer.html#astra_rl.training.trainer.Trainer","title":"<code>Trainer</code>","text":"<p>               Bases: <code>Generic[StateT, ActionT, Step, Batch]</code></p> <p>A high-level trainer that pushbutton trains your policy</p> Example <p>Here is an example of how to use the <code>Trainer</code> class with the DPO algorithm and an AST problem environment</p> <p>import torch from astra_rl import ( ...     Trainer, ...     TrainingConfiguration, ... ) from astra_rl.algorithms.dpo import ( ...     DPO, ... ) from astra_rl.methods.ast import ( ...     ASTProblem, ...     ASTEnvironment, ... )</p> <p>problem = ( ...     ASTProblem() ... ) environment = ( ...     ASTEnvironment( ...         problem, ... ...     ) ... ) algorithm = DPO(...) config = TrainingConfiguration( ...     lr=1e-3, ...     batch_size=16, ...     optimizer=\"adamw\", ...     gradient_accumulation_steps=1, ...     training_steps=1024, ...     num_episodes_per_experience=8, ... ) trainer = Trainer( ...     config, ...     environment, ...     algorithm, ... ) trainer.train()</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>TrainingConfiguration</code> <p>The configuration for the training process.</p> <code>harness</code> <code>Harness</code> <p>The harness that manages the training loop and interactions with the environment. See <code>astra_rl.training.harness</code> for what it does.</p> <code>optimizer</code> <code>Optimizer</code> <p>The optimizer used for updating the model parameters.</p> <code>_global_step_counter</code> <code>int</code> <p>A counter for global steps, used for gradient accumulation.</p> Source code in <code>src/astra_rl/training/trainer.py</code> <pre><code>class Trainer(Generic[StateT, ActionT, Step, Batch]):\n    \"\"\"A high-level trainer that pushbutton trains your policy\n\n    Example:\n        Here is an example of how to use the `Trainer` class with the DPO algorithm\n        and an AST problem environment\n\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; from astra_rl import (\n        ...     Trainer,\n        ...     TrainingConfiguration,\n        ... )\n        &gt;&gt;&gt; from astra_rl.algorithms.dpo import (\n        ...     DPO,\n        ... )\n        &gt;&gt;&gt; from astra_rl.methods.ast import (\n        ...     ASTProblem,\n        ...     ASTEnvironment,\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; problem = (\n        ...     ASTProblem()\n        ... )\n        &gt;&gt;&gt; environment = (\n        ...     ASTEnvironment(\n        ...         problem, ...\n        ...     )\n        ... )\n        &gt;&gt;&gt; algorithm = DPO(...)\n        &gt;&gt;&gt; config = TrainingConfiguration(\n        ...     lr=1e-3,\n        ...     batch_size=16,\n        ...     optimizer=\"adamw\",\n        ...     gradient_accumulation_steps=1,\n        ...     training_steps=1024,\n        ...     num_episodes_per_experience=8,\n        ... )\n        &gt;&gt;&gt; trainer = Trainer(\n        ...     config,\n        ...     environment,\n        ...     algorithm,\n        ... )\n        &gt;&gt;&gt; trainer.train()\n\n    Attributes:\n        config (TrainingConfiguration): The configuration for the training process.\n        harness (Harness): The harness that manages the training loop and interactions with the environment. See `astra_rl.training.harness` for what it does.\n        optimizer (Optimizer): The optimizer used for updating the model parameters.\n        _global_step_counter (int): A counter for global steps, used for gradient accumulation.\n    \"\"\"\n\n    optimizer: Optimizer\n\n    def __init__(\n        self,\n        config: TrainingConfiguration,\n        environment: Environment[StateT, ActionT],\n        algorithm: Algorithm[StateT, ActionT, Step, Batch],\n    ):\n        \"\"\"\n        Args:\n            config (TrainingConfiguration): The configuration for the training process.\n            environment (Environment): The environment to run our algorithm in.\n            algorithm (Algorithm): The algorithm used for training the attacker agent.\n        \"\"\"\n\n        self.config = config\n        self.harness = Harness(\n            environment, algorithm, config.num_episodes_per_experience\n        )\n\n        # TODO initialize LR scheduler?\n        # ?????????????????????????????\n\n        # initialize optimizer\n        if config.optimizer == \"adam\":\n            from torch.optim import Adam\n\n            self.optimizer = Adam(environment.problem.parameters(), config.lr)\n        elif config.optimizer == \"adamw\":\n            from torch.optim import AdamW\n\n            self.optimizer = AdamW(environment.problem.parameters(), config.lr)\n        elif config.optimizer == \"sgd\":\n            from torch.optim import SGD\n\n            self.optimizer = SGD(environment.problem.parameters(), config.lr)\n        elif config.optimizer == \"rmsprop\":\n            from torch.optim import RMSprop\n\n            self.optimizer = RMSprop(environment.problem.parameters(), config.lr)\n        elif config.optimizer == \"adagrad\":\n            from torch.optim import Adagrad\n\n            self.optimizer = Adagrad(environment.problem.parameters(), config.lr)\n        else:\n            raise ValueError(f\"Unknown optimizer configured: {config.optimizer}\")\n\n        # step counter, for acccmulutaion, etc.\n        self._global_step_counter = 0\n\n    def train(self) -&gt; None:\n        \"\"\"Run training by the specified config!\n\n        Note:\n            This method takes no arguments and returns nothing, and its\n            only used for side effects. We don't really need it other than\n            it's helpful for allowing the user to contro when training\n            actually starts (instead of immediately after Trainer construction).\n        \"\"\"\n        for _ in range(self.config.training_steps):\n            buf = self.harness.experience()\n            for batch in buf:\n                # increment counter first for occumulation\n                self._global_step_counter += 1\n                loss: torch.Tensor = (\n                    self.harness.step(batch)[0]\n                    / self.config.gradient_accumulation_steps\n                )\n                # typing disabled here b/c mypy can't statically verify\n                # that the loss has gradients\n                loss.backward()  # type: ignore[no-untyped-call]\n\n                # if gradient accumulation happens, step!\n                if (\n                    self._global_step_counter % self.config.gradient_accumulation_steps\n                    == 0\n                ):\n                    self.optimizer.step()\n                    self.optimizer.zero_grad()\n</code></pre>"},{"location":"api/training/trainer.html#astra_rl.training.trainer.Trainer.__init__","title":"<code>__init__(config, environment, algorithm)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>config</code> <code>TrainingConfiguration</code> <p>The configuration for the training process.</p> required <code>environment</code> <code>Environment</code> <p>The environment to run our algorithm in.</p> required <code>algorithm</code> <code>Algorithm</code> <p>The algorithm used for training the attacker agent.</p> required Source code in <code>src/astra_rl/training/trainer.py</code> <pre><code>def __init__(\n    self,\n    config: TrainingConfiguration,\n    environment: Environment[StateT, ActionT],\n    algorithm: Algorithm[StateT, ActionT, Step, Batch],\n):\n    \"\"\"\n    Args:\n        config (TrainingConfiguration): The configuration for the training process.\n        environment (Environment): The environment to run our algorithm in.\n        algorithm (Algorithm): The algorithm used for training the attacker agent.\n    \"\"\"\n\n    self.config = config\n    self.harness = Harness(\n        environment, algorithm, config.num_episodes_per_experience\n    )\n\n    # TODO initialize LR scheduler?\n    # ?????????????????????????????\n\n    # initialize optimizer\n    if config.optimizer == \"adam\":\n        from torch.optim import Adam\n\n        self.optimizer = Adam(environment.problem.parameters(), config.lr)\n    elif config.optimizer == \"adamw\":\n        from torch.optim import AdamW\n\n        self.optimizer = AdamW(environment.problem.parameters(), config.lr)\n    elif config.optimizer == \"sgd\":\n        from torch.optim import SGD\n\n        self.optimizer = SGD(environment.problem.parameters(), config.lr)\n    elif config.optimizer == \"rmsprop\":\n        from torch.optim import RMSprop\n\n        self.optimizer = RMSprop(environment.problem.parameters(), config.lr)\n    elif config.optimizer == \"adagrad\":\n        from torch.optim import Adagrad\n\n        self.optimizer = Adagrad(environment.problem.parameters(), config.lr)\n    else:\n        raise ValueError(f\"Unknown optimizer configured: {config.optimizer}\")\n\n    # step counter, for acccmulutaion, etc.\n    self._global_step_counter = 0\n</code></pre>"},{"location":"api/training/trainer.html#astra_rl.training.trainer.Trainer.train","title":"<code>train()</code>","text":"<p>Run training by the specified config!</p> Note <p>This method takes no arguments and returns nothing, and its only used for side effects. We don't really need it other than it's helpful for allowing the user to contro when training actually starts (instead of immediately after Trainer construction).</p> Source code in <code>src/astra_rl/training/trainer.py</code> <pre><code>def train(self) -&gt; None:\n    \"\"\"Run training by the specified config!\n\n    Note:\n        This method takes no arguments and returns nothing, and its\n        only used for side effects. We don't really need it other than\n        it's helpful for allowing the user to contro when training\n        actually starts (instead of immediately after Trainer construction).\n    \"\"\"\n    for _ in range(self.config.training_steps):\n        buf = self.harness.experience()\n        for batch in buf:\n            # increment counter first for occumulation\n            self._global_step_counter += 1\n            loss: torch.Tensor = (\n                self.harness.step(batch)[0]\n                / self.config.gradient_accumulation_steps\n            )\n            # typing disabled here b/c mypy can't statically verify\n            # that the loss has gradients\n            loss.backward()  # type: ignore[no-untyped-call]\n\n            # if gradient accumulation happens, step!\n            if (\n                self._global_step_counter % self.config.gradient_accumulation_steps\n                == 0\n            ):\n                self.optimizer.step()\n                self.optimizer.zero_grad()\n</code></pre>"},{"location":"api/training/trainer.html#astra_rl.training.trainer.TrainingConfiguration","title":"<code>TrainingConfiguration</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A typechecked dataclass which configures the training procedure.</p> <p>Attributes:</p> Name Type Description <code>lr</code> <code>float</code> <p>Learning rate for the optimizer.</p> <code>batch_size</code> <code>int</code> <p>Size of each batch (after flattening from experience) for training.</p> <code>optimizer</code> <code>str</code> <p>Type of optimizer to use [choices: \"adam\", \"adamw\", \"sgd\", \"rmsprop\", \"adagrad\"].</p> <code>gradient_accumulation_steps</code> <code>int</code> <p>Number of steps to accumulate gradients before updating the model weights.</p> <code>training_steps</code> <code>int</code> <p>Total number of rollouts to run and train for.</p> <code>num_episodes_per_experience</code> <code>int</code> <p>Number of rollouts to run before making a gradient update.</p> Source code in <code>src/astra_rl/training/trainer.py</code> <pre><code>class TrainingConfiguration(BaseModel):\n    \"\"\"A typechecked dataclass which configures the training procedure.\n\n    Attributes:\n        lr (float): Learning rate for the optimizer.\n        batch_size (int): Size of each batch (after flattening from experience) for training.\n        optimizer (str): Type of optimizer to use [choices: \"adam\", \"adamw\", \"sgd\", \"rmsprop\", \"adagrad\"].\n        gradient_accumulation_steps (int): Number of steps to accumulate gradients before updating the model weights.\n        training_steps (int): Total number of rollouts to run and train for.\n        num_episodes_per_experience (int): Number of rollouts to run before making a gradient update.\n    \"\"\"\n\n    # optimization configuration\n    lr: float = 3e-3\n    batch_size: int = 16\n    optimizer: str = \"adamw\"\n    gradient_accumulation_steps: int = 1  # how many\n\n    # training configuration\n    training_steps: int = 1024  # how many rollouts to train for\n\n    # rollout configuration\n    num_episodes_per_experience: int = 8  # how many rollouts per gradient update\n</code></pre>"},{"location":"tutorials/index.html","title":"Tutorials","text":"<p>This section provides step-by-step guides and examples to help you get started with the ASTRA-RL toolbox. Each tutorial covers a specific aspect of the toolbox, from basic usage to advanced features.</p>"},{"location":"tutorials/running_an_evaluation.html","title":"Running An Evaluation","text":"<p>This tutorial will guide you through the process of running an evaluation using an existing evaluator model. The evaluator is a model that can be used to assess the quality of other models' outputs. </p> <p>For the purposes of this tutorial, we will assume that you have already trained an evaluator model from the Training an Evaluator tutorial.mk</p>"},{"location":"tutorials/training_an_evaluator.html","title":"Training an Evaluator","text":"<p>This tutorial will guide you through the process of training a custom model evaluator using the ASTRA-RL toolbox. The evaluator is a model that can be used to assess the quality of other models' outputs.</p>"}]}
{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Adaptive Stress Testing for Robust AI &amp; Reinforcement Learning (ASTRA-RL)","text":"<p>Welcome to the ASTRA-RL toolbox documentation! This documentation provides an overview of the ASTRA-RL toolbox, its features, and how to use it effectively.</p> <p>Warning</p> <p>This documentation (and project) is a work in progress. If you have any questions or suggestions, please feel free to open an issue on the GitHub repository</p>"},{"location":"api/index.html","title":"Library API","text":"<p>This section provides detailed information about all the classes, functions, and modules available in the ASTRA-RL toolbox. Each entry includes a description of its purpose, parameters, and usage examples.</p> <p>This documentation is generated automatically from the codebase using docstrings and comments, ensuring that it stays up-to-date with the latest changes.</p>"},{"location":"tutorials/index.html","title":"Tutorials","text":"<p>This section provides step-by-step guides and examples to help you get started with the ASTRA-RL toolbox. Each tutorial covers a specific aspect of the toolbox, from basic usage to advanced features.</p>"},{"location":"tutorials/quick_start_training.html","title":"Quick Start: Training a HuggingFace Attacker with ASTRA-RL","text":"<p>Do you want to train a HuggingFace attacker using an ASTRA-supported algorithm (e.g., DPO, IPO, PPO) and problem formulation (ASTPrompter, RL - Perez*, MALIBU, CRT*)?</p> <p>Then this guide is for you. We\u2019ll walk through every step required to train a red-teaming attacker using our pre-configured classes and point you to customization guides when your use case goes beyond the defaults.</p>"},{"location":"tutorials/quick_start_training.html#step-1-setup","title":"Step 1: Setup","text":"<p>Please see the README for full setup instructions. Here's a quick recap:</p> <pre><code># Install the ASTRA-RL toolbox\npip install astra-rl\n\n# Clone the repository (for examples and development)\ngit clone git@github.com:sisl/astra-rl.git\ncd astra-rl\n\n# Sync dependencies\nuv sync --dev\n\n# (Optional) Install pre-commit hooks to auto-format code\nuv run pre-commit install\n</code></pre>"},{"location":"tutorials/quick_start_training.html#step-2-create-a-training-script","title":"Step 2: Create a Training Script","text":"<p>Create a Python file for your training code (e.g., train.py).</p>"},{"location":"tutorials/quick_start_training.html#step-3-import-required-modules","title":"Step 3: Import Required Modules","text":"<pre><code>from torch.optim import AdamW\n\n# ASTRA-RL core components\nfrom astra_rl import ASTEnvironment, DPO, DetoxifyModerator, Harness\n\n# HuggingFace-friendly problem wrapper for ASTPrompter-style red teaming\nfrom astra_rl.ext.transformers import HFASTProblem\n</code></pre>"},{"location":"tutorials/quick_start_training.html#step-4-load-your-initial-prompts","title":"Step 4: Load Your Initial Prompts","text":"<p>To train an attacker, you\u2019ll need a list of comma-separated strings that act as initial prompts\u2014these initiate attacker-target rollouts used for online training. </p> <pre><code>import json\n\nwith open(\"prompts_reddit_train.json\") as f:\n    PROMPTS = json.load(f)\n</code></pre> <p>Since ASTPrompter red-teams for harmful outputs in conversational settings, it uses the ConvoKit Reddit Small Corpus (filtered for proper formatting and for non-toxicity using Detoxify) as its default source of initial prompts.</p> <p>The ASTRA-RL toolbox also supports external prompt datasets or APIs\u2014just ensure the final PROMPTS variable is formatted as a list of strings.</p>"},{"location":"tutorials/quick_start_training.html#step-5-set-your-device","title":"Step 5: Set Your Device","text":"<pre><code>DEVICE = \"cuda\"  # or \"cpu\" if GPU is not available\n</code></pre> <p>We support both lightweight (e.g., GPT-2 + Detoxify) and heavyweight (e.g., LLaMA + LlamaGuard) setups.</p>"},{"location":"tutorials/quick_start_training.html#step-6-instantiate-your-problem","title":"Step 6: Instantiate Your Problem","text":"<p>The <code>HFASTProblem</code> class is a HuggingFace-compatible extension of <code>ASTProblem</code>. It simplifies red-teaming with transformer-based language models by handling:</p> <ul> <li>Automatic loading of attacker, target, and baseline models</li> <li>Tokenizer setup with fallback padding tokens for compatibility</li> <li>Rollout \"step\" generation</li> <li>Log probability computation of attacker, target, and baseline responses</li> </ul> <p>You simply provide the model IDs and a <code>Moderator</code> instance, and <code>HFASTProblem</code> manages the rest\u2014making it easy to plug into ASTRA-RL\u2019s training and evaluation pipeline.</p> <p>Note: If your attacker and target are different models (e.g., GPT-2 attacker, LLaMA target), this class handles all the tokenizer/model interop for you.</p> <pre><code>from astra_rl.modifiers import LlamaGuardModerator  # optional\n\n# Example 1: GPT2 attacker, target, baseline with Detoxify (lightweight setup)\nproblem = HFASTProblem(\"gpt2\", \"gpt2\", \"gpt2\", DetoxifyModerator(), DEVICE)\n\n# Example 2: GPT2 attacker, LLaMA target, GPT2 baseline with LlamaGuard (GPU recommended)\nproblem = HFASTProblem(\"gpt2\", \"meta-llama/Llama-3-8b\", \"gpt2\", LlamaGuardModerator(), DEVICE)\n</code></pre> <p>Need a custom model or rollout step logic? See customize/problems</p> <p>Want to use a custom moderator? See customize/moderators</p>"},{"location":"tutorials/quick_start_training.html#step-7-instantiate-the-environment","title":"Step 7: Instantiate the Environment","text":"<p>The environment defines how training rollouts are structured and collected. In ASTRA-RL, the default is the <code>ASTEnvironment</code>, which implements the conversation tree rollout used in the ASTPrompter paper.</p> <pre><code>env = ASTEnvironment(problem, PROMPTS)\n</code></pre> <p>This environment builds a tree-structured conversation graph, where:     - The root node starts from a random initial prompt (from PROMPTS)     - At each turn, the attacker generates multiple candidate utterances (tree_width, default 2)     - Each of those utterances is fed to the target model, which produces a response     - The resulting attacker\u2013target\u2013response tuples form child nodes     - This process repeats for tree_depth levels (default 3), yielding a multi-turn attacker-target dialogue tree This structure enables preference-based learning algorithms like DPO and IPO to reason over multiple conversational branches at once, training the attacker to elicit increasingly harmful responses over time.</p> <p>By default, rollouts are configured with tree_width=2 and tree_depth=3, but you can customize both:</p> <pre><code>env = ASTEnvironment(problem, PROMPTS, tree_width=4, tree_depth=5)\n</code></pre> <p>Want a different rollout graph structure or a multi-agent setup? See customize/environments</p>"},{"location":"tutorials/quick_start_training.html#step-8-choose-your-algorithm-and-optimizer","title":"Step 8: Choose Your Algorithm and Optimizer","text":"<pre><code>solver = DPO(problem)\noptimizer = AdamW(problem.parameters(), lr=1e-5)\n</code></pre> <p>To integrate your own RL algorithm, see customize_training/solvers</p>"},{"location":"tutorials/quick_start_training.html#step-9-create-the-training-harness","title":"Step 9: Create the Training Harness","text":"<pre><code>harness = Harness(\n    env,\n    solver,\n    num_episodes_per_experience=2,\n    use_wandb=True,\n    dataloader_kwargs={\"batch_size\": 4},\n)\n</code></pre> <p>For more information on the training harness, see     -&gt; astra-rl/docs/tutorials/customize_training/harness</p>"},{"location":"tutorials/quick_start_training.html#step-10-train-the-attacker","title":"Step 10: Train the Attacker","text":"<pre><code>for step in range(1000):\n    # Collect experience rollouts from attacker-target interactions\n    buf = harness.experience()\n    for experience in buf:\n        # Compute loss using the solver (e.g., DPO)\n        loss, step_logs = harness.step(experience)\n\n        # Standard PyTorch optimization\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        step_logs[\"step\"] = step\n        harness.log_current_step(step_logs)\n</code></pre>"},{"location":"tutorials/quick_start_training.html#full-example-examplesast_hfpy","title":"Full Example: examples/ast_hf.py","text":"<p>We provide a complete working example that mirrors this guide!</p>"},{"location":"tutorials/running_an_evaluation.html","title":"Running An Evaluation","text":"<p>This tutorial will guide you through the process of running an evaluation using an existing evaluator model. The evaluator is a model that can be used to assess the quality of other models' outputs. </p> <p>For the purposes of this tutorial, we will assume that you have already trained an evaluator model from the Training an Evaluator tutorial.mk</p>"},{"location":"tutorials/training_an_evaluator.html","title":"Training an Evaluator","text":"<p>This tutorial will guide you through the process of training a custom model evaluator using the ASTRA-RL toolbox. The evaluator is a model that can be used to assess the quality of other models' outputs.</p>"},{"location":"tutorials/customize_training/environments.html","title":"How to Customize the Environment","text":"<p>The environment defines how attacker\u2013target interactions are structured and collected for training. It controls how rollouts are generated, how trajectories are organized, and what data is passed to the solver.</p> <p>In ASTRA-RL, the default environment is <code>ASTEnvironment</code>, which constructs a tree of attacker\u2013target interactions as used in the ASTPrompter paper. You can fully customize this structure to support: - Multi-agent interactions - Turn-based or fixed-length rollouts - Flat trajectories instead of trees - New state advancement logic - Different reward shaping strategies</p>"},{"location":"tutorials/customize_training/environments.html#step-1-subclass-the-base-environment","title":"Step 1: Subclass the Base Environment","text":"<p>All custom environments must subclass:</p> <pre><code>from astra_rl.core.environment import Environment\n\nclass MyCustomEnvironment(Environment[StateT, ActionT]):\n    ...\n</code></pre> <p>This base class defines the interface between the environment and the training harness.</p>"},{"location":"tutorials/customize_training/environments.html#step-2-implement-the-rollout-method","title":"Step 2: Implement the rollout method","text":"<p>Your custom class must implement:</p> <pre><code>def rollout(self, seed: Optional[int] = None) -&gt; Graph[StateT, ActionT]\n</code></pre> <p>The rollout() method should return a Graph object, which holds a tree or list of conversation trajectories, each node containing at a minimum: - state (e.g. prompt so far) - action (e.g. attacker utterance) - target_response (optional) - reward (float)</p> <p>Within your rollout() method, you can utilize the instance of ASTProblem your encironment will interact with, providing... - rollout_prompt_with_attacker(prompts: Sequence[str]) \u2192 Sequence[str] - rollout_prompt_with_target(prompts: Sequence[str]) \u2192 Sequence[str] - reward(prompts, attacks, defenses) \u2192 Sequence[float] - advance(prompt, attack, defense) \u2192 str: updates the conversation state</p> <p>You can call these methods directly inside your environment to implement the rollout logic of your choice.</p>"},{"location":"tutorials/customize_training/environments.html#tips-and-best-practices","title":"Tips and best practices","text":"<ul> <li>Use Node(...) to represent each step of a rollout. Each node includes:</li> </ul> <pre><code>Node(state: str, action: str, target_response: str, reward: float, children: List[Node])\n</code></pre> <ul> <li>Return a Graph(prompt, nodes) from your rollout() function.</li> <li>advance() helps build the new state after attacker and target interact.</li> <li>Want to add additional metadata? Subclass Node and modify Graph accordingly.</li> </ul>"},{"location":"tutorials/customize_training/moderators.html","title":"How to Create a Custom Moderator Class","text":"<p>Moderators play a central role in LM red teaming, acting similarly to reward models in traditional reinforcement learning. Their job is to quantify the reward an adversarial agent receives for reaching a particular state\u2014typically by measuring how harmful or unsafe a target model's output is.</p> <p>In many RL-based red-teaming setups, the moderator provides the signal that trains the attacker to generate utterances that elicit harmful responses from a target model. This achieves the red-teaming objective by exposing weaknesses in the target model\u2019s safety alignment and highlighting where additional fine-tuning is needed.</p> <p>To serve this purpose, a moderator must: - Accept a sequence of target model generations (e.g., text), - Return a scalar score (e.g., a toxicity value from 0 to 1) indicating the level of harm.</p> <p>The astra-rl toolbox currently supports text-based moderation using: - Detoxify, for toxicity classification, - Llama Guard 3, for a variety of harm categories (e.g., hate speech, threats, etc.).</p> <p>But the framework is modular\u2014you can define your own moderator class, wrapping any model that takes in your defined StateT and ActionT types (see astra_rl/core/common) and returns a Sequence[float].</p> <p>This guide walks you through creating a new Moderator subclass.</p>"},{"location":"tutorials/customize_training/moderators.html#step-1-subclass-the-moderator-base-class","title":"Step 1: Subclass the Moderator Base Class","text":"<p>To define your own moderator, create a class that inherits from:</p> <pre><code>Moderator[StateT, ActionT]\n</code></pre> <p>Where: - StateT is the type of state your environment uses (e.g., a string prompt) - ActionT is the type of action your model produces (e.g., a generated response) For most NLP use cases, both StateT and ActionT are str.</p> <p>example:</p> <pre><code>from typing import Sequence\nfrom detoxify import Detoxify\nfrom astra_rl.core.moderator import Moderator\n\nclass DetoxifyModerator(Moderator[str, str]):\n    def __init__(self, harm_category: str = \"toxicity\", variant: str = \"original\"):\n        self.model = Detoxify(variant)\n        self.harm_category = harm_category\n</code></pre>"},{"location":"tutorials/customize_training/moderators.html#step-2-implement-the-moderate-method","title":"Step 2: Implement the moderate Method","text":"<p>You must implement the abstract method:</p> <pre><code>def moderate(self, x: Sequence[Union[StateT, ActionT]]) -&gt; Sequence[float]:\n</code></pre> <p>This method: - Takes a sequence of states and/or actions. - Returns a sequence of floats, where each float is the moderation score (e.g., toxicity score) for the corresponding input.</p> <p>example:</p> <pre><code>def moderate(self, x: Sequence[str]) -&gt; Sequence[float]:\n        return self.model.predict(x)[self.harm_category]\n</code></pre>"},{"location":"tutorials/customize_training/moderators.html#step-3-integrate-your-moderator","title":"Step 3: Integrate your moderator","text":"<p>Once your class is defined, you can plug it into the RL pipeline like any other component:</p> <pre><code>moderator = DetoxifyModerator(harm_category=\"insult\", variant=\"unbiased\")\nscores = moderator.moderate([\"you are stupid\", \"have a nice day!\"])\n</code></pre> <p>To train with your custom moderator, modify your problem subclass to instantiate it during initialization:</p> <p>example:</p> <pre><code>class ExampleDetoxifyProblem(ASTProblem):\n    def __init__(self, device=\"cpu\"):\n        # your choice of moderator\n        super().__init__(DetoxifyModerator()) ## Plug in your custom moderator here ##\n\n        self.device = device\n        self.attacker = GPT2LMHeadModel.from_pretrained(MODEL_NAME).to(self.device)\n        self.target = GPT2LMHeadModel.from_pretrained(MODEL_NAME).to(self.device)\n\n        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n        self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n</code></pre>"},{"location":"tutorials/customize_training/moderators.html#helpful-notes","title":"Helpful Notes:","text":"<ul> <li> <p>Your moderator can wrap any scoring model\u2014e.g., classifiers, LLMs, rule-based filters\u2014as long as it implements moderate(...) \u2192 Sequence[float].</p> </li> <li> <p>You can include internal logic to handle tokenization, batching, preprocessing, etc.</p> </li> <li> <p>Return one score per input in the same order as received.</p> </li> <li> <p>If you're using a library or model that scores multiple types of harm (like Detoxify or llamaguard), your class can expose a harm_category attribute to customize which score to extract.</p> </li> </ul>"},{"location":"tutorials/customize_training/moderators.html#full-examples","title":"Full examples:","text":"<p>See the following files for complete, working implementations: - astra_rl/moderators/detoxify.py \u2014 wraps the Detoxify library - astra_rl/moderators/llamaGuard.py \u2014 wraps Meta\u2019s Llama Guard 3 model</p>"},{"location":"tutorials/customize_training/problems.html","title":"How to Create a Custom Problem Class for Red-Teaming with ASTRA-RL","text":"<p>The <code>Problem</code> class defines the core logic of how attacker-target interactions occur in reinforcement-learning-based red-teaming. It specifies:</p> <ul> <li>How the attacker and target interact and advance the conversation state.</li> <li>How reward signals are computed from attacker-target interactions.</li> <li>How a model rollout step is performed.</li> <li>What models and tokenizers are used for attacker, target, and baseline (if applicable).</li> </ul> <p>By subclassing the <code>Problem</code> or <code>ASTProblem</code> class, you can customize any of these aspects to fit your particular red-teaming scenario, algorithm, or dataset. In general, we reccomend subclassing from the ASTProblem or HFASTProblem whenever you can and then over-writing methods or definitions to suite your needs. </p> <p>This guide will show you how to:</p> <ul> <li>Create your own custom subclass.</li> <li>Integrate your own models and tokenizers.</li> <li>Customize reward computation and rollout logic.</li> </ul>"},{"location":"tutorials/customize_training/problems.html#understanding-the-base-class","title":"Understanding the Base Class","text":"<p>The base <code>ASTProblem</code> class provides default implementations suitable for the ASTPrompter approach:</p> <ul> <li><code>advance</code>: Defines how a prompt advances given an attacker action and a target response.</li> <li><code>reward</code>: Defines how rewards are calculated from attacker-target interactions, typically using toxicity scoring and perplexity measures.</li> <li><code>rollout_prompt_with_attacker</code> / <code>rollout_prompt_with_target</code>: Methods to generate attacker and target model outputs from a given context.</li> <li><code>parameters</code>: Specifies model parameters for optimization.</li> </ul> <p>You should subclass this base class to preserve and extend this default behavior. If you want to change a method, simply define it in your subclass and it will over-right the original implementation while preserving the rest of the base class functionality.</p>"},{"location":"tutorials/customize_training/problems.html#how-to-create-a-subclass-with-custom-modelstokenizers","title":"How to create a subclass with custom models/tokenizers","text":"<p>To subclass your own <code>Problem</code>, follow this template:</p> <pre><code>from astra_rl.methods.ast_problem import ASTProblem\nfrom astra_rl.core.moderator import Moderator\n\nclass MyCustomProblem(ASTProblem):\n    def __init__(self, moderator: Moderator[str, str], my_custom_param: float = 1.0):\n        super().__init__(moderator)\n        self.my_custom_param = my_custom_param\n\n    def advance(self, state: str, action: str, response: str) -&gt; str:\n        # Example: Simply concatenate with separators\n        return f\"{state}\\n[Attacker]: {action}\\n[Target]: {response}\"\n\n    def reward(self, contexts, attacks, responses):\n        # Implement your own reward logic here\n        scores = self.moderator.moderate(responses)\n        return [score * self.my_custom_param for score in scores]\n\n    def rollout_prompt_with_attacker(self, prompts):\n        # Implement custom attacker rollout logic\n        raise NotImplementedError\n\n    def rollout_prompt_with_target(self, prompts):\n        # Implement custom target rollout logic\n        raise NotImplementedError\n\n    def parameters(self):\n        # Implement if your problem has trainable model parameters\n        return []\n</code></pre>"},{"location":"tutorials/customize_training/problems.html#step-3-integrating-your-own-models-attacker-target-or-baseline","title":"Step 3: Integrating Your Own Models (Attacker, Target, or Baseline)","text":"<p>If you are using huggingface models, save time by subclassing from our HFASTProblem base class which takes in any huggingface model names for the attacker, target, and baseline. Additionally, you can integrate any pretrained language model by loading the model and tokenizer in your constructor. Here's how to do it clearly and correctly:</p> <pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\n# Create your subclass from the base Problem (most bare-boned) or the base ASTPrompter class (takes care of rollout and log probability methods)\nclass MyHuggingFaceProblem(ASTProblem):\n    def __init__(self, attacker_model_id: str, target_model_id: str, moderator, device=\"cuda\"):\n        super().__init__(moderator)\n        self.device = device\n\n        # Load your models and tokenizers\n        self.attacker = AutoModelForCausalLM.from_pretrained(attacker_model_id).to(self.device)\n        self.attacker_tokenizer = AutoTokenizer.from_pretrained(attacker_model_id)\n\n        self.target = AutoModelForCausalLM.from_pretrained(target_model_id).to(self.device)\n        self.target_tokenizer = AutoTokenizer.from_pretrained(target_model_id)\n\n    # required method to perform one step of a rollout in a batched manner: must take in a state (eg. conversation so far) and return a list of continuations (attacker utterances) in the corresponding order\n    def rollout_prompt_with_attacker(self, prompts):\n        inputs = self.attacker_tokenizer(prompts, return_tensors=\"pt\", padding=True).to(self.device)\n        outputs = self.attacker.generate(**inputs, max_new_tokens=32)\n        generated_texts = self.attacker_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n        continuations = [gen[len(prompt):] for prompt, gen in zip(prompts, generated_texts)]\n        return continuations\n\n    def rollout_prompt_with_target(self, prompts):\n        inputs = self.target_tokenizer(prompts, return_tensors=\"pt\", padding=True).to(self.device)\n        outputs = self.target.generate(**inputs, max_new_tokens=32)\n        generated_texts = self.target_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n        continuations = [gen[len(prompt):] for prompt, gen in zip(prompts, generated_texts)]\n        return continuations\n\n    def parameters(self):\n        return self.attacker.parameters()\n</code></pre>"},{"location":"tutorials/customize_training/problems.html#step-4-customizing-rollout-logic","title":"Step 4: Customizing Rollout Logic","text":"<p>To customize how attacker-target rollouts are performed, override these methods clearly:</p> <p>rollout_prompt_with_attacker(prompts: Sequence[str]) \u2192 Sequence[str]</p> <p>rollout_prompt_with_target(prompts: Sequence[str]) \u2192 Sequence[str]</p> <p>For example, you might use sampling strategies, temperature adjustments, or custom stopping criteria:</p> <pre><code>def rollout_prompt_with_attacker(self, prompts):\n    inputs = self.attacker_tokenizer(prompts, padding=True, return_tensors=\"pt\").to(self.device)\n    outputs = self.attacker.generate(\n        **inputs,\n        do_sample=True,\n        max_new_tokens=50,\n        temperature=0.8,\n        top_k=50,\n        top_p=0.9\n    )\n    texts = self.attacker_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n    return [text[len(prompt):] for prompt, text in zip(prompts, texts)]\n</code></pre>"},{"location":"tutorials/customize_training/problems.html#step-5-customizing-reward-logic","title":"Step 5: Customizing Reward Logic","text":"<p>Override the reward method to compute your custom reward signal. Typically, you'll combine toxicity, relevance, perplexity, or other metrics:</p> <pre><code>def reward(self, contexts, attacks, responses):\n    attack_scores = self.moderator.moderate(attacks)\n    response_scores = self.moderator.moderate(responses)\n    combined = [(a_score + r_score) / 2.0 for a_score, r_score in zip(attack_scores, response_scores)]\n    return combined\n\n</code></pre>"},{"location":"tutorials/customize_training/problems.html#tips-and-best-practices","title":"Tips and Best Practices:","text":"<p>Always clearly document parameters and logic for your custom problem class.</p> <p>Ensure models and tokenizers are device-aware (e.g., GPU-compatible).</p> <p>Thoroughly test your rollouts independently before integrating them into the full RL loop.</p> <p>For debugging, add verbose logging to track input-output sequences.</p>"},{"location":"tutorials/customize_training/problems.html#further-reading-and-examples","title":"Further Reading and Examples","text":"<p>Default ASTPrompter implementation: ASTProblem</p> <p>HuggingFace-compatible subclass example: HFASTProblem</p> <p>Environment customization guide: Custom Environments</p>"}]}